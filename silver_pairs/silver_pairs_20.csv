paragraph,questions
"All right? So this is just scaling.
[NOISE] Now scaling.
[NOISE] That is scaling along the axes- [NOISE] along the, uh, the x and y-axes, not the eigen- eigenvector but scaling along the axes.
[NOISE] Right? Now, [NOISE] the scaling for the singular value decomposition is always real-valued, right? Uh, because S is always gonna be real-valued for SVD.
So this is a real-valued scaling, [NOISE] right? But for the eigenvalue decomposition, some of your eigenvalues may be complex.
And scaling by a complex value essentially means there is some amount of rotation involved, right? Scaling by a- a- a complex number, ah, means there could be rotation.
[NOISE] If eigenvalue is complex, [NOISE] right? And then, Step 3 is another rotation.
Here, the Step 3 is gonna be, ah, rotating it by U.
So this is just Rotation 1 [NOISE] and this is Rotation 2, [NOISE] right? Means your- your- we rotate in- in SVD.
We [NOISE] rotate it, scale it along the, ah, diagonals after rotating.
And then, rotate it by a different amount corresponding to U.
Whereas with singular value decomposition, ah, with eigenvalue decomposition, first, we rotate it, then scale it.
But the scaling may- you know, if you have complex, ah, eigenvalue, the scaling may, ah, involve some implicit rotation, and then we [NOISE] rotate it by U, which is basically the inverse of the 1st step.",I have a question at https://youtu.be/b0HvwszmqcQ?t=3327. If the eigen values are equal that means do we get a scaled sphere instead of an ellipsoid?
"And so we do inference on the target samples-- Yeah.
--for which we don't have labels.
But the predictions, if those have good certainty, if the model is much more confident on the samples-- of some samples-- On test domain? On the test domain, yeah.
Could we use them as a ground truth-- [INTERPOSING VOICES] The question is, we can use labels from-- you can some examples with some pseudo-labels in the target domain.
Yeah.
This is another question would be close to some domain adaptation or semi supervise the linear settings.
Yeah.
We will not consider these settings here.
Is there data on how this performs on, I mean, out of either domain distribution? So if you then showed a blue five and a blue two.
Does it work better on those than if you had not done any red-green augmentation? Your question is that if we have any blue ones or any auto-distribution, something like that? Yeah, it definitely works.
So for example, we can put this example here, is we change the background from green to blue.
So it can still work very well.
So because in case you will learn some digit information and ignore this domain information.
So another variant of LISA is called intra-domain LISA.",would abductive generalization help with domain adaptation?
"What is another way to do that? You relabel your datasets by taking into account the boundaries.
The model still doesn't perform well.
I think it's all about the weighting of the loss function.
It's likely that the number of pixels that are boundaries are going to be fewer than the number of pixels that are cells or no cells.
So the network will be biased towards predicting cell or no cell.
Instead, what you can do is, when you compute your loss function, your loss function should have three terms.
One, binary cross-entropy let's say for no cell, one for cell, [NOISE] and one for boundary.
[NOISE] Okay, and this is going to be summed over, i equals 1 to n_i.
The whole output pixel values.
What you can do is to attribute a coefficient to each of those; alpha, beta or one.",Hi. I think you are missing the non-linearity added in each layer( like RELU or tanh ) which affects the derivatives.
"So, as soon as that clock runs out at two minutes, some answer is available.
It'll be the best one that the system can compute in the time available given the characteristics of the game tree as it's developed so far.
So, there are other kinds of anytime algorithms.
This is an example of one.
That's how all game playing programs work, minimax, plus alpha-beta, plus progressive deepening.
Christopher, is alpha-beta a alternative to minimax? CHRISTOPHER: No.
SPEAKER 1: No, it's not.
It's something you layer on top of minimax.
Does alpha-beta give you a different answer from minimax? CHRISTOPHER: No.
No, it doesn't.
SPEAKER 1: Let's see everybody shake their head one way or the other.","I have a few questions. The first one is, is Minimax considered as a state space search? If it is is there a Goal state/node?
I understand how minimax and alpha-beta algorithms work. But I can't understand how they are able to find a good solution. How can alternatively selecting the minimum and maximum values in consecutive levels of a game tree provide a good solution? Can someone help me out here?
I paused the video just after he explained the 2^2 British museum method, and I thought ""hang on, shouldn't the player who's making the move be trying to figure out what the implications of planning 5 moves ahead are given that each second successive state has to make assumptions about what the opponent would do in the intermediate state? So I started thinking about how you might predict an opponents move and even got onto thinking if you could use a neural network to predict it based off the opponents previous moves!"" Then he explained minimax and I felt stupid... Although it does raise a second question, ""If you matched two of these algorithms against each other, is the result deterministic?"" Any ideas?
I wonder how much you save by using the tree of the last move as a basis for the next one, since the min player can be a human, and he might not take the branch you predicted. So the alpha beta algorithm assumes the min player will always take the option that is most in the min player's own interest, which is not always the case in computationally ""flawed"" humans."
"How far should the drunk move in zero steps? Zero.
How far should the drunk move in one steps? We know that should be one.
Two steps, well, we knew what that should be.
Well, if I run this sanity check, these are the numbers I get.
I should be pretty suspicious.
I should also be suspicious they're kind of the same numbers I got for 10,000 steps.
What should I think about? I should think that maybe there's a bug in my code.
So if we now go back and look at the code, yes, this fails the pants on fire test that there's clearly something wrong with these numbers.
What we were appending is walk of Homer, numTrials, 1.
Well, numTrials is a constant.
It's always 100.
What I intended to write here was not numTrials but numSteps.
I actually did this the first time I wrote this simulation many years ago.","If someone could help me, in the textbook there's another exemple of drunk: the EW Drunk, moving only in the horizontal axe (-1, 0) and (1, 0). However this drunk is also getting farther away from the origin. But why? If after n number of steps he has equal chance to step etiher W or E, wasn't he supposed to be back to the origin according to the law of big numbers? Isn't it the same as to flip n coins and count number of head and tails?
I did not have run the simmulation because the result is presented in the textbook. The professor is computing the average of distances and not the expected value of the random walk. The random walk should be a bell curve with its peak at distance zero, so the expected value of the walk is always zero for the EW. What happens is that when the number of steps increases the bell curve becomes wider and you have small probabilities of finding bigger distances, hence the 'mean' distance increases a little bit. Hower the expected distance to the origin is still zero. Kind of misleading IMO but it is correct."
"Oh by the way, whenever I do a rotation, I'm also going to have to update my subtree properties.
When I rotate this edge, A does not change, B does not change, C does not change.
So that's good.
But x's subtree changes.
It now has y.
It didn't before.
So we're going to have to also update the augmentation here in y.
And we're going to have to update the augmentation in x.
And we're going to have to update the augmentation of all of the ancestors of x eventually.
So rotation is locally just changing a constant number of pointers.
So I usually think of rotations as taking constant time.
But eventually, we will have to do-- this is constant time locally.
But we will need to update h ancestors in order to store all of-- keep all of our augmentations up to date.
We'll worry about that later.
All right, so great.
Now we have the height of all the nodes.
We can compute the skew of all the nodes, cool.
We have this rotation operation.
And we want to maintain this height balance property.
Height of left node-- left and right of every node-- is plus or minus 1, or 0.
Cool, so I said over here somewhere, whenever we-- so the only things that change the tree are when we insert or delete a new node.","His formula skew(node) = height(node.right) - height(node.left) is correct. But your calculation implicitly suggests that you thought height(NIL) = 0, which is wrong. In fact, the height (and depth) in his class (see Lecture 6) is defined as the # edges, not # nodes. So height(leaf) = 0 (1 node subtree has 0 edge), and height(NIL) = -1. The latter case for NIL must be defined this way to ensure that the recursive relationship height(node) = 1 + max{height(node.left), height(node.right)} holds even if node = leaf, i.e., height(leaf) = 1 + max{height{NIL}, height{NIL}} = 1 + max{-1, -1} = 1 + (-1) = 0. Now back to your example, the correct drawing with more details becomes: Z (height(Z): 2, skew(Z) = height(Y) - height(NIL) = 1 - (-1) = 2) \ Y (height(Y) = 1, skew(Y) = height(X) - height(NIL) = 0 - (-1) = 1) \ X (height(X) = 0, skew(X) = height(NIL) - height(NIL) = -1 - (-1) = 0) So Z is unbalanced, and we need to perform right_rotate(Z) to restore the balance.
Just think about a local property, as that depends only on the node's subtree. The depth depends on what happens above the node, so it cannot be local."
"There's combinations, there's also a little bit of look ahead, where the hardest puzzles are the ones where you run out of the eights and the ones in terms of the examples that we have here where we've just sort of implied-- without guessing, we've implied the location of a number.
And then because of that, our puzzle got smaller in the sense that there's fewer blank locations, blank squares.
And then that helps us move forward.
So the easy puzzles are the ones where fairly straightforward implications like the ones we did here always exist, are easy to find.
Sometimes you have to search a little bit, look at the top, look at the bottom, look at the middle.
And then you fill things in.
And because you filled things in, something else now is in play, right? It becomes viable in terms of an implication.
The fact that I put an eight in there implies that the eight is now taken-- its location.
And so now obviously there's only four left here.","Interesting. Are you saying that if a puzzle requires guessing then it's essentially ""indeterminate"" and would have more than one solution? I mean, if you had a sudoku board with only one number populated, you'd have a large number of solutions that would be possible, and then if you had a board with another number added, you'd have a smaller set, and so on until you would have one that would still have multiple solutions until you constrained it with a final number."
"It's going to call the init method with x is equal to 3 and y is equal to 4.
I'm just going to go over here and I wrote this previously, because notice when we're creating an object here, we're only giving it two parameters.
But in the init method, we have actually three parameters, right? We have these three parameters here, but when we're creating an object, we only give it two parameters.
And that's OK because implicitly, Python is going to say self is going to be this object C, so just by default, OK? So when you're creating a coordinate object, you're passing it all the variables except for self.
So this line here is going to call the init and it's going to do every line inside the init.
So it's going to create an x data attribute for C, a y data attribute for C, and it's going to assign 3 and 4 to those respectively.","Why doesn't the innit have a return statement?
Nice ! extremely grateful learn from anna ma'am. Well I have questions( time-----24.11) what is guarantee that use will provide X,Y in distance(self,other) for OTHER parameter.how he/she even know this ? Definitely One way by documentation but any other way .?"
"Um, so, so, so, so there are some logistical changes.
For example, uh, uh, we've gone from- uh, what we used to hand out paper copies of handouts, uh, that we're, we're trying to make this class digital only.
Uh, but let me talk a little bit about, uh, prerequisites as well as in case your friends have taken this class before, some of the differences for this year, right? Um, so prerequisites.
Um, we are going to assume that, um, all of you have a knowledge of basic computer skills and principles.
Uh, so, you know, Big O notation, queues, stacks, binary trees.
Hopefully, you understand what all of those concepts are.
And, uh, assume that all of you have a basic familiarity with, um, uh, probability, right? Hopefully, you know what's a random variable, what's the expected value of a random variable, what's the variance of a random variable.
Um, and if- for some of you, maybe especially the SCPD students taking this remotely, it has been, you know, some number of years since you last had a probability and statistics class.
Uh, we will have review sessions, uh, on, on, on Fridays, uh, where we'll go over some of this prerequisite material as well.
But it's okay.
Hopefully, you know what a random variable is, what expected value is.
But if you're a little bit fuzzy on those concepts, we'll go over them again, um, at a- at a discussion section, uh, on Friday.
Also, seem to be familiar with basic linear algebra.
So hopefully that you know what's a matrix, what's a vector, how to multiply two matrices and multiplying matrices and a vector.
Um, if you know what an eigenvector then that's even better.
Uh, if you're not quite sure what an eigenvector is, we'll go over it that- that you, you, you better- uh, yeah, we'll, we'll, we'll go over it I guess.
And then, um, a large part of this class, uh, uh, is, um, having you practice these ideas through the homeworks, uh, as well as I mention later a, uh, open-ended project.","Do you need to know linear algebra for this course.
Hey bro any previous knowledge is required for this course ?
Hey question, what math should I exactly understand as a background before I start learning this course?"
"I mean, you're not going to have V3 left and V7 left.
There's no way that's going to happen.
You're just going to keep shrinking, taking things from the left or the right.
So it's going to be i and i plus 1.
That make sense? And so in this case, what would you pick? Vi and i plus 1.
You just pick the max.
Because at the end of this, either you did it right or you did it wrong.
Either way, you're going to improve your situation by picking the max of Vi or Vi i plus 1.
So there's no two things about it.
So here, you're going to pick the maximum of the two.
And you might have Vi i plus 2, which is an odd number of coins that your opponent might see.
It gets more complicated for Vi and i plus 2.
We're going to have to now start thinking in more general terms as to what the different moves are.
But we've got the base cases here.
All I did here was take care of the base case or a couple of base cases associated with a single coin, which is what your opponent will see and pick, or two coins, which is your last move.
So with DP, of course, you always have to go down to your base case, and that's when things become easy.","I am not sure what you mean by sum(i,j) here. It looks to me as if the point that your and Praveen's point can be made more clearly by omitting the references to sum, like so (using C ""?"" syntax): V(i,j) = (i == j) ? v[i] : max(v[i]-V(i+1,j), v[j]-V(i,j-1))."
"So if we're trying to find the cube root of 8, for example, versus a cube root of 9-- this is 8 and this is 9-- what is this code going to do? It's going to first guess 0.
0 cubed is not greater or equal to 8.
1 cubed is not greater or equal to 8.
2 cubed is greater or equal to 8, so here, once we've guessed 2, we're going to break.
Because we found a number that works.
And there's no need to keep looking.
Once we've found the cubed root of this number 8, there's no need to keep searching the remainder, 3, 4, 5, 6, 7, 8.
Sort of the same idea when we're trying to find the cube root of 9.
We're going to start with 0.
0 to the power of 3 is less than 9.
1 to the power of 3 is less 9.
2 to the power of 3 is less than 9.
When we get to 3 to the power of 3, that's going to be greater than 9.
So this code tells us, once we've picked a number that's beyond the reasonable number of our cubed root, of our cube, the cubed root of our cube, then we should stop.
Because, again, it doesn't make sense to keep searching.
Because if 3 to the power of 3 is already greater than 9, 4 to the power of 3 is also going to be greater than 9 and so on.
So once we break here, we either have guess being 2 or guess being 3 depending on what cube we're trying to find.","can someone explain for me what is the meaning of iteration in programing in a simple way
I though it was called binary search why it's called bisection in here?
Why aren't we getting cube root of 27 and 8120601 in form of x.xx as we always increment guess by second decimal place.
Cube root simple guess. Why does that code keeps running?
Am I stupid? In Approximations code we set guess=0.0 and increment=0.01. How can our answer be different than increment times number of guesses??
and guess**3<=cube is better choice ?
What I get from the approximation algorithm is that it will only give you an answer if the difference between guess**3 and cube is equal to epsilon, if it manages to jump above epsilon, guess will increase in value until it is <= cube, and thus the message ""failed to obtain a value"" will appear. How can we make it so that if the difference does jump above epsilon that the value will get will be the value of guess at time - 1 of that moment.
Why did she set N/2k to 1? (N/2k =1) Thanks!
For bisection I wanted to use this code to take direct input, but error is coming. #To find the cube root of a number using bisection method cube=float(input(""Put the number... "")) accuracy=float(input(""Give accuracy limit... "") low=0.0 high=cube guess=((high+low)*0.5) iteration=0 while abs(guess**3.0-cube)>accuracy: if (guess**3.0)<=cube: low=guess else: high=guess guess=((high+low)*0.5) iteration += 1 print (guess, ""is close to the cube root of"", cube) print (""Total iteration="", iteration) **************************** syntax error in low=0 why?
can anyone tell me what am i doing wrong!! code: cube = int(input('type the no....')) for guess in range(cube+1): if guess**3 == cube: print(""Cube root of"", cube, ""is"", guess) else: print(""cube root does'nt exist"") output:- type the no....1(#1 was the input) cube root does'nt exist Cube root of 1 is 1
How do you do a bisection search for a cuberoot when x is a decimal? I cant be find an answer anywhere.
why are we writing range(cube+1) instead of *range(cube)*?
I know she said it is a simple if statement but I can't figure it out. What did you do to make it work? My thought is: if cube < 0: guess = -guess
What is the answer of cube = 27 on the exercise. I rewrote the code on the IDE but the execution never ended, why? Help, please
What is the if statement for negative values in the bisection search
what is the meaning of epsilon here ???"
"All right? Between the two of those, we get our supreme pizza, but sometimes, as we'll see, they sort of mess each other up.
So one of the toppings just doesn't go well with the other one.
Maybe someone doesn't Hawaiian.
They think that the ham doesn't go well with the pineapple.
So let's do a branch and bound that just has an extended list.
Maybe we've got some green peppers on this pizza.
This is going to be safe.
All right.
So we're going to list the nodes as to the extended list, and the way we're going to do that is, well, you guys said you like goal tree more than queue, so let's do it with a goal tree.
So we've got S, and as we already know, S is the only path.
Our current length at S is 0.
That's the lowest of all of our lengths on the tree cause it's the only length.
So we go to A, B, and C.
It may be hard to see, so the length of SA is 100.
The length of SB is-- AUDIENCE: You mean ABEF? AUDIENCE: Yeah.
[CHATTER] PROFESSOR: Oh.
You're right.
It's a different tree.
I was doing A, B, and C from the other tree.
Thank you, friends, for correcting my foolishness.
The length of SB is 3.
It is a different tree.
I was trying to save work that wound up creating more because someone's going to be confused by this.
So the length of A, B, E, and F are 100, 3, 14, and 14.
Which one do we choose? Lexicographically, it's A.
We choose that one, right? AUDIENCE: [CHATTER] PROFESSOR: F is going to be 14.
That' a 1.
Sorry.
AUDIENCE: 4.
4.
PROFESSOR: Oh, it's 4? I wrote this problem.
I should know it.
You're right.
F is 4.","Hi, Is there anyone who knows and can explain how he came up with the choice tree using the table? Thanks in advance..."
"Uh, that's what it means for a product of two things to be greater than zero, both things have to have the same sign, right, and so if this is if- if, um, so as long as this is bigger than 0, it means it's classifying that example correctly.
Um, and the SVM is asking for it to not just classify correctly, but classify correctly with the- with the functional margin of the- at least 1.
Um, and if you allow CI to be positive, then that's relaxing that constraint.
Okay.
Um, but you don't want the CIs to be too big which is why you add to the optimization cost function, a cost for making CI too big.
[NOISE] And so you optimize this as function of W.
[NOISE] And these are Greek alphabets c.
[NOISE] Um, and if- if you draw a picture, it turns out that, um, in this example with that being the optimal decision boundary, um, it turns out that these examples- [NOISE] these three examples would be equidistant from this straight line, right? Because if they weren't, then you can fiddle the straight line to improve the margin even a little bit more.
It turns out that these few examples have, um, functional margin exactly equal to 1.
And this example over there, we have functional margin equal to 2, and the further away examples of even bigger functional margins.
And what this optimization objective is saying is that it is okay if you have an example here, where functional margin so everything right so everything here has functional margin one.
If an example here I have functional margin a little bit less than one.
And this by having- by setting Ci to 0.5 say is letting me [NOISE] get away with having function module lower than, less than 1.","Question for the kind hearted people who understood: in the geometric margin, y is either 1 or -1 right? But wasnt it mentioned before that y is either 1 or 0? I got a little messed up on that.
What is that that weird symbol he wrote during L1 soft margin svm? He called it c-i , i guess"
"Or if we choose x6 to be false, then that path will lead to here and we'll be able to hit this question mark and get the star up here.
So as long as we satisfy the clause, there will be at least one star.
Won't help if you have multiple stars.
Then the final traversal part-- so that was this first clause.
And now we're traversing through.
Actually in this picture, it's left to right.
Just turn your head.
And so now Mario is going to have to traverse this gadget from left to right on this top part.
And if Mario comes in here and you can barely jump over that.
If there's a star, you can collect the star and then run through all of these flaming bars of death.
If there's no star, you can't.
You'll die if you try to traverse.
So in order to be able to traverse all these clauses, they must all be true.
And them all being true is the same is their AND being true.
So you will be able to survive through all these clauses if and only if this formula has a satisfying assignment.
The satisfying assignment would be given to you by the level play.
The choices that Mario makes in this gadget will tell you whether each variable should be true or false.
So to elaborate just a little bit more in general, when you have a reduction like this, to prove that it actually works, you need to check two things.","can someone explain the mario and clause reduction to me, or provide a link for theory.
Why create the last crossover gadget instead of using pipes to move you where you need to be? Or was the spirit of the exercise to use the most basic implementation of Mario possible? Is the whole goomba/mushroom thing better? (Also, the final mushroom check can be a small tunnel of fire vines short enough for the hit invincibility to last, but full enough that small Mario can't pass.)"
"Um, I think on Monday someone asked, ""Do the number of speakers and number of microphones need to be equal?"" So it turns out that, um, if the number of, uh, um, microphones is larger than the number of speakers, that's actually fine, right? If you- if the number of microphones is larger than the number of speakers, then if you run ICA or, or a slightly modified version of it, you'll find that some of the speakers are just silent speakers.
Um, uh, and so, you know, if you have, uh, 10 microphones and five speakers, if you run this algorithm on 10 microphones, you can find that, well, maybe five of the sources are just silent or there are ways to just not model those five sources as well, right? If, if you think that, uh, they're just some sources of silence.","Do you guys know when is the ""last time"" he mentions about ""cocktail party problem"" before this lecture?"
"So we're just going to exponentiate and normalize these distances where we are dividing by the distance between the anchor in each of the negatives.
And so this basically gives you the probability that x plus is a positive example, rather than being a negative example.
And when you actually then compute the loss for this-- oh, actually, sorry.
These should technically, I guess, be f of x.
Or equivalently, you could write these as d of z or d of z and d of z plus and d of z and d of z minus.
So d will just correspond to either the Euclidean losses before or something like negative cosine similarity.","Also, why did the professor use a negative sign for the distance? The paper clearly doesnt do that. I think this is corrected but it makes it confusing"
"However, we can also take the same set of individuals and connect them based on sexual relationships, but then, we'll ab- creating a sexual network, or for example, if we have a set of scientific papers, we can connect them based on citations, which paper cites which other paper.
But for example, if we were to connect them based on whether they use the same word in the title, the- the quality of underlying network and the underlying, uh, representations might be, uh, much worse.
So the choice of what the- the nodes are and what the links are is very important.
So whenever we are given a data set, then we need to decide how are we going to design the underlying graph, what will be the objects of interest nodes, and what will be the relationships between them, what will be the edges.",what will be the effect of losing use-case-specific information?
"I'm biased, but I really like the recursive one.
It is crisper to look at, you can see what it's doing.
I'm reducing this problem to a simpler version of that problem.
Pick your own version but I would argue that the recursive version is more intuitive to understand.
From a programmer's perspective, it's actually often more efficient to write, because I don't have to think about interior variables.
Depending on the machine, it may not be as efficient when you call it because in the recursive version I've got it set up, that set of frames.
And some versions of these languages are actually very efficient about it, some of them a little less so.
But given the speed of computers today, who cares as long as it actually just does the computation.
Right, one more example, how do we really know our recursive code works? Well, we just did a simulation but let's look at it one more way.
The iterative version, what can I say about it? Well, I know it's going to terminate because b is initially positive, assuming I gave it an appropriate value.
It decreases by 1 every time around this loop, at some point it has to get less than 1, it's going to stop.
So I can conclude it's always going to terminate.
What about the recursive version? Well, if I call it with b equal to one, I'm done.
If I call it with b greater than one, again it's going to reduce it by one on the recursive call, which means on each recursive call it's going to reduce and eventually it gets down to a place, assuming I gave it a positive integer, where b is equal to one.
So it'll stop, which just good.
What we just did was we used the great tool from math, second best department at MIT.
Wow, I didn't even get any hisses on that one, John, all right, and I'm now in trouble with the head of the math department.","Would inclusion in the lecture of the 'call stack' or Python's symbol table concept help explain recursion? As you recursively call the function object's return value, the frames get 'popped' off the stack (symbol table on Python I think?) Sorry, self-taught. Still learning every day.
0!=1 but the function given here runs in an infinite recursion when the input is 0
How do we initialise variables in recursion in Python so that it continues values in every local scope without it's initial value in global scope?
In his Towers of Hanoi code, I can figure out the process when n==1 and n==2, but I can't figure out steps in the process when n==3 or more. I think he wants us to think of it recursively: if it is true for smaller versions of the problem, then it is true for the bigger version of the problem and the code turns out to be true when n==3
Ken MacDonald Yeah I know but as n goes up, there will be hundreds of steps. So do they recurse on the other because the same rules still applied?
I'm a little confused on one point. I understand recursion, but from what I am reading online, most of the time it is more efficient in terms of processing times to write code using an iteration than a recursion. I get that some people might find one more readable than the other and that for many programs, most users wouldn't notice a difference, but wouldn't it be a best practice to normally iterate to make code as efficient as possible?"
"Now for the next element of this I need something to read.
Has anyone got a textbook handy? Ah, ""China, an Illustrated History."" Now I need a volunteer.
OK.
Andrew, you want to do this? So here's what you're going to do.
You can stay there.
But you need to stand up.
So what I'm going to do is, I'm going to read you a passage from this book.
And I want you to say it back to me at the same time I read it.
It's as if you're doing simultaneous translation, except it's English to English.
This things got words I can't pronounce.
OK, are you ready to go? All right.
""When overwhelmed by the magnitude of the problems he tackled, he began to suspect that others were plotting against him or secretly ridiculing him."" Thank you very much.
That's great.
So you see, he could do it.
Some people can't do it.
At least it take a little practice.
But he did it.
And guess what I've done to him? I've reduced his intelligence to that of a rat.
Because if you do this experiment with an adult human who's doing this simultaneous English to English translation, they go with equal probability to the two corners.
So what's happened? What's happened is you've jambed their language processor.
And when their language processor is jambed they can't put the blue wall together with the rectangular shape.
So it seems to be that language is the mediator of exactly the combinators you need in order to build descriptions.
Because they can't even put those things together when their language processor is jambed by the simultaneous translation phenomenon.","Great lecture. What is the experiment name that Patrick Winston described at the end of the lecture?
What was the topic covered in Lecture 20??"
"In fact, that's how we got here in the first place.
So the question is-- let's see, the question is, do we have a rule in the consequent that matches Millicent studies a lot.
Yeah, that's right, P3.
That's right.
So P3 would give us Millicent studies a lot.
And so therefore what will we add to the goal tree? AUDIENCE: We add both that is Millicent either a protagonist or a villain and [INAUDIBLE].
MARK SEIFTER: Yeah, she.
Turns out to be a girl.
So that's exactly right.
We heard that we need to add-- we need to add an AND node, which also has an OR node at the bottom of it, because this is a little bit of a complicated rule.","In rule P2, it states: IF ?x lives in SD THEN ?x is a villain ?x is ambitious. Why does the word AND not appear there? Shouldn't it say this: IF ?x lives in SD THEN AND(?x is a villain, ?x is ambitious) Or is adjacency meaning multiple things written next to each other automatically assumed to be conjunction? If that is the case, why is AND ever used in these rules?"
"Just two differences I guess.
For linear regression.
Last week, I have written this down, theta J gets updated as theta J minus partial with respect to theta J of J of theta, right? So you saw this on Wednesday.
So the two differences between that is well, first instead of J of theta you're now trying to optimize the log-likelihood instead of this squared cost function.
And the second change is, previously you were trying to minimize the squared error.
That's why we had the minus.
And today you're trying to maximize the log-likelihood which is why there's a plus sign.
Okay? And so, um, so gradient descent you know, is trying to climb down this hill whereas gradient ascent has a, um, uh, has a- has a concave function like this.
And it's trying to, like, climb up the hill rather than climb down the hill.
So that's why there's a plus symbol here instead of a minus symbol because we are trying to maximize the function rather than minimize the function.
So the last thing to really flesh out this algorithm which is done in the lecture notes, but I don't want to do it here today is to plug in the definition of H of theta into this equation and then take this thing.
So that's the log-likelihood of theta and then through calculus and algebra you can take derivatives of this whole thing with respect to theta.
This is done in detail in the lecture notes.
I don't want to use this in class, but go ahead and take derivatives of this big formula with respect to the parameters theta in order to figure out what is that thing, right? What is this thing that I just circled? And it turns out that if you do so you will find that batch gradient ascent is the following.",can anyone explain me where that x came from in the final equation of gradient ascent
"So I had F and B, and so I don't want to add F or B there.
But what is convenient to do, as it turns out, is to just have something that essentially says, there's an ending point.
So you have an explicit end to the array that is actually not something that would crash if you tried to access it.
You had n people in line, and now you have effectively n plus 1 people in line.
But that n plus one person is a dummy, or you can call them ""end."" And then if you do that, then that's it.
The previous code works.
You can just remove those four lines, and it'll all just work because your F or your B is not equal to ""end."" So whatever you had at the end-- I'm sorry.","do we actually need two if checks?
Did this guy just populate the array randomly? I cant tell. I tried 2^(2n) -2n, -n, and -1 to check for indices and their evenness to determine where the Bs are being populated. Because if you know the loc of the Bs in the array, you know where the Fs are. In theory, iff you know the number of moves to flip the array to conform to one of the two, you know the other procedure. But, then you know based on which case you use for performance efficiency, the effectiveness of the algorithm . But for his array, the case of populating in B was like ceil(n/2) and F case was like floor(n/2) + c, c contains [1,2] for cases of 0...9 and 0...10. How it was populated is like the one thing I dont understand here. The best I can think of is using 2^i somewhere: Edit: parameters for c is [0,2], not [1,2]"
"For that to work, of course, the relations between subproblems must be acyclic, so we'd like to give an explicit topological order.
Usually it's a couple of for loops.
But this is a topological order in the subproblem DAG, which I defined somewhat informally last time.
the.
Vertices are subproblems, and I want to draw an edge from a smaller problem to a bigger problem, meaning that, if evaluating b in this relation calls a, then I'll draw an arrow from a to b, from the things I need to do first to the things I'll do later.
So then topological order will be ready-- by the time I try to compute b, I will have already computed a.","Is the topology reversed when you go from top down vs bottom up? I'm a bit confused by the topology. Are suffix subproblems always have topology of decreasing i ?
Hard time understanding relation between graphs and answer of problems"
"Like kind of the [INAUDIBLE].
LING REN: And, OK, so then it's let them select their leader respectively and compare which one is larger.
It's interesting thought.
AUDIENCE: First, for example, A, B finds the maximum between these two, the C, D finds the maximum E [INAUDIBLE] maximum.
Kind of merging, considering them as one node.
LING REN: I think the first difficulty I see is that if you cut it by half, it's no longer a ring, right.
AUDIENCE: Also they can't cut themselves in half.
LING REN: Yeah.
Yeah.
Correct.
Yeah, yeah.
Yeah, it's definitely not an easy problem, not a easy algorithm.
And the idea is to, well, you had the right idea.
That we want to-- what's the word? Let the weak candidates shut up early.","Not much intuitive , if half of node goes inactive in next round how could they ever know who is their supreme leader ? If I have A, B, C, D if say B send message to A, C I am the leader and they agree , then B sends message to D and say I am the leader , by D says that , I have large number dude , you are not leader then D is the leader now B know that D is the leader it need to pass that information down to B and C and now all know that D is there leader ."
"LING REN: Pardon? AUDIENCE: It would be O of n factorial? LING REN: You're going too fast.
Let's write the algorithm first.
So I want to solve my rectangle block problem, say from 1 to n.
What are my subproblems? AUDIENCE: Choose one block.
LING REN: OK.
Let's choose one block.
AUDIENCE: And then you run RB of everything except that block.
LING REN: So I get its height, and then I have a subproblem.
What is the subproblem? And then I'll take a max.
So the difficulty here is this subproblem.
So Andrew, right? So Andrew said it's just everything except i.
Is that the case? Go ahead.
AUDIENCE: It's everything except i, and anything with wider or longer than i.
LING REN: Do you get that? Not only do we have to exclude i, we also have to exclude everything longer or wider than i.
So that's actually a messy problem.
So let me define this subproblem to be a compatible set of w i.
And let me define that to be the set of blocks where the length is smaller than the required length, and their which is also smaller than the required width.
So this should remind you of the weighted interval scheduling problem, where we define a compatible set once we have chosen some block.
Question? AUDIENCE: What are we trying to do here? Are we trying to minimize h? LING REN: Maximize h.
We want to get as high as possible.
I choose a block, I get its height, and then I find out the competitive remaining blocks, and I want to stack them on top of it.
Everyone agrees this solution is correct? OK, then let's analyze its runtime.
So how do we analyze runtime? So what's the first question I always ask? AUDIENCE: How many subproblems? LING REN: Yeah.
I'm not sure who said that, but how many subproblems do we have? AUDIENCE: At most n? LING REN: At most n.","I doubt if O(m*n) is the solution for the grid problem.. Suppose we are going to m,n = (2, 2) , from x, y = (0, 0). Up (U) or Right (R) are options: P1: UURR P2: URUR P3: URRU P4: RURU P5: RRUU # of distinct path: 5 != 2*2 = m*n ..?
First of all time complexity is not the same as # of distinct paths from 0,0 to 2, 2. Second of all, you have a 3x3 matrix which has 6 distinct paths, you are missing the P6: RUUR.
16: 00 I couldn't understand the line ""One is East West, One is North South"" ... ?? I understood about that l ,w are to be compared against l1 and w1 and not w1 and l1 respectively. but how does that english sentence mean this mathematical expression.
what's the difference between these R videos and the normal lecture?"
"So we've got that the image of this set T, when we restrict it just to the part that's in the complement of S-- notice I'm leaving out this point here.
I'm not taking the image of T.
I'm taking the image of T restricted to the points that are not in the image of S.
And what I want to know is, could that cause a bottleneck? After all, I've left out some points that used to be included in the image of T.
And so the image of T might have been bigger than T because of those extra points that I'm leaving out, I have to be sure that that doesn't happen.
Could there be a bottleneck that has been created by these points that I've left out? Could there be a bottleneck in E of T intersection, a complement to E of S.
Could that orange guy be a bottleneck? Well, if it was a bottleneck, then I'd have a bottleneck in the whole graph.","what is S?
What is a bottleneck?"
"So that's why I'm doing self dot x here, right.
If I just did x, I would be accessing just some variable named x in a program which actually isn't even defined.
So you always have to refer when as we're thinking about classes, you always have to refer to whose data attribute do you want to access? In this case, I want to access the x data attribute of my self, and I want to subtract the x data attribute of this other coordinate, square that, same for y, square that, and then add those and take the square root of that.
So notice this method is pretty much like a function, right? You have DF, some name, it takes in parameters.
It does some stuff and then it returns a value.
The only difference is the fact that you have a self here as the first thing and the fact that you always have to be conscious about whose data attributes you're accessing.
So you have to use the dot notation in order to decide whose data attributes you want access.
So we've defined the method here, distance.
So this is in the class definition.
Now how do we use it? So let's assume that the definition of distance is up here.
I didn't include the code.
But really all you need to know is what it takes.
It takes a self and an other.
So when you want to use this method to figure out a distance between two coordinate objects, this is how you do it.
So the first line, I create one coordinate object.
Second line, I create another coordinate object.
First one is named C, the second one is named 0.
These are two separate objects.
And I'm going to find the distance.","Damoon Rastegar what does x represent here vs x0?
Why should coordinate be an object. It has no behaviour in itself, it's just a tuple of two ints. Just data. And a function to compute euclidian distance is just a function that takes two coordinates, four integers or two arrays, depending on how you decide to represent coordinates as data.
Nice ! extremely grateful learn from anna ma'am. Well I have questions( time-----24.11) what is guarantee that use will provide X,Y in distance(self,other) for OTHER parameter.how he/she even know this ? Definitely One way by documentation but any other way .?"
"And there are b to d of those.
How many are there at this next level up? Well, that must be b to the d minus 1.
How many fewer nodes are there at the second to the last, the penultimate level, relative to the final level? Well, 1 over b, right? So, if I'm concerned about not getting all the way through these calculations at the d level, I can give myself an insurance policy by calculating out what the answer would be if I only went down to the d minus 1th level.
Do you get that insurance policy? Let's say the branching factor is 10, how much does that insurance policy cost me? 10% of my competition.",When exactly does A-B prune the most nodes?
"And so that's something to keep in mind.
So this could get pretty messy.
So let's talk about insert, and I've spent a bunch of time skirting around the issue of what exactly happens when you insert an element.
Turns out delete is pretty easy.
Insert is more interesting.
Let's do insert.
To insert an element x into a skip list, the first thing we're going to do is search to figure out where x fits into the bottom list.
So you do a search just like you would if you were just doing a search.
You always insert into the appropriate position.
So if there's a single sorted list, that would pretty much be it.
And so that part is easy.
If you want to insert 67, you do all of the search operations that I just went over, and then you insert 67 between 66 and 72.
So do your pointer manipulations, what have you, and you're good.
But you're not done yet, because you want this to be a skip list and you want this to have expected search over any random query as the list grows and shrinks of order log n, expectation, and also with high probability.
So what you're going to have to do is when you start inserting, you're going to have to decide if you're going to what is called promote these elements or not.
And the notion of a promotion is that you are going up and duplicating this inserted element some number of levels up.
So if you just look at how this works, it's really pretty straightforward.
What is going to happen is simply that let's say I have 67 and I'm going to insert it between 66 and 72.",It's difficult to update a Skiplist efficiently without backpointers
"That's all there is to it.
If you can find a path, you could use depth-first search, you could use breadth-first search, you could use whatever you wanted.
You find a path from s to t, OK? If you find such a path, if this path exists, it means that the flow is not maximum, OK? If no path exists, the flow is maximum, and you're done.
If such a path exists, you will be able to increase the flow, in this case, because we have integral quantities, by at least one, OK? And you will be able to increase the flow by one.",How would you claim that using bottleneck value as the flow will always be correct ?
"Semantics is really about truth, right? It's about entailment, about the meaning-- what is actually true.
When we say a knowledge base entails f, what that means is that models of knowledge base is a subset of models of f, and in terms of meaning, that is actually what the truth is.
On the other hand, we've talked about syntax.
In syntax, we just do symbol manipulation, right, using inference rules.
We've looked at modus ponens as an inference rule, and we have looked at things like derivation, so knowledge base derives f, OK? So how are these two related? Let's talk about that.
And that brings us to the idea of soundness and completeness, OK? So we're going to talk about soundness and completeness.","Great lecture. Just wanted to check that you wanted to say models of KB belonging to models of g right? Since g is a superset of KB.
I thought is should be M(KB)  M(g) too. Thats what you mean right? M(g)  M(KB) would mean that M(g) is a subset of M(KB) and hence that some models of KB would not satify g, which goes against the definition of entailment."
"But just to give you an idea, the idea here is we don't measure time.
We instead measure ops.
And like your colleague over here was saying before, we expect performance-- I'm going to use performance, instead of time here-- we expect that to depend on size of our input.
If we're trying to run an algorithm to find a birthday in this section, we expect the algorithm to run in a shorter amount of time than if I were to run the algorithm on all of you.
So we expect it to perform differently, depending on the size of the input, and how differently is how we measure performance relative to that input.",why does he use k prime instead of k?
"So what can I do? I can subtract cn from both sides, maybe put that 1 on the other side here.
Then we get the c equals big I of 1.
c is, of course, a constant.
So we're in good shape.
My undergrad algorithms professor told me never to write a victory mark at the end of a proof.
You have to do a little square.
But he's not here.
So now, I see you.
But we're a little low on time.
So we'll save it for the lecture.
OK.
So if we want to implement the selection sort algorithm, well, what do we do? Well, we're going to think of i as the index of that red line that I was showing you before.
Everything beyond i is already sorted.
So in selection sort, the first thing I'm going to do is find the max element between 0 and i.
And then I'm going to swap it into place.
So this is just a code version of the technique we've already talked about.","Does anyone have resources for studying the substitution method the way it was done in this lecture?
You clearly don't understand what the Omega notation is in Algorithms."
"ey everyone.
Um, let's get started.
So, um, let's see, the plan for the [NOISE] day is, uh, we'll go over the rest of ICA, independent component analysis.
In particular, talking about CDFs, cumulative distribution functions [NOISE].
And then, um, actually, uh, let's do that later.
[NOISE].
All right.
So the plan is we'll go over, uh, the rest of ICA, independent component analysis, and we'll talk a bit about CDFs, um, cumulative distribution functions [NOISE], and then derive the ICA model.
And uh, in the second half of today, we'll start on the final of the, um, interesting, four major topics of the class, which is reinforcement learning.
We'll talk about MDPs, or Markov decision processes, okay? So to recap briefly, um, we had- you remember the overlapping voices demo.
So we said that in the ICA problem, independent component analysis problem, we'll assume we have sources S, which are RN if you have N speakers.","Does there seem to be a disconnect between this lecture and the previous one? Like something was skipped?
Do you guys know when is the ""last time"" he mentions about ""cocktail party problem"" before this lecture?"
"And the complete graph on n vertices is n minus 1 connected, but it has about n squared over 2 edges.
The hypercube.
Now, if we're going to be talking about graphs of size n to be uniform, the hypercube on Hn has 2 to the n vertices.
So if we want an n-vertex version, we're talking about h sub log n, which has n vertices.
And then it has about n log n over 2 edges.
So it's got fewer edges than the complete graph but significantly less, exponentially less connectivity.","thank you it was brief and clear.... what is well-linked and x-well-linked graph?
What is meant by triple connected"
"Um, and there is no leakage because each image is a data- data point, uh, by its own.
Uh, splitting a graph is different.
Um, the problem with the graph is that nodes are connected with each other.
So for example, in node classification, each data point is a node now, um, but these nodes are not independent from each other.
The nodes are actually connected, uh, with each other, meaning that, you know, for example, in this case, if I look at node 5, in order to predict node 5, it will, in terms of a graph neural network also collect information from nodes, uh, 1 and 2.
So this means that, uh, nodes, um, 1 and, um, uh, 2, um, will affect, uh, the prediction on- on- of node 5.
So if 1 and 2 are the training dataset and 5 is the test dataset, then clearly we have some, um, information, uh, leakage.
So, uh, this is why this is interesting.
So then the question is, um, what are our options? What can we- uh, what can we do? Um, we can do the following.
Um, the first solution is to do what we call, um, a transductive setting, where the input graph can be observed over, for all the dataset splits.
So basically we- we will work with the same graph structure for training, validation, and, uh, test set.
And we will only split the node- the node labels, meaning we'll keep the graph structure as is, but we're going to put some nodes into the training, test and, uh, validation, uh, set.",Thank you for this presentation! Very interesting. The question: When we do the lint prediction - the number of nodes should be constant in all dataset graphs? So they can not be like parts of each oter
"Which might be remotely.
But you don't want the launch daemon to know about which services each service is going to connect to.
PROFESSOR: Maybe, yeah.
That's a good question, right? So in Capsicum, as we were talking about, the network is in global namespace.
You have to have existing file descriptors for all the outstanding connections ahead of time.
AUDIENCE: Right.
But you don't necessarily want okld to open up all the sockets for all the services.
Because it might not know where the services are connected.
PROFESSOR: That's right.
Yeah.
So that's a little bit of an awkward thing.
I absolutely agree.
And this is part of the reason why I think capabilities haven't completely subsumed everything in the security world, is because they are kind of awkward to use.
Because the guy that gives you all the privileges has to know exactly what things you're going to need, like these connections to backend servers.
So at some level, maybe this is not such a huge problem in OKWS.
Because the launcher daemon has to read a Config file and is going to pass the token to the service in the first place.
So maybe the token is going to contain the host and port number to which you're connected to.
But I agree.
It's not great.
Because especially, suppose the database server disconnects you.
Well, you're kind of stuck now.
The file server is not connected anymore, and you can't connect to a new one.
So basically, if the database server crashes, or restarts, or the network breaks, you basically have to terminate it, get yourself response, so you can get a new one of these connections past you.
So it's maybe not a great plan in that sense.
AUDIENCE: Could we wrap the system call, the function [INAUDIBLE] to open a socket so that it faults the middleman instead of the socket that the users send out to [INAUDIBLE]? PROFESSOR: Yeah.",What is OKWS?
"So that's a correct model.
Now when I do a simulation, I could fill in the tables in either model, right? I'm sure you'd like to see a demonstration.
So let me show you a demonstration of that.
So there are the two tables.
And I can run 10,000 simulations on those guys, too.
Now, look.
The guy on the left is a pretty good reflection of the probabilities in a model I used to produce the data.
But the guy on the right doesn't know any better.
it just fills in its own tables, too.
So what to do? I say this one's the right model.
And you say that one's the right model.
Who's right? Maybe we'll never know.
And the guy on the left will get rich in the stock market and the guy on the right will go broke.
I would be nice if we could actually figure out who's right.
So would you to see how to figure out who's right? Yeah, so would I.
What we're going to do is we're going to look at naive Bayesian inference.
And that's our next chore.
So here's how it works.
We know, from the definition of conditional probability, we know that the probability of A given B is equal to the probability of A and B divided by the probability of B, right? Equal to by definition.
So that means that the probability of A given B times the probability of B-- I'm just multiplying it out-- it equal to that joint probability.
Oh, but by symmetry, there's no harm in saying I can turn that around and say that the probability of B given A times the probability of B is also equal to that joint probability, right? I've just expanded it a different and symmetric way.","What is the common benchmark for evaluating a Bayesian network? Thank you!
Can you use the same data both for estimation of the probabilities, and for model checking?"
"All you have to do is solve the equations.
There are the equations.
Good luck.
Why are they so complicated? Well because of the complicated geometry.
You notice we've got some products of theta 1 and theta 2 in there, somewhere, I think? You've got theta 2's.
I see an acceleration squared.
And yeah, there's a theta 1 dot times a theta 2 dot.
A velocity times a velocity.
Where the hell did that come from? I mean it's supposed to be f equals ma, right? Those are Coriolis forces, because of the complicated geometry.
OK.
So you hire Berthold Horn, or somebody, to work these equations out for you.
And he comes up with something like this.
And you try it out and it doesn't work.
Why doesn't it work? It's Newtonian mechanics, I said.
It doesn't work because we forgot to tell Berthold that there's friction in all the joints.
And we forgot to tell him that they've worn a little bit since yesterday.
And we forgot that the measurements we make on the lab table are not quite precise.
So people try to do this.
It just doesn't work.
As soon as you get a ball of a different weight you have to start over.
It's gross.
So I don't know.
I can do this sort of thing effortlessly, and I couldn't begin to solve those equations.
So let's see.
What we're going to do is we're going to forget about the problem for a minute.
And we're going to talk about building ourselves a gigantic table.
And here's what's going to be on the table.
Theta 1, theta 2, theta 3, oops, there are only two.
So that's theta 1 again, but it's the velocity, angular velocity.
And then we have the accelerations.
So we're going to have a big table of these things.
And what we're going to do, is we're going to give this arm a childhood.
And we're going to write down all the combinations we ever see, every 100 milliseconds, or something.","Can anyone please help me? 1. Regarding the Robotic Hand Solutions Table: If I understand correctly in the case of the robotic hand, we start from an empty table and drop a ball from a fixed height on the robotic hand. When the robotic hand feels the touch of the ball, we give a random blow as we record the robotic hand movements. Now, only if the robotic arm detects after X seconds that the ball has hit the surface again, it realizes that the previous movement was successful and records the movements it made for the successful result in the table for future use. I guess there is a way to calculate where on the surface the ball fell and then in case the robotic hand feels that the ball touched a region close to the area it remembers it will try the movement closest to these points in the table. Now there are a few things I do not understand: A. The ball has an angle, so that touching the same point on the board at different angles will lead to the need to use a different response, our table can only hold data of the desired point and effect and do not know the intensity of the fall of the ball or an angle, the data in the table will be destroyed or never fully filled ? B. How do we update the table? It is possible that we will drop a ball and at first when the table is empty we will try to give a random hit when the result of this is that the ball will fly to the side so we will not write anything in the table, now this case may repeat itself over and over and we will always be left with an empty table? It seems to me that I did not quite understand the professor's words and therefore I have these questions. I would be very happy if any of you could explain to me exactly what he meant by this method of solution. 2. In relation to finding properties by vector: If I understand correctly, we fill in the data we know in advance, and then when a new figure is reached, and we do not know much about it, we measure the angle it creates with the X line (the angle of the vector) and check which group is the most suitable angle. Now there is a point I do not understand. Suppose I have 2 sets of data, 1 group have data with very low Y points and very high X points and a second group having data with high X and Y points when I get a new data with a low Y and low X , the method of the vector angle will probably associate them with group 1 although it appears on paper that the point is more suitable for group 2. It seems that if we used a simple surface distribution here (as in the first case presented by the professor) we would get more accurate results than the method of pairing according to vectors angle?"
"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
PROFESSOR: All right everyone.
Let's get started.
So today's lecture and Wednesday's lecture, we're going to talk about this thing called object oriented programming.
And if you haven't programmed before, I think this is a fairly tough concept to grasp.
But hopefully with many, many examples and just by looking at the code available from lectures, you'll hopefully get the hang of it quickly.
So let's talk a little bit about objects.
And we've seen objects in Python so far.
Objects are basically data in Python.
So every object that we've seen has a certain type.
OK, that we know.
Behind the scenes, though, every object has these two additional things.","I understand what classes are.But what are the use of classes? What are the things that can be done in class but not in a normal function?
I get the idea but why and when do we should use OOP. Why don't we just use the existent classes such as tuples, lists, dictionaries? Any examples for comparision between normal programming and OOP?
This is a question I have had. I ""get"" OOP but what are real world examples of where OOP is beneficial over Procedural programming? Someone explaining that would be very helpful."
"RINI DEVADAS: We're going to do something quite different in terms of a puzzle.
And while it's a recursive puzzle, it's going to play out to be a recursive solution.
It looks quite different from the N-queens puzzle.
And so this is going to be a tiling puzzle.
And so I set up the puzzle, and then we're going to take a little segue off into a canonical recursive algorithm called merge sort because I think it's good for you to see that algorithm before you dive into solving this particular puzzle because it kind of shows you the way.
All right, and but I'll set up the puzzle.
I guess I didn't have to erase this square.
It's a courtyard that has a bunch-- of tiles.
You're supposed to tile it, and it's all square tiles.
And I'll just make it eight by eight like a chessboard.",the merge sort code shown only works on arrays are a length of a power of 2. Adding another condition for len(L) == 1 in mergeSort that returns a single element array and a 2 element array will take care of the general case. Otherwise you'll get stuck in an endless recursion loop since it can't handle the base case of a 3 element array.
"And this could be 10 levels deep.
It could be 40 levels deep, given that I've called things 40 levels in.
But it's just the one backtracks.
So as you can see, what backtracks does is anytime you have a valid location and you've gone ahead and-- essentially you've failed.
The reason it's out here is solve Sudoku did not return true.
When solved Sudoku actually returns false, that's when you come out and you increment backtracks.
So it meant that you had to do some undoing.
When you set grid IJ to be zero, that's when you're undoing your guess, right? So backtracks makes sense from a standpoint of I need to backtrack and go in a different fork in the road.
And so that's why I have backtracks plus equals one when I'm undoing my decision that I made.
So this kind of gives you a sense for how many wrong guesses that this program did.
And as you can imagine, the more the number of wrong guesses, the more the computation and the longer it takes.
So it is definitely a proxy for performance.
But it's a platform independent proxy that's more algorithm related as opposed to the speed of the computer.
Because if this computer were twice as fast, I mean I'd just see things running faster even though the algorithm isn't any better.
Right? That make sense? So it's a very simple use of global.
You don't want to use global variables except in certain constrained settings.
This is a fine use of global variables.
Cool, good.
So any questions about this code? So what I've done here is I just have the naive code.","Why does solveSudoku take i and j as args but then just overwrite them? I dont think you need those args.
Do there exist sudoku puzzles (specifically, steps of these puzzles) that can only be solved by the ""guess and backtrack"" method?
I've been solving a lot of sudokus lately, and one thing I've noticed that this brute force + implication method doesn't account for is a puzzle with the potential for a ""uniqueness"" problem. It's entirely possible that a sudoku with a unique solution might have a valid position along the way that presents a choice where one option would eventually lead to the correct solution and one (or more) of the other options would lead to a set of solutions where you could swap/rotate certain cells' values and still have a valid position by the algorithm's standards. With this naive approach, I wonder how you could build in a feature that detects uniqueness problems and discounts them from the set of valid positions in the solving path.
Ping Pong Cup Shots: Fair enough, I was mainly asking about whether there are any conditions on how the grid should be initially filled which guarantees your grid to be solvable, rather than having to attempt to solve it explicitly.
When I tried to run these codes on my laptop, the ""better"" solution with fewer backtracks always takes longer time than the unoptimized one. How to explain?
I was just thinking... how does the program handle multiple solutions? Seems it stops with the first solution."
"So the set of integers is a subset of the real numbers.
A real number is a special case of a complex number, so the real numbers are a subset of the complex numbers.
And here's a concrete example, where I have a set of three things, 5, 7, and 3, and this is the set with just the element 3 in it.
Now, we sometimes are sloppy about distinguishing the element 3 from the set that's consisting of just 3 as its only element.
But in fact, it's a pretty important distinction to keep track of.
In this case, 3 is not a subset of this set on the right.","Why is null set IMPROPER subset of all set?
Aint u jus repeating what the professor saying? :) What I am not understanding why u don't care? What kinda weird messy explanation is this? ...One part is false then u don't look at other one and jus say the whole thing is true. That rule comes out from some sorta bible or what... that whatever written there is true? : I perfectly understand why a null set can be assumed a subset of every set without knowing all this mess... but this kinda explanation I don't understand. Do we devise things for easier explanation or to complicate things to nonsense level? Why don't just assume null set has null element (nothing) and so any other set can be assumed to contain that... no? Isnt that how we learned it in prep school? ... Now we doing higher maths and it all goes ZOOOMMMMM! :) Isnt x+ 0 = x? that we know since elementary times.....So can be set .... Any Set + null element = Any Set ... I mean this could be another way of seeing it. "
"PATRICK WINSTON: What kind of classes? STUDENT: Physics.
STUDENT: Thermodynamics.
PATRICK WINSTON: Thermodynamics! The thermodynamicists are good at measuring disorder, because that's what thermodynamics is all about.
Entropy increasing over time, and all that sort of stuff.
There's another equally good answer.
STUDENT: Statisticians? PATRICK WINSTON: Statisticians.
Perhaps, but it's not the second best answer.
It's actually not even the best answer.
That's the best answer.
What's your name? STUDENT: Leo.
PATRICK WINSTON: Oh, yeah.
[LAUGHTER] PATRICK WINSTON: Leonardo has got his finger on it.
The information theorists are pretty good at measuring disorder, because that's what information is all about, too.
So we might as well borrow a mechanism for measuring the disorder of a set from those information theory guys.
So what we're going to do is exactly that.
Let's put it over here, so we'll have it handy when we want to try to measure those things.
The gospel according to information theorists is that the disorder, D, or some set is equal to-- now let's suppose that this is a set of binary values.
So we have positives and then we have negatives.
Pluses and minuses.
But pluses, they don't go very well in an algebraic equation, because they might be confused with adding.",What is a PDA?
"So this is starting to look kind of familiar because it borrows from other data structures.
And what this is I'm just going to substitute log n for k, and I got this kind of scary looking-- I was scared the first time I saw this.
Oh, this is n.
It's the log n-th root of n, OK? And so it's kind of scary looking.
But what is the log n-th root of n-- and we can assume that n is a power of two? AUDIENCE: 2.
SRINIVAS DEVADAS: 2, exactly.
It's not that scary looking, and that's because I'm not a mathematician.
That's why I was scared.
So 2 log n.
All right.
So that's it.
So you get a sense of how this works now, right? We haven't talked about randomized structures yet, but I've given you the template that's associated with the skip list, which essentially says what I'm going to have are-- if it was static data items and n was a power of two, then essentially what I'm saying is I'm going to have a bunch of items, n items, at the bottom.",Any idea how can I understand how log n is derived?
"And we somehow want to fiddle them all to maximize the prediction of context words.
And so the way we kind of do that then is we use calculus.
So if what we want to do is take that math that we've seen previously and say, well with this objective function, we can work out derivatives and so we can work out where the gradient is.
So how we can walk downhill to minimize loss.
So we're at some point and we can figure out what is downhill, and we can then progressively walk downhill and improve our model.","It seems this course is theory based, where can I learn to code these concepts and algorithms?
Objective function seeks to maximise the probable likelihood of context word given center word. However should it also not try to minimise the probability of incorrect context words given center word?"
"o given the insights so far, let's now go and design the most powerful graph neural network.
So let's go and design the most expressive, uh, graph neural network.
And let's develop the theory that will allow us, uh, to do that.
So the key observation so far is that the expressive power of a graph neural network can be characterized by the expressive power of the neighborhood aggregation function they use.
Because the more expressive the neighborhood aggregation leads to a more expressive graph neural network.
And we saw that if the neighborhood aggregation is injective, this leads to the most expressive, uh, GNN.
A neighborhood aggregation being injective, it means that whatever is the number and the features of the- of the children, you- you map every different combination into a different output, so no information get- gets lost.
So let's do the following next, let's theoretically analyze the expressive power of different, uh, aggregation functions.",Are there applications where one would prefer GCNs or GraphSage over the most expressive GINs (Graph Isomorphism Networks)?
"You just have to remember what all the letters mean.
But not too hard.
So that's pretty much all the classes we'll be using.
There are a few others.
I'm sorry.
I'll give you one example to go with our other examples.
The problem we'll look at today is Rush Hour.
This is a one player puzzle, where you're trying to move the cars, and they could only go vertically or horizontally.
This is in PSPACE, which is maybe not so obvious.
And actually, it's PSPACE-complete.
So this is from the diagram, PSPACE-complete is harder than NP-complete.
Now of course, we don't actually know whether these two points are the same.
It's kind of annoying.",It seems that rush hour is also in NP. You can easily check if a sequence of moves are valid and does move the desired block. What am I missing?
"What I've down here by this process of sorting each column and finding the median of medians is that I found this median of medians such that there's a bunch of columns on the left.
And roughly half of those elements in those columns are less than x.
And there are a bunch of columns on the right.
And roughly half of those columns have elements that are greater than x.
So what I now have to do is to do a little bit of math to show you exactly what the recurrence is.
And let me do that over here.
So that's the last thing that we have to do.
I probably won't solve the recurrence, but that can wait until tomorrow.","In median of median do we sort the ""medians"" row too? if not how can we guarantee those picture?
Where did 7n+7/10 + 7 come from in the end?
So is he finding the median of medians using the same approach again?
If we have n/5 columns then to find the median of medians we'll have to call the Select function on the list of medians. We're basically computing 1 subproblem that is 1/5th the size of the original. Hence, the T(n/5) term for finding the median of medians."
"That's the dead horse we don't want to beat.
There's no point in doing that calculation, because it can't figure into the answer.
The development of the progressive deepening idea, I like to think of in terms of the martial arts principle, we're using the enemy's characteristics against them.
Because of this exponential blow-up, we have exactly the right characteristics to have a move available at every level as an insurance policy against not getting through to the next level.
And, finally, this whole idea of progressive deepening can be viewed as a prime example of what we like to call anytime algorithms that always have an answer ready to go as soon as an answer is demanded.","+Daniel Jones So it is like breadth first search in a way, right ?
what does he mean by martial art example? Using your opponents force against him"
"And then I can collect that connected component and continue.
We have to be a little bit careful because, of course, we don't want like, checking things-- something to be visited to somehow take a bunch of time and make my algorithm slower than it needs to be.
But of course, we have a set data structure that we know can do that and order one time at least in expectation.
OK.
So this is the full DFS algorithm.
It's really simple.
Of DFS because I called DGS on every vertex.
And it's full because I looped over all the vertices.
Right.
And so if we think about it, how much time does this algorithm take? It's little bit sneaky because somehow I have a for loop over all the vertices.","Help me out!!!! I have doubt on the efficiency of BFS : Is it O ( |E| ) or O ( |V| ) + O ( |E| ) ??? You proved ,' recursive neighbors are reachable' number of comparisons operation : = O(1) * |deg+(recursive neighbors)| = O ( |deg+(recursive neighbors)| ) setting parent operation : = O(1) *| recursive neighbors| = O (| recursive neighbors|) efficiency of BFS : = O ( |deg+(recursive neighbors)| ) + O (| recursive neighbors|) In the worst case , efficiency of BFS : = O ( |V| ) + O ( |E| ) when each vertex can reach every other vertices ."
"We can go to 8, 6 or 7 and we pick one of them at random and move there.
Um, and this process continues, let say for a- for a- in this case for a fixed, uh, number of steps.
So the way you can think of this is that we basically simulated this, uh, random walk over- over this graph and let's say, um, over our fixed, uh, number of steps where the random walk can traverse the same edge multiple times, can return, can go back and forth, do, uh, whatever the random walk, uh, wants to do.
All right.
And this random walk is a seq- sequence of nodes visited this way, uh, on a graph across the- across the edges.
So now how are we going to- to define this notion of similarity and these probabilities that we talked about? What we are going to do is to say we want to learn these coordinate z such that the product of two, uh, nodes u and v, um, is similar or equals is, uh, approximates the probability that u and v co-occur on a random walk, uh, over the graph.
So here is- here is the idea, right? First we will need to estimate the probability of visiting node v on a random walk starting, uh, at some node u using some, let's say, a random walk strategy R.","why should theta between embeddings be directly proportional to co-occurrence of the two nodes? shouldn't the embeddings be close together if the nodes co-occur often implying similarity, not further apart? shouldn't it be inversely proportional, i.e. the more times they occur on the same path the closer they are in angle? I must be missing something obvious here."
"We lose.
It doesn't help.
You think you can make it, but you can't.
STUDENT: You could actually construct it as a 3-D object, though.
PATRICK WINSTON: He thinks you can construct it as a 3-D object.
Let me show you the next example, Christopher.
Consider this example.
Can you make that? Your intuition is yes.
So let's label it.
Oh, I've already lost.
We just boost that up a little bit to make the situation more clear.
So already, I've got myself in a situation where I can't label that.
But you feel like you can make it.
So what's wrong? What's wrong is-- what, Elliott? STUDENT: You have an obscured-- or, we're presuming that we have an obscured [INAUDIBLE] alley from the upper-left corner to the [INAUDIBLE].","What's ""stuff of the object"" ? I hate it when people use vague terms"
"If we print the type of C, this is actually going to give us class main coordinate, which tells us that C is going to be an object that is of type class coordinate.
If we look at coordinate as a class, if we print what coordinate is, coordinate is a class, right? So this is what Python tells us, if we print coordinate, it's a class named coordinate.
And if we print the type of a coordinate, well that's just going to be a type.
So class is going to be a type.
So you're defining the type of an object.
If you'd like to figure out whether a particular object is an instance of a particular class, you use this special function called is instance.
So if you print is instance C comma coordinate, this is going to print true because C is an object that is of type coordinate.
Couple more words on these special operators.
So these special operators allow you to customize your classes which can add some cool functionality to them.
So these special operators are going to be things like addition, subtraction, using the equal equal sign, greater than, less than, length and so on and so on.
So just like str, if you implement any of these in your classes, this is going to tell Python.
So for example, if we've implemented this underscore, underscore, add, underscore, underscore in our class, this is going to tell Python when you use this plus operator between two objects of type coordinate to call this method.
If you have not implemented this method and you try to add two objects of type coordinate, you're going to get an error because Python doesn't actually know right off the bat how to add two coordinate objects, right? You have to tell it how to do that.","What makes special operators special; I mean we can define them normally, right?
Will 'return' work instead of '__str__' ?
I understand what classes are.But what are the use of classes? What are the things that can be done in class but not in a normal function?
I get the idea but why and when do we should use OOP. Why don't we just use the existent classes such as tuples, lists, dictionaries? Any examples for comparision between normal programming and OOP?
Nice ! extremely grateful learn from anna ma'am. Well I have questions( time-----24.11) what is guarantee that use will provide X,Y in distance(self,other) for OTHER parameter.how he/she even know this ? Definitely One way by documentation but any other way .?"
"It says my problem is at least as hard as sorting.
But I already know how to solve sorting.
But if I start from an NP-complete problem, then I know, by the definition, that every problem in NP can be reduced to that problem.
And if I show how to reduce the NP-complete problem to me, then I know that I'm NP-complete too.
Because if I have any problem Z in NP, by the definition of NP-complete of Y I can reduce that to Y.
And then if I can build a reduction from Y to X, then I get this reduction.
And so that means I can convert any problem in NP to my problem X, which means X is NP-hard.
That's the definition.
So all this is to say the first time you prove a problem is NP-complete in the world-- this happened in the '70s by Cook.
Basically he proved that 3SAT is NP-complete.
That was annoying, because he had to start from any problem in NP, and he had to show that you could reduce any problem in NP to 3SAT.
But now that that hard work is done, our life is much easier.
And in this class all you need to think about is picking your favorite NP-complete problem.","can someone tell me at which point he proves that 3dm is np complete... and what is the proof exactly?
If we can't prove P = NP, can we prove P < NP?
Q on proving NP-completeness: if X is NP-Complete it belongs to both NP and NP-hard I understand that you have to show that X belongs to both sets. But if you have a reference NP-complete problem like 3SAT wouldn't it suffice to show that 3SAT is reducible to X? Why do you need step 1?
I believe the second statement in lecture is incorrect. It should be the other way around: if A is in NP, then B is in NP, because reduction implies that for A->B, A is at most as difficult as B. Otherwise, we can always convert from A to B and then solve it. As a result, this shows that if A is in NP, then it will not be P, which implies B will be in NP as well. In the meantime, I do not believe the second state: ""if B is in NP, then A is in NP"" is correct because I can easily reduce a polynomial time algorithm to a NP problem such as MBM into MVC.
If problem A can be converted to problem B(which is NP complete) in polynomial time then, what can we say about problem A? Is problem A, NP then? I think that nothing can be said about A. Can someone please confirm?
First of all, if we can reduce problem A to B, it means B is not easier than A. If as you said ""If A is in NP"", then we cannot say anything about B since B could be NP or NP-hard. Further, P is a subset of NP. if B is NP, then A could be P or NP. Therefore, it is still correct to say A is in NP. If B is P, then we know A must be P too. The whole point is that B is the upper bound of A. It is meaningless to say ""if A is xxx, then B is xxx , since B can be anything harder than A. If we say ""if B is xxx, then A is xxx"", this is meaningful as we know the worst case complexity for A. This is what the professor wrote.
It depends. If B is P, then most certainly A is P. If B is NP, then we can relax and say A is at most NP, even when A in truth is P, given P is a subset of NP (we can reduce P to NP). It gets tricky though when B is NP-complete. If B is NP-complete and A could be reduced to B, shouldn't that mean A is both NP and NP-hard and therefore, NP-complete? And isn't that why, in order to prove the completeness of A, we first ensure A is NP and then try to reduce a problem Z which is NP-complete to A (essentially declaring that A is either equal to or harder than Z, i.e. NP-hard)? Now, if B is NP-hard and one can reduce A to B, only here we cannot say what exactly A is, i.e., A could either be NP or NP-hard or both (NP-complete). Please correct me (with explanation) if I am wrong.
How does he conclude that rectangle packing is weakly NP-Hard? In general, Im a little confused about how hes concluding NP strength/if/how its transferred with reductions. It makes sense to me how he showed subset sum to be weakly NP-hard, and maybe why partition was also therefore weak (because it is reducible to finding a subset sum of t?). It would make sense to me that any time you can reduce a problem A to a weakly NP-Hard problem B, that you could conclude that A must not be strongly NP-Hard (and that if A is NP-hard it must be weakly so). However, with rectangle packing we only did the one way reduction from partition to it. This doesnt by itself guarantee that rectangle packing is weakly NP-Hard, does it? That is reducing a weakly NP-Hard problem A to some other problem B doesnt guarantee that B is also weakly NP hard, does it? Otherwise every NP-Complete problem would be weakly NP hard, because some weakly NP Hard problem reduces to it, and clearly some NP-Complete problems are strong NP-Hard. Now that I think back on it, an NP-Hard A > weakly NP-Hard B reduction also doesnt seem to imply that A is weakly NP-Hard, because any weakly NP-Hard problem being NP-Complete would imply all NP-Complete problems were weak, another contradiction. So how is he determining strength or weakness of these problems? Is it something Im missing or just a detail hes glossing over?
I can't fully understand that the professor stated that ""X is NP-hard if every problem y  NP reduces to X"". Doesn't it mean that a problem(NP) would then reduce to a harder problem(NP-hard)? How should I interpret it? Thanks!
Wait, so you are proving not that the game super mario is NP-hard but that you can construct NP-hard levels in mario? The fact that people have said Super Mario is NP-hard has always confused me.
As far as I understood. 1) If B is in P, then A is also in P : This is saying that if problem A can be converted to problem B in poly-time and if problem B can be solved in poly-time then it follows that you can also solve A in poly-time. 2) If B is in NP, then A is also in NP: This is saying that if problem A can be converted to problem B in poly-time and if problem B belongs to NP i.e. its solution can be verified in poly-time, then A must also belong to NP. This doesn't mean that A cannot be faster than NP. Problem A could belong to P while also belonging to NP, as P is in NP.
Note that P is a subset of NP, not the complement of NP. Thus the contrapositive would be (A not in P) -> (B not in P), not what was claimed above. If we think of the difficulty graph illustrated by the presenter, then problem A reduces to problem B amounts to A is at or to the left of B. So (B in NP) -> (A in NP) is accurate.
If we have a problem A that is in NP and we reduce it to B then we know that B is NP. if we dont know if A is NP and we know that B is in NP are we sure that A is in NP?Could'nt be an algorithm that do not use this reduction but solves the A problem? is the second statement in https://youtu.be/eHZifpgyH_4?t=1426 correct?
Could someone please help me understand what did he mean by, ""Computer would guess an option from polynomially many options""? I am not able to think(and relate) of an example when an algorithm guesses while solving a decision-based problem. e.g. while determining a chess move. His next statement only adds to my confusion, ""The guess is guaranteed to be a good one"" :(
Reduction ... Time point https://youtu.be/eHZifpgyH_4?t=22m28s . Chalkboard writing .. then A element of P ... seems to disagree with the notes from the MIT site ... which is written A element of NP ... . Are the notes incorrect?
Whether something is in P or NP, that something must be a decision problem. It does not make sense to talk about whether an algo is in P or NP. For example, we can talk about the problem ""Is 73642387 a prime?"" and ask whether it's in P or NP. However, we can't talk about whether a MergeSort algo is in P or NP becus it's an algo, not a decision problem. Here's an explanation by Alon Amit regarding this confusion: https://www.quora.com/Is-finding-prime-numbers-in-P-or-NP Hope this helps :)"
"If delta s,v equals minus infinity-- so I guess this is, run single source shortest paths from s.
And really, because this graph could contain negative edge weights and could contain negative cycles, we can't really do better than running Bellman-Ford here from s to compute these paths.
If there exists in this new graph this Gs, if there exists a vertex that has negative infinite weight in the reweighted graph-- sorry, in the original graph G-- G hasn't been reweighted yet.
If there's a negative weight distance from s, then there was a negative weight cycle in the original graph.
Why is that? Well, if this was set to minus infinity, then there is some negative weight cycle in the graph.","doesn't connecting all vertices in V to a supernode s with 0 weight edges makes all delta(s, v) to 0, (and makes all h(v) = 0)? Or is the s in the Johnson's algorithm another s different from the supernode"
"And it is because what I do to this pin doesn't depend too much to what I do to a pin much later.
There's a lot of intuition going on here for what-- when DP works.
But we're going to see a lot more examples of that coming up.
And I just want to mention the intuition for how to write a recurrence like this is to think about-- in the case of suffixes, you always want to think about the first item, or maybe the first couple of items.
The case of prefixes, you always think about the last item.
And for substrings, it could be any item-- maybe in the middle.
If I remove an item from the middle of a substring, I get two substrings, so I can recurse.
Here or in general, what we want to do is identify some feature of the solution that if we knew that feature we would be done.
We would reduce to a smaller sub problem.
In this case, we just say, well, what are the possible things I could do to the first pin? There are three options.
If I knew which option it was, I would be done.
I could recurse and do my addition.
Now, I don't know which thing I want to do.
So I just try them all and take the max.
And if you're maximizing, you take the max.
If you're minimizing, you take the min.","Question: is the proposed algorithm optimal when pins are -1,-1,-9,1? In the first step it will choose to hit first and second pins -1 and -1. On the second step the algorithm will choose to skip -9. On the third step the algorithm will hit 1. Overall score will be 2 if we use the algorithm. But optimal solution is to hit the second and the third pins -1 and -9, and then hit the last, so the score would be 10. Or I misunderstood something?"
"So what happens here? So let's analyze this to make sure that we're good in terms of an improvement.
We have to be careful.
So let's just arbitrarily say that it broke at 64, because that's kind of what happened over in that previous algorithm too.
So that means I've done four drops without a break.
I'm sorry, four drops.
Three drops without a break and four drops total.
So I've done four drops.
But now the cool thing is I don't have to start from 16.
I know it didn't break at 48.
So I can now look at four drops till break of first ball.
That is what I wanted to say.
And now, I can look at the interval 49.
Because 48, it did not break-- 49 to 63, inclusive.
And so if you do that, that's 15.
It's 15.
The difference is 14, but it's inclusive, so it's 15.
So 15 plus 4, that is 19 drops.
Yeah.
Back there.
AUDIENCE: Wouldn't the worst case though be when it's at 127.
SRINI DEVADAS: Yes, that's absolutely correct.
So 19 is not the worst case.
I mean it's looking good, but this was exactly the point I was trying to make right from the beginning.
You are not done yet.
I mean, you cannot just go off and say that 19 is the worst case for this algorithm.
The worst case for this algorithm-- this particular algorithm occurs at a different point from the original algorithm that went midway.
Every algorithm is different.
The parameters are different here.
And so I'm just calling a different algorithm.
But the case that would happen is I guess you would have 112 here.
And then you'd go 1, 2, so how many? So let's say it gets to 128.
And if it doesn't break, then you're clearly done.
So that's not the worst case.
So if it gets to 128 and it breaks, then so how many drops have we done? 128 divided 16 is what? Is 8.","What is the worst case if you have 128 balls?
How is binary search not the answer??? Drop a ball at 64. If it breaks, drop a ball at 32. If it doesn't break drop a ball at 96. Is it a rule that you have to pick an interval k? If so, I didn't pick up on that. With binary search, it the maximum drops would be log base 2 of n + 1. That would be 8 in the case of 128."
"The basic idea here is that if I take the k-th vertex in the topological order, assuming that these distances are all equal for the ones before me in the topological order, I can prove by induction.
We can consider a shortest path from s to v, the k-th vertex, and look at the vertex preceding me along the shortest path.
That vertex better be before me in the topological order or we're not a DAG.
And we've already set its shortest path distance to be equal to the correct thing by induction.
So then when we processed u-- s to u to v-- when we processed u in DAG relaxation here, processed the vertex and looked at all its outgoing adjacencies, we would have relaxed this edge to be no greater than that shortest path distance.
So this is correct.
You can also think of it as the DAG relaxation algorithm for each vertex looks all at its incoming neighbors, assuming that their shortest path distances are computed correctly already.
Any shortest path distance to me needs to be composed of a shortest path distance to one of my incoming neighbors through an edge to me, so I can just check all of them.","great lecture, thanks mit! I have a doubt, in the dag relaxation algo we init the distance estimate from the source to the source equal to 0, but the professor said that this value could negative or -infinity, however that can occur when the graph has a negative cycle, given that the graph is a DAG then the shortest path distance from the source to the source has to be 0 because DAG has no cycles
but I think d(s,v)>delta(s,u)+w(u,v) can be correct if vertex u comes before v in topological order.because d(s,u) is the same as delta(s,u) as there is no subroute that can reach vertex u from vertex s going through any vertex that come after u in the topological order. am I right?"
"But of course, once you go to n sequences, you can't afford this.
You would an n to the n behavior, so that's a limit to how far you could go.
Two sequences is fine, three sequences is fine, but n sequences-- there probably is no polynomial time algorithm for this problem.
Cool-- I want to show you an example.
I have an example here.
I didn't want to try out hieroglyphology versus Michelangelo, so I came up with another example.
Their habit is to say hi.
So the longest common subsequence of there and habit is HI.
And it's a giant-- well, not that giant-- it looks kind of like a grid graph.
The base cases are out here, because those correspond-- each of these nodes is a subproblem, and this corresponds to, what is the longest common subsequence between EIR and ABIT? And it should be I.
It's the only letter they have in common, and that's why there's a 1 here to say that the longest common subsequence has a 1-- has size 1.",Hard time understanding relation between graphs and answer of problems
"Linear time to do an insertion is clearly bad.
This is all during one insertion operation that this would happen, but overall it's not going to be bad, because you only double log n times.
And if you look at the total cost-- so maybe you think, oh, is it log n per operation, but it's not so bad because total cost for n insertions starting from an empty structure is something like 2 to the 0-- this is a big theta outside-- 2 to the 1, 2 to the 2.
If we're only doing insertions, this is great.
2 to the log n.
This is a geometric series and so this is order n.
Theta head I guess.
So to do n insertions, cost theta n, so we'd like to say the amortized cost per operation is constant, because we did n operations.
Total cost was n, so sort of on average per operation, that was the only constant.
So this is the sense in which hash tables are constant, expected, amortized.","I was wondering about that. In his analysis table doubling would have increased the runtime complexity, this makes more sense."
"And now, right, I- basically the idea is that now I can go and update the summaries because some labels might have changed.
I update the summary and reapply the Phi 2 predictor- Phi 2 classifier.
And I keep doing this, basically reclassifying nodes with Phi 2 until the process converges.
Right? So I'll be updating this until- until the label prediction stabilize.
I think because if our prediction for a given node changes, then its vector z is also going to change.
And if the vector z is going- changes then the- I have to apply the predictor Phi 2 to update the belief or the prediction about the label of a given node.
And I keep iterating this until the process stabilized, right? So here the prediction for this node flipped from 0 to 1.
Now this vectors z have changed, have been updated.
So I have to re-update my classifier Phi 2 again over all these nodes.
And I keep doing this until the process converges meaning until no node labels change, or until some maximum number of iterations is reached.
And that is essentially the idea of what we are trying to do here and how do we arrive to the final prediction.
So to summarize, so far, we have talked about two approaches to collective classification.
First, we talked about relational classification, where we are iteratively updating probabilities of nodes belonging to a labeled class based on the labels of its neighbors.","What is the point on the feature-only classifier phi-1 ? Professor never explained this. Just train one model, Phi-2, which uses features and neighbors and use regularization. One model is enough. Use XG-Boost. You don't even need to iterate. Train on the known labels and predict on the unknown. I believe that a data scientist would never do this method because it is so unnecessary with extra programming work for no benefit.
Thank you for the lecture! I haven't come across methods such as Iterative classification before. The idea - using a separately trained classifier in an iterative non-trainable process - sounds a bit unnatural to me. My intuition tells me it's better to train the classifier inside each iteration, thus making the iterative process trainable. I suppose, that's when is going to happen in GNNs. A couple of comments about the training set. We don't need any graph structure to train phi1 classifier, therefore it can be done with traditional methods. In order to train phi2 classifier, we need labeled nodes that have all their neighbours also labeled - that's necessary to set z_v. We don't necessary need to split all graphs into training and test sets. Instead, for phi2 training we can use all the nodes with mentioned property - as it's not going to be evaluated during the iterative stage anyway (as the labels are known - and thus 'converged' already).
phi1 is required at iteration 0 to create the initial labels at inference. In later iterations, only phi-2 is needed."
"So, let's carry on and see if we can complete this equal to or less than 8, equal to 8, equal to 8-- because the other branch doesn't even exist-- equal to or less than 8.
And we compare these two numbers, do we keep going? Yes, we keep going.
Because maybe the maximizer can go to the right and actually get to that 8.
So, we have to go over here and keep working away.
There's a nine, equal to or less than 9, another 9 equal to 9.
Push that number up equal to or greater than 9.
The minimizer gets an 8 going this way.
The maximizer is insured of getting a 9 going that way.
So, once again, we've got a cut off situation.
It's as if this doesn't exist.
Those static evaluations are not made.","I have a few questions. The first one is, is Minimax considered as a state space search? If it is is there a Goal state/node?
I find something here about alpha & betha, what if we're changing the position between 3 and 9 on the left tree...then the first cut off wouldn't happen... so the interesting thing is the alpha betha depended on the evaluation method... For example if you're doing evaluation from the right position so the cut-off will be different :D ... anyway thank you for the explanation... it's really clear
I got thrown off a little on the alpha beta part. So at each level we when we make comparisons do we look at the values from both the min and max perspective?"
"And you can build a tree that's as simple as possible if you look at the data, and say, well, which test does the best job of splitting things up? Which test does the best job of building subsets underneath it that are as homogeneous as possible? So all this information theory, all this entropy stuff, is just a convenient mechanism for doing something that is intuitionally sound.
OK? It's not about information theory.
It's about a sound intuition.
Oh, by the way.
Does this kind of stuff ever get used in practice? 10s of thousands of times.
This is a winning mechanism that's used over and over again, even when the data is numeric.
How would it work if it's numeric data? Well, let's think about that for a little bit.
So let's suppose that we have an opportunity.
We're an EMT or something, we work in the infirmary.
What do they call it these days? Something else.
But anyhow, you work in that kind of area, and you have the opportunity to take people's temperature.
And so over time, you've accumulated some data on the temperature of people.
And maybe you've found that there's a vampire here at about 102.
There's a normal person here, about 98.6.
But then they're scattered around.
Some people have fevers when they come in.
So the question is, is there a way of using numerical data-- things that you can put real numbers-- is there a way of using that with this mechanism? And the answer is yes.
You just say, is the temperature greater than or less than some threshold? And that gives you a test, a binary test, just like any of these other tests.
[? Krishna? ?] Right? But where would I put the threshold? I suppose I could just put it at the average value.",Why does he make the difference between Identification tree and decision tree?
"But the comma here makes it clear to Python that this is a tuple with only one element in it.
We can slice even further to get a tuple with two elements.
And we can do the usual operations like get the length of a tuple, which says, how many elements are in my tuple? And len of this t would evaluate to 3, because there are three elements inside the tuple.
Each element, again, separated by the comma.
And just like strings, if we try to change a value inside the tuple-- in this case, I wanted to try to change the value of the second element to 4-- Python doesn't allow that, because tuples are immutable.","I have a confusion, If tuples are immutable how does nums = nums + (t[0],) work?
it's concatenating an empty tuple to a singleton tuple (and on and on as the for loop runs). Consider; nums = 'walk' nums = nums + 'ing' print(nums) will output walking. 'Walk' and 'ing' are immutable as strings, but they can be concatenated together. In the same way a tuple can be concatenated to a tuple. nums = (1,2) nums = nums + (3,4) print(nums) will output (1, 2, 3, 4) What it's not doing is changing an element within a tuple, as tuples are immutable.
I am curious as to the advantage of using a tuple over a list. It seems as though lists have the exact same functionality as tuples but with the added advantage of you being able to modify them(mutability) if you so choose to. Why not just alway use lists?"
"ATRICK WINSTON: I have extremely bad news.
Halloween falls this year on a Sunday.
But we in 6.034 refuse to suffer the slings and arrows of outrageous fortune.
So we've decided that Halloween is today, as far 6.034 is concerned.
Kenny, could you give me a hand, please? If you could take that and put it over there.
STUDENT: [INAUDIBLE]? PATRICK WINSTON: Mm hm.
Just give it to them.
You can take as much of this as you like.
The rest will be given to that herd of stampeding freshman that comes in after.
It's a cornucopia of legal drugs.
Chocolate does produce a kind of mild high, and I recommend it before quizzes and giving lectures.
I have a friend of mine, one of the Nobel laureates in biology, always eats chocolate before he lectures.
Gives him a little edge.
Otherwise, he'll be flat.
So I recommend it.
It will take, I suppose, a little while to digest that neural net stuff.
A little richer than usual in mathematics.
So today, we're going to talk about another effort at mimicking biology.
This is easy stuff.
It's just conceptual.
And you won't see this on the next quiz.
But you will see it on the final.
It's one of those quiz five type problems, where I ask you questions to see if you were here and awake.
So a typical question might be, Professor Winston is a creationist, or something like that.
Not too hard to answer.
In any event, if it's been hard to develop a real understanding of intelligence, occasionally the hope is that by mimicking biology or mimicking evolution, you can circumnavigate all the problems.
And one of those kinds of efforts is ever to imitate evolution.
So we're going to talk today about so called genetic algorithms, which are naive attempts to mimic naive evolution.","Hey guys, I might be a bit thick here, but what does the professor mean when he says near the end of the lecture -> ""We were amazed by the SPACE of solutions ... and not by the GENETIC algorithms'? Any further explanation is welcome :)
outstanding teacher. Thanks a lot. But can anybody explain it's real life application."
"You still need to run uniform cost search or dynamic programming.
In this case, if you decide to run uniform cost search, remember, uniform cost search computes past costs.
In this case, I really wanna compute future costs.
So you need to do a bunch of engineering to get that working.
In this particular case, the relaxed problem, you need to reverse it.
Because when you reverse it, past costs of the reversed relax problem becomes future cost of the relaxed problem, if that makes sense.
So, so the way I'm reversing this is I'm basically saying start state is n.
End state is 1, and my walk action takes me to s minus 1, instead of s plus 1, and my tram action takes me to s over 2 instead of S times 2, and the whole reason I'm doing that is- is that the past cost of this new problem is the future costs of the non-reversed version.
Okay.
Because I, I need to use uniform cost search here, okay? So I run my uniform cost search, that gives me a heuristic, and that heuristic gives me this future cost of the relaxed problem, and everything will be great.
Another example is, I can have independent subproblems using my heuristic.
So in this case, like we have these tiles, they technically cannot overlap.
Instead, what we are allowing is, you're allowing them to overlap.
So if we allow them to overlap, I have eight independent subproblems that I can solve.
These subproblems give me heuristics, and I can just go with them, okay? So, so these were just a bunch of examples, and kind of the key idea was reducing edge, li- like when we are coming up in these relaxed problems, we're reducing edge costs from infinity to some finite costs.",would not removing constraint increase search space making computationally inefficent?
"Then you get to the next clause.
You need another star for each one.
Conversely, if there is a satisfying assignment, you can actually play through the level, you just make these choices according to what the satisfying assignment is.
So either way it's equivalent.
We always get a yes or no answer here whenever we get a corresponding yes or no answer to the 3SAT process.
You also need to check that this reduction is polynomial size.
It can be computed in polynomial time.
So there's an issue.
Given this thing, you have to lay this out in a grid and draw all these wires.
And there's one problem here, which is, these wires cross each other.
And that's a little awkward, because these wires are basically just long tunnels for Mario to walk through.
But what does it mean to have a crossing wire? Really, if Mario's coming this way, I don't want them to be able to go up here.
He has to go straight.
Otherwise this reduction won't work.
So I need what's called a crossover gadget.
And everywhere here I have a crossing, I have a crossover.
And this gadget has to guarantee that I can go through one way or the other way, but there's no leakage from one path to the other path.
Actually, if I first traverse through here, and then I traverse through here, it's OK if I leak back.
Because once I visit a wire, it's kind of done.
But I can't have leakage if only one of them is traversed.
So this is the last gadget, the most complicated of them all.
So this took a while to construct, as you might imagine.","can someone explain the mario and clause reduction to me, or provide a link for theory.
Why do we need poly size certificates here ?
why is there no x_5 or x_6 in the 3SAT example?
Why create the last crossover gadget instead of using pipes to move you where you need to be? Or was the spirit of the exercise to use the most basic implementation of Mario possible? Is the whole goomba/mushroom thing better? (Also, the final mushroom check can be a small tunnel of fire vines short enough for the hit invincibility to last, but full enough that small Mario can't pass.)
i got lost once he started drawing the wheel. can anyone explain?
Wait, so you are proving not that the game super mario is NP-hard but that you can construct NP-hard levels in mario? The fact that people have said Super Mario is NP-hard has always confused me."
"I'm not even going to try that one.
But what I'm trying to get at is that you can use comparisons of previous and future time steps in order to attempt to determine the magnitude of the pole if you're working with the first order system.
If you're working with the second order system, then it's possible that you'll see some really interesting initialization effects.
And you should probably ask one of us what's up.
But for this example, we're going to put our pole over here.
Here's the last graph I want to talk about.
The first thing that I notice is that it doesn't seem to be diverging, but it doesn't really seem to be converging either.",How do you get .07 from 1.6 and .09 from -.63? I have tried looking this up but can't figure it out.
"Maybe i is in the longest increasing subsequence, or it's not in.
So the question I would like to answer is, is i in the longest increasing subsequence of A-- of A from i onwards? This is a binary question.
There are two options-- so again, just like before.
And so I can brute force those two options.
And then I want the longest one, so I'm going to take the max.
So I'd like to take the max of something like Li plus 1.
So in the case that I don't put i in the solution, that's fine.
Then I just look at i plus 1 on, and recursively compute that, and that would be my answer.
And the other option is that I do put i in the longest increasing subsequence, so I do 1 plus the rest L i plus 1.
If I close this brace, this would be a very strange recurrence, because this is always bigger than this one.","I'm still not quite clear about the LCS algo. If we are in the else case (so if A[i] != A[j]) and it turns out that both cases in the max-fkt equally end up in that else case, how does it continue?
What is the difference between the DP solution to LIS in this video and BruteForce?"
"So if you have a reduction like this and if say, B, has a polynomial time algorithm, then so does A, because you can just convert A into B, and then solve B.
Also this works for nondeterministic algorithms.
Not too important.
So what this tells us is that in a certain sense-- get this right-- well this is saying, if I can solve B, that I can solve A.
So this is saying that B is at least as hard as A.
I think I got that right, a little tricky.
So if we want to prove the problem is NP hard, what we do is show that every problem in NP can be reduced to the problem of X.
So now we can go back and say well, if we believe that there is some problem Y, that is in NP minus P, if there's something out here that is not in P, then we can take that problem Y, and by this definition, we can reduce it to X, because everything in NP reduces to X.
And so then I can solve my problem Y, which is in NP minus P, by converting it to X and solving X.","If we can't prove P = NP, can we prove P < NP?
Q on proving NP-completeness: if X is NP-Complete it belongs to both NP and NP-hard I understand that you have to show that X belongs to both sets. But if you have a reference NP-complete problem like 3SAT wouldn't it suffice to show that 3SAT is reducible to X? Why do you need step 1?
I believe the second statement in lecture is incorrect. It should be the other way around: if A is in NP, then B is in NP, because reduction implies that for A->B, A is at most as difficult as B. Otherwise, we can always convert from A to B and then solve it. As a result, this shows that if A is in NP, then it will not be P, which implies B will be in NP as well. In the meantime, I do not believe the second state: ""if B is in NP, then A is in NP"" is correct because I can easily reduce a polynomial time algorithm to a NP problem such as MBM into MVC.
If problem A can be converted to problem B(which is NP complete) in polynomial time then, what can we say about problem A? Is problem A, NP then? I think that nothing can be said about A. Can someone please confirm?
First of all, if we can reduce problem A to B, it means B is not easier than A. If as you said ""If A is in NP"", then we cannot say anything about B since B could be NP or NP-hard. Further, P is a subset of NP. if B is NP, then A could be P or NP. Therefore, it is still correct to say A is in NP. If B is P, then we know A must be P too. The whole point is that B is the upper bound of A. It is meaningless to say ""if A is xxx, then B is xxx , since B can be anything harder than A. If we say ""if B is xxx, then A is xxx"", this is meaningful as we know the worst case complexity for A. This is what the professor wrote.
It depends. If B is P, then most certainly A is P. If B is NP, then we can relax and say A is at most NP, even when A in truth is P, given P is a subset of NP (we can reduce P to NP). It gets tricky though when B is NP-complete. If B is NP-complete and A could be reduced to B, shouldn't that mean A is both NP and NP-hard and therefore, NP-complete? And isn't that why, in order to prove the completeness of A, we first ensure A is NP and then try to reduce a problem Z which is NP-complete to A (essentially declaring that A is either equal to or harder than Z, i.e. NP-hard)? Now, if B is NP-hard and one can reduce A to B, only here we cannot say what exactly A is, i.e., A could either be NP or NP-hard or both (NP-complete). Please correct me (with explanation) if I am wrong.
How does he conclude that rectangle packing is weakly NP-Hard? In general, Im a little confused about how hes concluding NP strength/if/how its transferred with reductions. It makes sense to me how he showed subset sum to be weakly NP-hard, and maybe why partition was also therefore weak (because it is reducible to finding a subset sum of t?). It would make sense to me that any time you can reduce a problem A to a weakly NP-Hard problem B, that you could conclude that A must not be strongly NP-Hard (and that if A is NP-hard it must be weakly so). However, with rectangle packing we only did the one way reduction from partition to it. This doesnt by itself guarantee that rectangle packing is weakly NP-Hard, does it? That is reducing a weakly NP-Hard problem A to some other problem B doesnt guarantee that B is also weakly NP hard, does it? Otherwise every NP-Complete problem would be weakly NP hard, because some weakly NP Hard problem reduces to it, and clearly some NP-Complete problems are strong NP-Hard. Now that I think back on it, an NP-Hard A > weakly NP-Hard B reduction also doesnt seem to imply that A is weakly NP-Hard, because any weakly NP-Hard problem being NP-Complete would imply all NP-Complete problems were weak, another contradiction. So how is he determining strength or weakness of these problems? Is it something Im missing or just a detail hes glossing over?
I can't fully understand that the professor stated that ""X is NP-hard if every problem y  NP reduces to X"". Doesn't it mean that a problem(NP) would then reduce to a harder problem(NP-hard)? How should I interpret it? Thanks!
Wait, so you are proving not that the game super mario is NP-hard but that you can construct NP-hard levels in mario? The fact that people have said Super Mario is NP-hard has always confused me.
As far as I understood. 1) If B is in P, then A is also in P : This is saying that if problem A can be converted to problem B in poly-time and if problem B can be solved in poly-time then it follows that you can also solve A in poly-time. 2) If B is in NP, then A is also in NP: This is saying that if problem A can be converted to problem B in poly-time and if problem B belongs to NP i.e. its solution can be verified in poly-time, then A must also belong to NP. This doesn't mean that A cannot be faster than NP. Problem A could belong to P while also belonging to NP, as P is in NP.
Note that P is a subset of NP, not the complement of NP. Thus the contrapositive would be (A not in P) -> (B not in P), not what was claimed above. If we think of the difficulty graph illustrated by the presenter, then problem A reduces to problem B amounts to A is at or to the left of B. So (B in NP) -> (A in NP) is accurate.
If we have a problem A that is in NP and we reduce it to B then we know that B is NP. if we dont know if A is NP and we know that B is in NP are we sure that A is in NP?Could'nt be an algorithm that do not use this reduction but solves the A problem? is the second statement in https://youtu.be/eHZifpgyH_4?t=1426 correct?
Could someone please help me understand what did he mean by, ""Computer would guess an option from polynomially many options""? I am not able to think(and relate) of an example when an algorithm guesses while solving a decision-based problem. e.g. while determining a chess move. His next statement only adds to my confusion, ""The guess is guaranteed to be a good one"" :(
Reduction ... Time point https://youtu.be/eHZifpgyH_4?t=22m28s . Chalkboard writing .. then A element of P ... seems to disagree with the notes from the MIT site ... which is written A element of NP ... . Are the notes incorrect?
should NP be the set of decision problem that can be solved in polynomial time with Non-deterministic machine, or, the set of decision problems whose ""TRUE"" instances can be solved in polynomial time with ND machine. if the former, how is it different from co-NP
Whether something is in P or NP, that something must be a decision problem. It does not make sense to talk about whether an algo is in P or NP. For example, we can talk about the problem ""Is 73642387 a prime?"" and ask whether it's in P or NP. However, we can't talk about whether a MergeSort algo is in P or NP becus it's an algo, not a decision problem. Here's an explanation by Alon Amit regarding this confusion: https://www.quora.com/Is-finding-prime-numbers-in-P-or-NP Hope this helps :)
This may clear things up: a non-deterministic Turing machine can solve a problem with an exponential number of options in polynomial time, and it can solve a problem with polynomial options in constant time. In his definition, of NP he's referring to the later case, where a deterministic machine would have to check through each of N^k possibilities, but an non-deterministic machine could guess which of the N^k possibilities is the right one. You can think of this process as occurring at each branching of an exponential problem, where there could be up to N^k branches to check. The non-deterministic machine knows which branch to take (and makes this choice N^k times moving down the tree), whereas the deterministic one has to check every branch."
"Right? So this might have led people to think like well, it shouldn't change because they are independent.
So why should the probability change? But the key thing is that when you condition on A, you actually changed, uh, the independent structure of the model.
So this is why writing things down really precisely is helpful to kind of reconcile these seemingly, um, contradictory intuitions that you might get.
[NOISE] Okay, any questions about this? [NOISE] All right, let's move on.
So we've talked about the alarm network.
This is your first example of a small Bayesian network.
Hopefully, you have an idea of the intuition behind this and now I'm going to generalize it.","In 25.54 we are told that B and E are independent. But shouldn't we have specified that B and E are independent given A, or am I missing some major point? Because since we aggregate probabilities against hidden variables so this shrinks the expressivenes of our knowledge.
B and E independent is a less general claim than saying they are independent given A. Saying that given A they are independent it implies A is a confounding variable so fixingconditional on A we can state that they are now variable. Like high blood pressure and the chance of me going to McDonalds are dependent but independent if someone is in a place with no mcdonalds
No idea. I feel like it should just be 0.5. Both mathametically and intuitively, if there is an alarm and we have no other information, and the probability of an earthquake = probability of burgalary, then it should be equally as likely for it to be an earthquake that triggered the alarm as it is a burgalary to trigger the alarm."
"So a change in T could cause this thing to shift over that way.
And if we change the value of w, that could change how steep this guy is.
So we might think that the performance, since it depends on w and T, should be adjusted in such a way as to make the classification do the right thing.
But what's the right thing? Well, that depends on the samples that we've seen.
Suppose, for example, that this is our sigmoid function.
And we see some examples of a class, some positive examples of a class, that have values that lie at that point and that point and that point.
And we have some values that correspond to situations where the class is not one of the things that are associated with this neuron.
And in that case, what we see is examples that are over in this vicinity here.
So the probability that we would see this particular guy in this world is associated with the value on the sigmoid curve.
So you could think of this as the probability of that positive example, and this is the probability of that positive example, and this is the probability of that positive example.
What's the probability of this negative example? Well, it's 1 minus the value on that curve.
And this one's 1 minus the value on that curve.","at can any one explain why local maxima can be turn into saddle point?
a question what does he mean by positive and negative examples?"
"OK, so there are a few details to get right, but in general, once you figure out what these recurrences look like, they're very simple.
This is one line of code, and then all you need in addition to this is the original subproblem and some other things.
We need the base cases, but I should do them in order.
Topological order is just the usual for loop, because I'm doing suffixes.
It's going to be i equals length of A down to 0.
Base case is going to be L of length of A, which is 0, because there's no letters in that suffix.","Is the topology reversed when you go from top down vs bottom up? I'm a bit confused by the topology. Are suffix subproblems always have topology of decreasing i ?
The topoligical order of LIS is for i in 0 to |A| ?"
"So I like to reinforce that by giving you a little puzzle.
Let's see, who's here? I don't see [? Kambe, ?] but I'll bet he's from Africa.
Is anyone from Africa? No one's from Africa? No? Well so much the better-- because they would know the answer to the puzzle.
Here's the puzzle.
How many countries in Africa does the Equator cross? Would anybody be willing to stake their life on their answer? Probably not.
Well, now let me repeat the question.
How many countries in Africa does the Equator cross? Yeah, six.
What happened is a miracle.
The miracle is that I have communicated with you through language, and your language system commanded your visual system to execute a program that involves scanning across that line, counting as you go.
And then your vision system came back to your language system and said, six.
And that is a miracle.
And without understanding that miracle, we'll never have a full understanding of the nature of intelligence.
But that kind of problem solving is the kind of problem solving I wish we could teach you a lot about it.
But we can't teach you about stuff we don't understand.
We [INAUDIBLE] for that.
That's a little bit about the definition and some examples.
What's it for? We can deal with that very quickly.
If we're engineers, it's for building smarter programs.
It's about building a tool kit of representations and methods that make it possible to build smarter programs.","why 2^4 possibilities in F Fx G Gn problem?
Hi, can someone please help me understand why there can be 16 possible constraints for the farmer problem. Each time the farmer cross the river, he can be one of his possession so I think there can be 3.
what was that gyroscope concept... can someone please explain, I didn't get it ."
"We're testing the middle point.
And based on that, we're giving a call where, now, the pointer is to the beginning and the middle of the list, simply passing it down, and same as I go through all of these pieces.
So that code now gives me what I'd like.
Because here, in the previous case, I had a cost.
The cost was to copy the list.
In this case, it's constant.
Because what am I doing? I'm passing in three values.
And what does it take to compute those values? It's a constant amount of work, because I'm simply computing mid right there, just with an arithmetic operation.
And that means order log n steps, because I keep reducing the problem in half.
And the cost at each point is constant.
And this is, as a consequence, a really nice example of a logarithmic complexity function.
Now, if you think about it, I'm cheating slightly-- second time today.
Because we said we really don't care about the implementation.
We want to get a sense of the complexity of the algorithm.
And that's generally true.
But here is a place in which the implementation actually has an impact on that complexity.
And I want to be conscious of that as I make these decisions.
But again, logarithmic in terms of number of steps, constant work for each step, because I'm just passing in values.","Inttostr method complexisty is O(n) it deal with lenght of string and reduce it only by 1 everytime not by 10 times so it O(n) and if it how it is it will be log10(n) if we deal with digits as big number to iterate over
If concatenation complexity is O(n^2) you are right, but if it is O(n), then it's O(log(i))... in this case, I assume it's O(n)
How is it implied that the first bisection search function copies the list first? And why is it o(n) even when length is halved at each search?
it says ""turns out that the total cost to copy is O(n) and this dominates the log(n) cost due to the recursive calls"" I agree that the cost will be O(n) but not with the part that says ""and this dominates the log(n) cost due to recursive calls"". The following is implied: O(log(n)) + O(n) = O(n), but such addition can't be the case. Log(n) is the amount of recursive calls, which should be used as a multiplicative factor with the amount of steps inside one function call. As was mentioned correctly, the amount of steps in each consecutive function call decreases by factor of 2. Suppose that n = 32; then the amount of function calls is s = log(n) = log(32) = 5. The total steps in all function calls combined is (1 - 1/(2^log(n))) * n = (1 - 1/2)n = (1/2 + 1/4 + 1/8 + ... + 1/2)n = n/2 + n/4 + n/8 + n/16 + n/32 = 32/2 + 32/4 + 32/8 + 32/16 + 32/32 = 0.96875n = 31  n (for large n) --> O(n). I don't see how the addition of log(n) with n is logical in the proof for the code being O(n)."
"So we want to approximate the random oracle.
And we're going to get to that.
Obviously we're going to have to do this in poly-space as well.
So what's wrong with this? Of course this picture is I didn't actually say this, but you'd like things to be poly-time in terms of space.
You don't want to store an infinite number-- this is worse than poly-time, worse than exponential time, because it's arbitrary strings that we're talking about here, right? So you can't possibly do that.
So we have to do something better.
But before I get into how we'd actually build this, and give you a sense of how SHA-1 and MD5 were built-- and that's going to come a little bit later-- I want to spend a lot of time on the what is interesting, which are the desirable properties.
Which you can kind of see using the random oracle.
So what is cool about the random oracle is that it's a simple algorithm.
You can understand it.
You can't implement it.
But now you can see what wonderful properties it gives you.
And these properties are going to be important for our applications, OK? And so let's get started with a bunch of different properties.
And these are all properties that are going to be useful for verification or computer security applications.
The first one, it's not ow, it's O, W.
It's one-wayness, all right? So one-way, or one-wayness.
And it's also called-- you're not going to call it this-- but perhaps this is a more technical term, a more precise term, pre-image resistance.
And so what does this mean? Well this is a very strong requirement.
I mean a couple of other ones are also going to be perhaps stronger.
But this is a pretty strong requirement which says it's infeasible, given y, which is in the-- it's basically a d-bit vector, to find any x such that h of x equals y.","Consider simpler examples to get the idea: If h(x) =x^2 (square of x) Clearly h(x) is not OW... x=sqrt( h(x) ) However, there is no collisions CR & TCR is satisfied . -A reverse example, let H(x) = XOR(xis) {I mean some kind of XORing of different bits of x in some manner You see here the reason that makes H(x) OW (hard to retrieve x after the mangling) is almost the same reason it is NOT CR & NOT TCR (It's not that we can't find an x given h(x), it is there r so many valid Xs) . and conceptually, there r applications that require OW not TCR (like storing pswds hashes to not reveal them), and other applications that cares more about TCR not OW (like file signatures where the file is not a secret, u care only that it has not been modified by an opponent or a virus for example by storing its hash... here ur worry is that they can modify F to F' where h(F) =h(F')...note that even if F' doesn't make sense and u won't be deceived by it the original F will be lost)"
"So when you have a system like this that works as I've indicated, then what we're going to call that, we're going to give that a special name, and we're going to call that a forward-chaining rule-based-- because it uses rules-- expert system.
And we're going to put expert in parentheses because when these things were developed, for marketing reasons, they called them expert systems instead of novice systems.
But are they really experts in a human sense? Not really, because they have these knee-jerk rules.
They're not equipped with anything you might want to call common sense.
They don't have an ability to deal with previous cases, like we do when we go to medical school.
So they really ought to be called rule-based novice systems because they reason like novices on the basis of rules.
But the tradition is to call them rule-based expert systems.
And this one works forward from the facts we give it to the conclusion off on the right.
That's why it's a forward-chaining system.
Can this system answer questions about its own behavior? [INAUDIBLE], what do you think? SPEAKER 5: [INAUDIBLE].
PROFESSOR PATRICK WINSTON: Why? SPEAKER 5: [INAUDIBLE].
PROFESSOR PATRICK WINSTON: Because it looks like a goal tree.
Right.
This is, in fact, building a goal tree because each of these rules that require several things to be true is creating an and node.
And each of these situations here where you have multiple reasons for believing that the thing is a carnivore, that's creating an or node.","I'm wondering why ""Eats Meat"" has it's own AND gate? Could someone explain this?"
"But you're exactly right, that the way that a patch in particular was violated was the way that you described.
So does it all make sense? OK.
So that's sort of a quick and dirty example of Shellshock stuff.
And so another example I wanted to give you was an example of a cross-site scripting.
And so the Shellshock bug was sort of an example of how content sanitization is very important.
So as we'd just been discussing, you shouldn't just take inputs from an arbitrary person and them use them directly in commands of any type.
So cross-site scripting attacks are another example of how something can go wrong.
So in this example, I have another sort of dumb CGI server here.
And if we look at this CGI server, so what is it going to do? So once again, I've written something very simple in Python.","Prof. Mickens, for the SQL injection attack, would using stored procedures be a way to prevent malicious content being used for injection?"
"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
PROFESSOR: So now we have this last exercise.
I'm creating a mysum.
I'm going to go through all of these values and you guys have already told me what values these are, 5, 7, 9, not 11.","after breaking from the if code block why didnt the value of i incremented ,for loop was still going on"
"It doesn't have to be key.
It doesn't have to be anything in particular.
It's very powerful.
You can take all sums, products and maintain them as long as they're downward looking-- as long as you're only thinking about the subtree.
Some examples of things you cannot maintain are-- not a nodes index.
So if you get a little bit too excited about augmentation, you might think, oh, I could do everything.
I needed to support subtree_at, or let's just say, get_at globally, I wanted to know what is the ith node in my tree? Well, I'll just use data structure augmentation and store in every node what is its index, 0 through n minus 1.
I can't maintain that efficiently.
Because if I insert at the beginning of my traversal order, then all the indices change.
So that's an example of a edit.
So if I insert a new node over here, so this guy's index was 0, now it's 1.
This guy's index was 1, now it's 2.
This was 2, now it's 3, and so on.
Every node changes its index.
Index is not a subtree property, and that's why we can't maintain it.
Because it depends on all of the nodes in the tree.
Or it depends on all the nodes to its left-- all the predecessors.
So for example, this guy's index depends on how many nodes are over here on the left, which is not in the subtree of that node.
So that's where you have to be careful.
Don't use global properties of the tree.
You can only use subtree properties.
Another example is depth.
Depth Is annoying to maintain, but it's not obvious why yet.
We will see that in a moment.
The rest of today is about going from order h order log n, which is what this slide is showing us.
So at this point, you should believe that we can do all of the sequence data structure operations in order h time-- except for build and iterate, which take linear time-- and that we can do all of the set operations in order h time, except build and iterate, which take n log n and n respectively.","Question: If height can be the property that can be stored in each node , then why not depth can be stored ? It's written that Index and depth we can't store in node bcz they are global properties
Just think about a local property, as that depends only on the node's subtree. The depth depends on what happens above the node, so it cannot be local."
"So in real life, you have inheritance.
And in object-oriented programming, you can also simulate that.
OK, so the first few slides are going to be a little bit of recap just to make sure that everyone's on the same page before I introduce a couple of new concepts related to classes.
So recall that when-- in the last lecture, we talked about writing code from two different perspectives, right? The first was from someone who wanted to implement a class.
So implementing the class meant defining your own object type.
So you defined the object type when you defined the class.
And then you decided what data attributes you wanted to define in your object.
So what data makes up the object? What is the object, OK? In addition to data attributes, we also saw these things called methods.
And methods were ways to tell someone how to use your data type.
So what are ways that someone can interact with the data type, OK? So that's from the point of view of someone who wants to write their own object type.
So you're implementing a class.
And the other perspective was to write code from the point of view of someone who wanted to use a class that was already written, OK? So this involved creating instances of objects.
So you're using the object type.
Once you created instances of objects, you were able to do operations on them.
So you were able to see what methods whoever implemented the class added.
And then, you can use those methods in order to do operations with your instances.
So just looking at the coordinate example we saw last time, a little bit more in detail about what that meant-- so we had a class definition of an object type, which included deciding what the class name was.
And the class name basically told Python what type of an object this was, OK? In this case, we decided we wanted to name a coordinate-- we wanted to create a Coordinate object.","Interesting lecture but I have never seen a real world example as to the benefits of Classes and OOP versus Functions, there is so much associated jargon. Can someone show me an example
Can I call a class' method w/o making an instance of it? For example, Anna is using a method of 'random' class which she imported w/o instancing."
"The upper confidence bound is not better than the- the real mean of the optimal arm? So this is the failure case, where the confidence, but um, we're gonna say that if we look at that, we- we don't want this thing to happen.
We can upper bound that by making sure our upper confidence bounds hold at all time steps.
Okay.
So these are the arms.
So what I said up there is that if all of our confidence bounds hold on all time steps we can ensure that.
So we're now going to write that down in terms of what? The probability that the upper confidence bounds do hold on all time steps.
And so we're gonna do probability that Q of at, that's Q hat of at is greater than U.
Okay.
This is the upper confidence bound we defined over there.
This is just saying that the upper confidence bound holds for each arm on each time step.
Well, by definition, over there we said we- we picked an upper confidence bound to make sure this held with the least probability of delta over t squared.","Using unions with probabilities can be a little confusing. In fact, it may be related to my following misunderstanding: if Q(a*) > U_t(a_t) whenever the upper bounds hold, how can be that the probability of the former is less than the probability of the latter?
Li Yep, I was referring to that moment. I think I got It wrong when I wrote that (using unions with probabilities is still terrible, they should use sums and mention dependence and how we get rid of it). She calculates is the probability of choosing the wrong arm at some step. Effectively when all the bounds are met, Q(a*) < Ut(at) (I got that reversed the first time). Therefore, Q(a*) > Ut(at) implies that some bound does not hold and thus, Q(a*) > Ut(at) is less likely than the failure of some bound."
"So let's go look at the code that does this.
So here you have it or maybe you don't, because every time I switch applications Windows decides I don't want to show you the screen anyway.
This really shouldn't be necessary.
Keep changes.
Why it keeps forgetting, I don't know.
Anyway, so here's the code.
It's all the code we just looked at.
Now let's run it.
Well, what we see here is that we use greedy by value to allocate 750 calories, and it chooses a burger, the pizza, and the wine for a total of-- a value of 284 happiness points, if you will.","Are the numbers inside the 'values' array randomly picked by the instructor or the does it act as a grading scale for each menu item?
Thank you! I was confused that he was describing a local optimum with those examples because the metrics he is using are qualitatively different, ie. it might be more desirable to me to have slightly less overall calories but me maximising on ""value"" (how much I like the food) rather than cost. What seems significant for determining the optimum is the order of the elements, and the metric (or the key function) determines the order. So then the global optimum is the solution with biggest total across all orderings."
"OK, how about this one.
So C21 is M2 plus M4.
So M2 plus M4.
So M2 will have A12 B11, A22 B11.
So M4, there's A22 minus B11, so that cancels out.
So we're left with A21 B11 plus A22 B21.
That's the correct answer.
So this, I guess, is a very clever algorithm.
You have to work in that area for 10 years to come up with this so that's not our concern.
Our goal is to analyze this algorithm.
What's the complexity of it? So does anyone understand this recursion? Can someone tell me, what's the recursion for this part, for this Strassen algorithm? We have the original problem, and we have some-- go ahead.
AUDIENCE: So since each of the M 1 through 7 only require one multiplication, you'll need to solve seven subproblems, so 7T n over 2 plus O n squared.
LING REN: That's absolutely correct.
Everyone gets this? So what Strassen did, is he came up with the seven matrices.
Each one requires only one multiplication.
So we have seven subproblems, instead of eight, and that's going to give us a benefit, an improvement.
So the question now becomes, how do I solve this recursion? Given this recursion, how do I know its complexity? And same question there.","So..... anyone can explain the philosophy of Strassen's method?
Hello Everybody, I don't understand how this course works. There are lecture videos taught by the lecturers and also videos taught by students with prefix R1,R2,R3, etc. If I'm not wrong, are R1,R2 revision videos of official video 1 and 2? Or are they supplementary to the videos taught by the lecturer?"
"If I have n elements in X, Y, Z over there-- I guess here they're called xi, yk, zk.
Sorry, maybe I should've called them that here.
Doesn't matter.
If I have n of those elements in X union Y union Z, the number of digits here is n.
So the number of digits in order n.
This is fine from an NP-completeness standpoint.
This is polynomial size.
The number of digits in my numbers is a polynomial.
And this base is also pretty small.
So if you wrote it out in binary, it would also be polynomial.
So just lost a log factor.
But the size of the numbers, the actual values of the numbers, is exponential.
With weak NP-hardness, that's allowed.
With strong NP-hardness, that's forbidden.
In strong NP-hardness, you want the values of the numbers to be polynomial.
So in this case, the number of bits is small, but the actual values are giant, because you have to exponentiate.
It would be cool.
And this problem is only weakly NP-hard.
Maybe you actually know a pseudo-polynomial time algorithm for this.
It's basically a knapsack.
If these numbers have polynomial value, then you can basically, in your subproblems in dynamic programming, you can write down the number t and just solve it for all values of t.","Is there anything between polynomial and exponential? Is there anything that grows faster than n^C for every C, but slower than e^(n^) for every >0?
I can't fully understand that the professor stated that ""X is NP-hard if every problem y  NP reduces to X"". Doesn't it mean that a problem(NP) would then reduce to a harder problem(NP-hard)? How should I interpret it? Thanks!"
"So what happens when we do this? Well note that strictly speaking, this access is out of bounds, right.
Because this was only allocated 44 bytes of memory, but of course the way that baggy bounds works is that it will actually allow axises that are out of bounds, if they stay within that baggy bounds.
So even though strictly speaking, the programmer probably shouldn't have done this, this is actually going to be OK, right.
We're not going to raise any flags or anything like that.
Now let's say that the next thing we do is we need to find another pointer, which is going to be set equal to q plus 16 right.
Now this is actually going to cause an error, right.
Because now q is at an offset of 60 plus 16, which equals 76.
So this is actually 12 bytes away from the end of that baggy bounds.
OK? And that's actually greater than half a slot away.
All right, so if you remember the baggy bounds system will actually throw a hard synchronous error if you get beyond 1/2 a slot from the edge of that baggy bounds.","Maybe I'm missing something, but how is char *r = q + 16 supposed to give out an error? Yes, it's way past the allocated 64 bytes(which are in fact 48 bytes. Malloc allocates in multiples of 16 bytes, but it's the address of the beginning of the 48 bytes that's supposed to be a power of two and aligned to the word size of the system, not the size...), but it's simply going to return a pointer to a valid address. Whatever you'll read at that address won't be p, it will be garbage from your process's memory. So in fact it won't error out... it's worse than that. It will act as valid memory and you can read from it. As a matter of fact, you can even write to it, no questions asked! (I'm staring at it in the debugger as I write this). This is an actual undefined behavior.
Can we say that 1/16 of memory size is used (or reserved) for baggy bounds table in x86 systems ? (16 is the slot size of course)
in malloc(44) we are telink to alocate 44bytes of memory but it will alocate 64bytes why ?"
"And then we'll print it, all right? So given a number of needles and a number of trials, we'll estimate pi and we'll give you the standard deviation of that estimate.
However, to do that within a certain precision, I'm going to have yet another loop.
And I'm showing you this because we often structure simulations this way.
So what I have here is the number of trials, which is not so interesting, but the precision.
I'm saying, I would like to know the value of pi and I would like to have good reason to believe that the value you give me is within 0.005, in this case, of the true value.","Could somebody please explain why the precision is chosen to be .005 for the estimation of Pi? And what did he mean by saying ""should probably use 1.96 instead of 2""? There are two ""2"" in the code, which one he meant? The whole lecture is titled ""Confidence Intervals"", but the actual topic is just skimmed in a couple of sentences 
Didn't you get a better estimate going from 1000 needles to 2000? Isn't 3.139 closer to Pi than 3.148 so it's an improvement isn't it? But it looks still be true from your samples that the simulations are not monotonically getting better.
I did not get the weight parameter in the formula shown at the beginning. It says [1/numSamples]*len(dist). However, numSamples is 1000000 and dist has always a length of 1000000 as well, so the weight will end up as 1. Am I missing something?
How can one use the Archimedes method for calculating pi in a Monte Carlo simulation and find a CI? Is seems like this method is a more straight forward way of finding pi."
"And the reason it's more complicated than what you see up there is that in a search, as you can see, you're going to be moving at different levels.
You're going to be moving at the top level.
Maybe at relatively small number of moves, you're going to pop down one, move a few moves at that level, pop down, et cetera, et cetera.
So there's a lot of things going on in search which happen at different levels, and the total cost is going to have to be all of the moves.
So we're going to think about all of the moves-- up moves, down moves, and add them all up.
They all have to be order log n with high probability.",Any idea how can I understand how log n is derived?
"So when I go to B, I'm actually going to expand B now.
I'm going to look at the nodes that I can visit as a consequence of expanding B.
The ones that I can visit are C and D.
And I'm going to add the partial paths ABC and ABD to my agenda.
So AC, I'm just going to move to the front of the queue, or the agenda, and I'm going to add ABC and ABD.
And I got there through B.
So I'm going to add C and D here.
Depth-first search grabs from the opposite end of the agenda.
So the first thing I'm going to look at is AC.
I'm going to expand to C, look at the nodes that I can reach as a consequence of expanding C, and visit B and D.
AB is still hanging out here.
I popped off AC to use it in order to expand C's children.
And I'm going to add ACB first and ACD second.
Note that our search trees already look different.
And we'll actually end up reaching the goal using one of these search strategies first than the other, or as opposed to the other.
If I go back to breadth-first search, I'm going to pop the partial path AC off the front of my agenda.","For the DFS, before finding the path ACDE, does it should visit ACDA first because you don't have a visited node constraint here?"
"No, the output is z.
So it's z time 1 minus e.
So whenever we see the derivative of one of these sigmoids with respect to its input, we can just write the output times one minus alpha, and we've got it.
So that's why it's mathematically convenient.
It's mathematically convenient because when we do this differentiation, we get a very simple expression in terms of the output.
We get a very simple expression.
That's all we really need.
So would you like to see a demonstration? It's a demonstration of the world's smallest neural net in action.
Where is neural nets? Here we go.
So there's our neural net.
And what we're going to do is we're going to train it to do absolutely nothing.",Is Conway's Game of Life hard to do with neural nets?
"And the type of this object was therefore going to be a coordinate.
We defined the class in the sort of general way, OK? So we needed a way to be able to access data attributes of any instance.
So we use this self variable, OK? And the self variable we used to refer to any instance-- to the data attributes of any instance in a general way without actually having a particular instance in mind, OK? So whenever we access data attributes, we would say something like self dot to access a data attribute.
You'd access the attribute directly with self.x.
Or if you wanted to access a method, you would say self, dot, and then the method name-- for example, distance.
And really, the bottom line of the class definition is that your class defines all of the data-- so data attributes-- and all of the methods that are going to be common across all of the instances.
So any instance that you create of a particular object type, that instance is going to have this exact same structure, OK? The difference is that every instance's values are going to be different.
So when you're creating instances of classes, you can create more than one instance of the same class.","What is the point of naming the class variable self.rid? She never used it, I don't understand that. If she directly used Rabbit.tag on the last line what was the point of creating that instance variable? Can someone please answer this? Thanks
Interesting lecture but I have never seen a real world example as to the benefits of Classes and OOP versus Functions, there is so much associated jargon. Can someone show me an example
Can I call a class' method w/o making an instance of it? For example, Anna is using a method of 'random' class which she imported w/o instancing.
I can still call this outside the class though, so its not private in the same sense as in Java for example?"
"So things like tcpdump and other applications that parse network data.
So why do they worry so much about applications that parse network inputs? What goes wrong in tcpdump? Why are they so paranoid? AUDIENCE: Well, an attacker can control what's being sent and what's being called.
PROFESSOR: Yeah.
I think what they really worry about is, very much like with OKWS, they worry about that attack surface and how much can an attacker really control the inputs? And with these network parsing programs, there's a lot of control that that factor has.",What is OKWS?
"We composed data in terms of lists, and we associated names with those lists in terms of variables.
The next thing we want to think about is a higher order construct where we would like to conglomerate into one data structure both data and procedures.
Python has a concept called a class that lets us do that.
In Python, you make a new class by saying to the Python prompt, I want a new class called Student.
And then, under Student, there is this thing which we will call an attribute.
An attribute to a class is simply a data item associated with the class.
And a method-- a method is just a procedure that is associated with the class.
So there's this single item class called Student that has one piece of data, its attribute, school, and one procedure, which is the method calculateFinalGrade.
So then, this is the kind of data structure you might imagine that a registrar would have.
It's a way to associate.
So the idea here is that everybody here is a student.
They all have a school.
And they all have a way of calculating their final grade.
That's a very narrow view that maybe a registrar would have.
So classes, having defined them, we can then use the class to define an instance.",So can an instance run a procedure in other classes which is in the same environment as the instance? In the final example he showed that pat is in the same global environment as staff601 but when he was asking for pat.salutation was the computer looking into the attributes of staff601 which is in the same class with pat?
"Um, it- it- it did not matter what Sigma squared was and, um, and- and- and that's fine.
In the case of classification, y given x was sampled from a Bernoulli distribution whose parameter was 1 over 1 plus, um, e^minus Theta transpose x, right? And this was also, uh, we call this g of z equals 1 over 1 plus e^minus z, and g of Theta transpose x becomes the, um, becomes the parameter of the Bernoulli distribution.
Right? And then we had a smaller side, uh, mostly, uh, a digression in terms of what we're going to cover today.
But, you know, this is going to be useful next week.
Um, think of functions as points in an infinite-dimensional functional space.
And you kind of get a one-to-one mapping between the domain of the function and the axes of the, uh, functional space.
Uh, anyway, so this is not relevant for today, but, you know, just kind of keep this in the back of your mind.
So we're gonna continue along the lines of- of, um, the probabilistic interpretations, and we're gonna cover generalized linear models.
So off the bat, we- we see a few things that are common between these two.
All right, first of all, we are modeling y given x in both the cases, right? The difference is that in one case, y is real valued, and in the other case y is binary valued, 0 and 1.
And the- the datatype of y, whether it is, you know, a real value or- or, um, discrete binary value, informed our choice of the distribution that we are gonna- that we used, right? Uh, it doesn't make sense to define a Bernoulli- a Bernoulli over real value.
Similarly, it doesn't make sense to define a normal distribution on, uh, discrete value.
So the choice of- the- the data type of the y-variable informed the choice of the distribution that we used.","So, Y(i)'s that we are given(Dependent variable) are nothing but different points (points that are determined by 'eeta') of an exponential family(Gaussian for Linear Regression). In our case h(x; teta) will be a linear combination of x values because we assumed the distribution to be Gaussian. What if, in reality Y(i) does not follow Gaussian distribution and has some deviations. Will developing a regression make sense in that case?"
"And I can do that.
I'm capturing what I had on that previous slide.
And on a given name to getNode, it will get out the actual node.
And I use that coming out of the graph g.
I do the same thing with the getNode from graph g for Providence.
And then I make an edge out of that.
And then I use the method from the graph to add the edge.
If this looks like a lot of code, yeah, it's a lot of words.
But it's pretty straightforward.
I'm literally creating nodes with the names, using the appropriate methods, creating an edge, adding it into the graph.
And when I'm done, I'm just going to return the graph g.
OK.
Now I want to find the shortest path.","Have I missed something or he didn't defined printPath anywhere?
Theres one issue with the code that is given. Nowhere in the lecture notes or in the video defines the printPath() function. Also how does he print out in that format when the only way to do it is by calling on the Edge class method to print? especially when he is appending nodes and not edges. I am guessing it is done in the printPath() function"
"Clearly, if I can prove for all nP of n, then I've proved that I can get for every amount greater than or equal to $0.08 stamps.
And let's do the base case.
Well, the base case, P of 0.
Can I make $0.8 stamps? Sure, $0.03 and $0.05.
That's that one, and that's OK.
For the inductive step, I have to get m-- I'm allowed to assume, rather, that I can get m plus $0.8 for any m from n down to 0, instead of just assuming that I can get n plus $0.8 to get n plus 1 plus $0.08.
I can assume any amount less than what I'm aiming for, so I may as well assume that I can get any amount of postage from $0.08 up to n plus $0.08, and my objective then is to get n plus 1 plus $0.08, namely n plus $0.09.
So I have to prove that for all n greater than or equal to 0, I can get n plus $0.09, assuming I can get from $0.08 to n plus $0.08.
Well, that's not too hard to do.
The inductive step is actually going to break up into a couple of cases, depending on the value of n.
I have to prove n plus $0.09 for all n, so suppose n equals 0, I have to get $0.09.
Well, three $0.03.
If n is 1, I have to get 1 plus $0.09 or $0.10, two $0.05.
So those cases are disposed of.
So now my job is to get n plus $0.09, where n is greater than or equal to 2.","Hey, This is for those who don't get it, like I did ... Why do we use strong induction in postage example? Because we need to prove not only the base case (like in induction), but + two cases after it. Why? If we able to get three consecutive numbers (e.g. 8<-base case, 9, 10) we can get any number more then 8, adding 3 for every of particular cases. 8+3 = 11, 9+3 = 12, 10+3 = 13 and so on 11+3 = 14, 12+3=15, 13+3 = 16. Now I have 9 numbers (8,9,10,11,12,13,14,15,16). And I can do it infinitely and get all the numbers >=8. Formula for making P(n) = 0+8 cents depends on the making P(n-3) cents, P(n+1) depends on the making P(n-2). P(n+1) = P(n-2) + 3. We have to directly prove three BS for 8, 9 and 10 (P(0), P(1) and P(2)). We must construct it using only 3s and 5s. And I can definitely do it: P(0) = 8 is one 5 and one 3, P(1) = 9 is three 3s, P(2) = 10 is two 5s. We have shown how to get cents for every value of P(n) from 0 to some m, where 0<=m<=n (in other words how to get values from 8 to ... 10). Next we need to prove that our P works for m+1 case. We know that the biggest base case P(2) = 10, then our P(m+1) >= 11. Subtract 3 cents for each side P(m+1)-3 >= 8 . We know how to construct 8 with some amount of 3s and 5s. P(m+1) - 3 = let say a*3+b*5 (where a and b are non negative ints). Then P(m+1) = a*3+3+b*5 or (a+1)*3+b*5 is also combination of 3s and 5s. It's what we needed to prove. And we have proved it. I'm not afraid to take P((m+1)+1), because I have the second base case proved P(1) = 9.
Why the base case is zero while the axiom only when greater than or equal eight
Ive done induction on this particular problem. I know it as the ""Coin problem"". They give us the two coin denominations first, then the Frobenius Number which claims they can make n cents out of blah cent coins and blah cent coins for all K greater than or equal to n. Example: Prove with induction that with 8 cent and 3 cent coins, for all ""n"" cents greater than or equal to 14 can be paid with 8 and 3 cent coins. I was taught my base case for this will be the trivial case of where n = 14, how it can be paid with 8 and 3 cents i.e 14 = 8a+3b. Not this P(0) as you point out. My hypothesis would be that P(k) is true such that K can be paid in 8 and 3 cent coins where K is greater than or equal to 14 and in the step prove P(K+1) in the step with cases. For this specific example, My cases would be the following: Case 1: I have at LEAST FIVE 3-cent coins. I can take out all five 3-cent coins and put in 2 8-cent coins. Case 2: I have at least FOUR 8-cent coins. I remove all four 8-cent coins and put in 11 3-cent coins. Case 3: I have at most FOUR 3-cent coins and i have at most THREE 8-cent coins. But in that case, that doesnt get me what I'm trying to prove so that can't happen."
"And what we saw is the outer loop, if it's a standard iterative thing, will be linear.
But inside of the loop, I'm doing a linear amount of work each time.
So it becomes n times n, so order n squared.
OK, exponential-- these are things-- sorry, yes, I did that right.
I'm going to go back to it.
Exponential-- these are things that we'd like to stay away from, but sometimes, we can't.
And a common characteristic here is when we've got a recursive function where there's more than one recursive call inside the problem.
Remember Towers of Hanoi, that wonderful demonstration I did.
I was tempted to bring it back, because it's always fun to get a little bit of applause when I do it.
But I won't do it this time.
But remember, we looked at that problem of solving the Towers of Hanoi.
How do I move a stack of size n of different-sized disks from one peg to another where I can only move the top disk onto another one and I can't cover up a smaller disk? Want to remind you, we saw, there was a wonderful recursive solution to that.",may I ask what you meant by Tn?
"Potentially when we stop merging, because we stole one key from our parent, it may now be a 2 node, whereas before it wasn't.
If it was already a 2 node, then it would be another merge, and that's actually a good case for us, but when the merges stop, they stop because we hit a node that's at least a 3 node, then we delete a key from it, so potentially it's a 2 node.
So potentially the potential goes up by 1.
We make one new bad node, but every time we do a merge, we destroy bad nodes, because we started with a 2 node, we turned it into a 3 node.
So, again, the amortized cost is the actual cost, which is k, plus the change in potential, which is minus k plus 1, and so the amortized cost is just 1.
Constant number of splits or merges per insert or delete.
So this is actually really nice if you're in a model where changing your data structure is more expensive than searching your data structure.
For example, you have a lot of threads in parallel accessing your thing.
You're on a multi-core machine or something.
You have a shared data structure, you really don't want to be changing things very often, because you have to take a lock and then that slows down all the other threads.",It's possible the log(n) is referring to the rebalancing of the tree(which wouldn't happen if the element didn't exist) and not the searching for the element. Otherwise you're correct.
"So we've seen, decoders can be good because we get to play with the contracts that they gave us, we get to play with them as language models, encoders give us that bidirectional context.
So encoder-decoders maybe we get both.
In practice they're actually, yeah, pretty, pretty strong.
So there was a-- right, we could-- so but I guess one of the questions is like, what do we do to pre-train them? So we could do something like language modeling, right, where we take a sequence of words, word 1 to word 2T, instead of T.
Right, and instead of word 1 here, dot, dot, dot, word T, we provide those all through our encoder, and we predict on none of them.
And then we have, word T plus 1 to word 2T, here in our decoder, right? And we predict on these.
So we're doing language modeling on half the sequence, and we're taking the other half to have our bidirectional encoder, right? So we're building strong representations on the encoder side, not predicting language modeling on any of this, and then we on the other half of the tokens, we predict as a language model would do.
And the hope is that you sort of pretrained both of these well through the one language modeling loss up here.
And this is actually-- so this works pretty well.
The encoder benefits from bidirectionality, the decoder, you can use to train the model.",It's unclear whether the pretrained models are working based on Word2Vec or One-Hot-Encoding
"Namely, that all horses have the same color.
Say they're all black.
So, there's a bunch of black horses with maybe some highlighted brown manes.
I'm going to prove this by induction.
And for a start, there's no n mentioned in the theorem, so that's common for various kinds of things that you need to prove by induction.
So I have to rephrase it in terms of n.
It's going to be by induction on n.
The induction hypothesis is going to be that any set consisting of exactly n horses will all have the same color.
Let's proceed to prove this.
Now, I'm going to use the base case n equals 1, just because I don't want to distract you with suspicions that the base case n equals 0, that is no horses, is cheating somehow.
It would be, in fact, be perfectly legitimate to start with n equals 0 and argue that all the horses in the empty set have the same color, because there's nothing in the empty set.
However, let's not get into that.
We'll start with n equals one.
And indeed, if you look at any set of one horse, it is the same color of it as itself.","It's actually very simple, but in my opinion not well explained. Assume that every pair of horses has the same color. Then, you can almost immediately say that every n horses will have the same color, because if there was a horse with a different color, then there would exist a pair of horses that had different colors. But we assumed that every pair is the same. So by negation, we proved that n horses have the same color (but starting with the assumption that every pair of horses has the same color!!). Now what the explained induction proof does is exactly the same thing, but it tricks you by stating that it starts with the assumption that every set of one horse has the same color (which is true), but then it makes you jump over the more necessary assumption, that all pair of horses have to have the same color, in order for the proof to work. Thats it.
I do not understand this, please bare with me Im too stupid to be here. So you have a group of horses (group A), you pick x amount of horses from that group (group B). Group B is all the same color. You take one horse from group B, and its the same color like stated. You take a horse from group A, and it is a different color from group B. Wheres the problem/paradox here? We never said group A had to be the same color.
I still don't understand why the induction breaks down when you get two horses. I get that there's no ""middle"" horse to compare with, but I don't understand why you can't compare the two each other.
Is the issue with the base case or is it with P(n) implies P(n+1). .
The proof for the Formula for the Sum of a Geometric Series (from the last video) suffers the same flaw as this bogus proof. That is, P(0) does not imply P(1). In order for it to be a correct proof, wouldn't the previous proof require the base case to be P(2)? If not, why not?
If the group A and B have same element (middle) then it have same color but that isnt the case if n = 1.
thats my exactly my question, but if i assume that any set of n horses have the same color, but not necessarily the same color in two seperate sets, for example: we can say that the first horse is a set of n(1) horses of black color, while the the second horse is a set of n(1) horses of white color. But that's not what the proposition says so i have no clue :-)
In the case of n+1=2, we can assume the set of the first n horses have the same color, and that the set of the second n horses have the same color. But look at the proposition P(n) again: it's only claiming that all horses in that set have the same color. So while the first and last horse have the same colors within their own respective sets, that doesn't mean that the colors of those two sets are equal. Maybe all the horses in the first set were white and all the horses of the second set were black. Hopefully that helps."
"So let's look at some examples.
I can start with 2 and then the only thing I can do as a constructor is multiply 2 by 2 to get 4.
Once I got 4, I can do 4 times 2 to get 8, and I can do 4 times 4 to get 16, and I can do 4 times 8 to get 32, and all of these are positive powers of 2.
Now let's define the log to the base 2 of a positive power of 2 recursively.
Well, the log to the base 2 of 2 is 1.","In general, you can have a very long sequence of depth 1: [][][][][][][][][][][][]..., so how is the length upper-bounded by any function of the depth?"
"Um, yeah, so what's we- what- so, uh, here we see that, um, the- the linear regression and logistic regression pop out as natural consequences by just choosing- uh, by just making the choice of our exponential family distribution, right? And now, how does this, kind of, um, look visually, okay? [NOISE].
So for linear regression, um, [NOISE] for linear regression we assume this to be x-axis the data.
Right.
And there's some Theta that- so x is in Rd, Theta is in Rd.
But for this picture, we're just assuming it's one-dimensional.","So, Y(i)'s that we are given(Dependent variable) are nothing but different points (points that are determined by 'eeta') of an exponential family(Gaussian for Linear Regression). In our case h(x; teta) will be a linear combination of x values because we assumed the distribution to be Gaussian. What if, in reality Y(i) does not follow Gaussian distribution and has some deviations. Will developing a regression make sense in that case?"
"So this point is negative root 2 over 2 by root 2 over 2, and then there's the negative over here.
What property do these points have? I heard a word.
AUDIENCE: Unit circle? ERIK DEMAINE: Unit circle.
Good.
Who said unit circle? Nice.
It's a unit circle, right? Clearly that deserves a Frisbee end.
Unit circle.
Hm.
Circle.
It seem good.
What's going on here is I took this number.
I claimed it was the square root of i because it turns out, if you take points on the unit circle in the complex plane, when you square a number it's like doubling the angle relative to the x-axis.
This is angle 0.
This is angle-- what do you call it-- 45 degrees.
This is angle 90 degrees.
So when I square this number I get 90 degrees.
That's why this number is a square root of i.
I probably should have labeled some of these.
This is i.
This is minus i.
This is minus 1, and this is 1.
In general, we get something called-- so these are called the n-th roots of unity.
Unity is just a fancy word for 1.
1 is here, and first we computed the square roots of 1.
They were minus 1 and 1.
Then we computed the fourth roots of 1.
All of these numbers, if you take the fourth power, you get 1.","why we must take the nth root of unity, cant we take like -1, 1, -2, 2 ....as X? This will also collapse?
Squaring those numbers will give you 1, 1, 4, 4, giving you the set {1, 4} (a collapse of 4 numbers to 2), but if you square those again you get {1, 16}, which doesn't collapse the set any further. You need the collapsed set to collapse again when you square each value a second time, and then collapse again when you square the numbers a third time, and so on, hence the complex numbers. You could use the nth roots of any number, but using the nth roots of 1 is simpler and lends the alternative representation to represent amplitude and phase information in frequency space. If you used the nth roots of another number I dont think the alternative representation could be interpreted the same way."
"An exercise that you can go through is to go back and think about all of the geometric information that's kind of being taken for granted in this picture.
Like over here, how did we know that that was a right angle, that this thing was a rectangle? We needed that to be a right angle because we were claiming that this was a square.
Well, how did we know that that was a rectangle? Well, the answers are obvious.
We're using the fact that the complementary angles of a right triangle sum to 90 degrees because the angles of a triangle in general sum to 180 degrees.
We're using that in a bunch of other places.
We're also using the fact that this is a straight line, which may or may not be obvious.
But it's true, and that's why it's safe to add those distances to figure out what it was.
My point is that there are really a whole lot of hidden assumptions in the diagram that it's easy to overlook and be fooled by.
So let me show you an example of getting fooled by a proof by diagram.
And here is how to get infinitely rich.",I didnt understand the problem in the infinitely getting rich part. Please explain what was wrong there.
"So we can sort all of you guys.
And that might be the key that's associated to every object in the room.
And so when I'm searching for students, maybe I enter in the student number.
And then I want to ask my set, does this number exist in the set of students that are in 6.006? And if it does, then I can pull that student back.
And then associated with that object is a bunch of other information that I'm not using to search-- so for instance, your name, your-- I don't know-- your social security number, your credit card number, all the other stuff that I need to have a more interesting profession.
So in any event, let's fill in the details of our set interface a little bit more.
So our set is a container.
It contains all of the students in this classroom, in some virtual sense at least.
And so to build up our set, of course, we need an operation that takes some iterable object A and builds a set out of it.
So in other words, I have all the students in this classroom represented maybe in some other fashion.
And I have to insert them all into my set.
I can also ask my set for how much stuff is in it.
Personally, I would call that size.",could anyone help me with the recitation 2 followed by exercise where Set from Sequence has to be built? I am not getting the part self.S.get_at(i).key inside the insert method. What kinds of items are we feeding to build the sequence seq() and later set from seq? Are we making custom item object with key attribute as intrinsic property? Please help me with this.
"So we need another test to divide that group up.
Yes.
STUDENT: How did you get the 4 on the shadow test again? Why was it 4? PATRICK WINSTON: Well, if I look at the data and I see who-- the question is, what about that shadow test? If you look at the shadow test, and you say, well, there are 4 question marks.
And if we look and see what kind of people belong to those 4 question marks, there are 2 vampires and 2 non-vampires.
That's why it's 2 pluses and 2 minuses.
STUDENT: No, I understand that.
The question is, how did you get to the score of 4? PATRICK WINSTON: Oh, yeah.
The question is how did I get this number 4? It has nothing to do this, because this is a mixed set.
In fact, I've got three guys in a homogeneous set here, and one guy in a homogeneous set here, and I'm just adding them up.
STUDENT: OK.
PATRICK WINSTON: So very simple classroom illustration.
Wouldn't work in practice.
Yes.
STUDENT: How do you adjust this for larger data sets where it's unlikely you're going to have any [INAUDIBLE]? PATRICK WINSTON: The question is, how do I adjust this for larger data sets? You're one step ahead.
Trust me, I'll be doing large data sets in a moment.
I just want to get the idea across.
And I don't want there to be any thought that the method we use for larger data sets has got anything magic about it.
OK, so we're off and running.
And now we have to pick a test that will divide those four guys up.
So we're going to have to work this a little harder, and repeat the analysis we did there.
But at least it'll be simpler, because now we're only considering 4 samples, not 8.
Just the 4 samples that we still have to divide up that have come down that left branch.
So I have the shadow test.
It has 3 outcomes.
We have the garlic test.
It has 2 outcomes.
Yes and No.
We have the complexion test.","Sorry, I still don't understand how you arrived at 4 in the shadow group. Is it because there were 4 question marks? I fully understand the other homogenous sets. Thanks."
"Reduction from one problem to another is just a polynomial time algorithm, regular deterministic polynomial time, that converts an input to the problem A into an equivalent input to problem B.
Equivalent means that it has the same yes or no answer.
And we'll just be thinking about decision problems today.
So why would I care about a reduction? Because what it tells me is that if I know how to solve problem B, then I also know how to solve problem A.
If I have a, say, a polynomial time algorithm for solving B and I want one for A, I just take my A input.
I convert it into the equivalent B input.
Then I run my algorithm for B, and then it gives me the answer to the A problem because the answers are the same.","I believe the second statement in lecture is incorrect. It should be the other way around: if A is in NP, then B is in NP, because reduction implies that for A->B, A is at most as difficult as B. Otherwise, we can always convert from A to B and then solve it. As a result, this shows that if A is in NP, then it will not be P, which implies B will be in NP as well. In the meantime, I do not believe the second state: ""if B is in NP, then A is in NP"" is correct because I can easily reduce a polynomial time algorithm to a NP problem such as MBM into MVC.
If problem A can be converted to problem B(which is NP complete) in polynomial time then, what can we say about problem A? Is problem A, NP then? I think that nothing can be said about A. Can someone please confirm?
It depends. If B is P, then most certainly A is P. If B is NP, then we can relax and say A is at most NP, even when A in truth is P, given P is a subset of NP (we can reduce P to NP). It gets tricky though when B is NP-complete. If B is NP-complete and A could be reduced to B, shouldn't that mean A is both NP and NP-hard and therefore, NP-complete? And isn't that why, in order to prove the completeness of A, we first ensure A is NP and then try to reduce a problem Z which is NP-complete to A (essentially declaring that A is either equal to or harder than Z, i.e. NP-hard)? Now, if B is NP-hard and one can reduce A to B, only here we cannot say what exactly A is, i.e., A could either be NP or NP-hard or both (NP-complete). Please correct me (with explanation) if I am wrong.
I can't fully understand that the professor stated that ""X is NP-hard if every problem y  NP reduces to X"". Doesn't it mean that a problem(NP) would then reduce to a harder problem(NP-hard)? How should I interpret it? Thanks!
As far as I understood. 1) If B is in P, then A is also in P : This is saying that if problem A can be converted to problem B in poly-time and if problem B can be solved in poly-time then it follows that you can also solve A in poly-time. 2) If B is in NP, then A is also in NP: This is saying that if problem A can be converted to problem B in poly-time and if problem B belongs to NP i.e. its solution can be verified in poly-time, then A must also belong to NP. This doesn't mean that A cannot be faster than NP. Problem A could belong to P while also belonging to NP, as P is in NP.
If we have a problem A that is in NP and we reduce it to B then we know that B is NP. if we dont know if A is NP and we know that B is in NP are we sure that A is in NP?Could'nt be an algorithm that do not use this reduction but solves the A problem? is the second statement in https://youtu.be/eHZifpgyH_4?t=1426 correct?
Question. you say ""framing of these problems requires us to peel away the contextual embedding of the problems for which they are supposed to clarify."" - but isn't that the goal of all mathematics? We don't care about context we care about abstract patterns. So do you extend your concerns to mathematics broadly, or, to this specific problem?
Not being rude, but can someone explain the applications of such mathematics? What does this course teach and why is it useful? Thanks!"
"Universal hashing, we're going to guarantee there are very few conflicts in expectation.
Perfect hashing , we're going to guarantee there are zero conflicts.
The catch is, at least in its obvious form, it only works for static sets.
If you forbid, insert, and delete and just want to do search, then perfect hashing is a good method.
So like if you're actually storing a dictionary, like the OED, English doesn't change that quickly.
So you can afford to recompute your data structure whenever you release a new edition.
But let's start with universal hashing.
This is a nice powerful technique.
It works for dynamic data.
Insert, delete, and search will be constant expected time with no assumptions about the input.
So it will not be average case.
It's in some sense worse case but randomized.
So the idea is we need to do something random.
If you just say, well, I choose one hash function once and for all, and I use that for my table, OK maybe my table doubles in size and I change the hash function.","I am confused when the dot product is performed on the Universal Hashing algorithm. Why there we are considering k_d - k'_d = 0 and is not (mod m) of that is also 0.
If you haven't yet found your answer: When you build the hash table and generate the hash function(s) (by choosing a random 'a') you store and re-use the 'a'. 'a' is not chosen randomly for each k-particular. You choose one 'a' for each hash function you use. The main idea is randomly choosing a hash function (by selecting a random 'a') to more likely get a random distribution of the keys across the bins, instead of assuming keys are random and having a static hash function.
One key may be hashed multiple times when using a hash table to insert an item, search for the item and finally delete it. So, there is a problem that how can i guarantee the hash code stays the same each time the key is hashed using randomized hashing? For example, how can i make sure the dot product hash function uses the same vector a (a1, a2, ..., ar-1) for the same key (k1, k2, ..., kr-1) at different times?"
"This is an alternative to expectation maximization.
And expectation maximization is defined as the coordinate ascent approach, right? So what we are instead doing here is we are just trying to fit a latent variable model where the relation to EM is through this coordinate ascent versus gradient descent interpretation.
Also, is coordinate ascent susceptible to saddle points or local optima? So the question is coordinate ascent susceptible to saddle points or local optima? Yes, they are.
VAE is also susceptible to local optima, absolutely.
They are not convex problems.
All right, so that's pretty much the variational autoencoder.","Professor, can I use gradient decent to solve GMM or factor analysis? Thanks!"
"So if we look at this example that- that we have here, for example, between a centrality of these, uh, nodes that are, uh, on the- uh, uh, on the edge of the network like A, B, and E is zero, but the- the betweenness centrality of node, uh, uh, C equals to three, because the shortest path from A to B pass through C, a shortest path from, uh, A to D, uh, passes through C, and a shortest path between, uh, A and E, again, passes through C.
So these are the three shortest paths that pass through the node C, so it's between a centrality equals to three.
By a similar argument, the betweenness centrality of the node D will be the same, equals to three.
Here are the corresponding shortest paths between different pairs of nodes that actually pass through, uh, this node D.
Now that we talked about how important transit hub a given, uh, node is captured by the betweenness centrality, the third type of centrality that, again, captures a different aspect of the position of the node is called closeness centrality.
And this notion of centrality importance says that a node is important if it has small shortest path- path lengths to oth- all other nodes in the network.","Yep, need more clarification regarding this matter. In my opinion, instance of (c) should be included, so the GDV will be [2, 1, 1, 4]"
"So, um, what are some advantages of belief propagation? Advantages are- ea- that it's easy to- to code up, and it's easy to paralleli- parallelize.
It is, uh, general.
It means that we can apply any graph model with any form of potential.
I showed you this label-label potential matrix, but you can also, uh, think of more complex, higher order potentials.
Um, so this is nice because label propagation or belief propagation does not, uh, consider only homophily anymore, but can learn more complex patterns, where labels change, based on the labels of the neighbors, right? So far, in- in previous methods, we only said, ""My label is- uh, depends on the label of my neighbors.
So if- whatever my neighbor preference is, my- why a- whatever my label- my neighbor's label is, this is also my label."" In belief propagation, the labels can flip because we have this notion of, uh, label-label, uh, affinity matrix.
Um, the challenge in belief propagation is that convergence is not guaranteed so we generally don't know when to stop, um, especially if there are many, uh, closed loops.
So the trick here would be to run a belief propagation for a short, uh, number of steps.
Um, and of course, these potential functions, this label-label, uh, potential, uh, matrix, uh, this needs to be- um, it requires training, uh, data analysis to be able to, uh, estimate it.
So to summarize, uh, we learned how to leverage correlations in graphs to make predictions of nodes.
We talked about three techniques.
Relational classification, where basically we say, my label is a sum of the labels from my neighbors, which basically means my label is, kind of, the label of my neighbors.
Um, this uses the network structure but doesn't use feature information.
Then we talked about iterative classification that use both node feature information, as well as the summary of the labels captured in the vector z around a given node.","Summarising the lecture contents to myself, the majority of thoughts are concerned with potential functions. I suppose, definition of Phi and Psi is the part engineering work that depend on the specific task. It's obviously the most non-trivial step in the belief propagation approach. I'd fancy a couple of examples of how can they be defined in specific tasks."
"So, we're gonna put in probability of Tau given Theta, divided by probability of Tau given Theta, times the derivative of the probability of Tau given Theta.
And the probability- if we instead had a log.
So, if you're taking the derivative of log of probability of Tau given Theta, that is exactly equal to the one over the probability of Tau given Theta times the derivative of p of Tau given Theta.
So, we can re-express this as follows; Sum over Tau r of Tau, p of Tau given Theta times derivative with respect to log of p of Tau given Theta.","I'm a bit confused by the notation - is pi(a|s) equal to pi(s, a)?
Why does rewriting the gradient with respect to theta of V reduce the variance? I thought the two were the same equation?"
"And you can't do better than that.
But of course, it gets more interesting when d is large.
So our goal now is to discover an algorithm that is going to minimize the worst-case number of drops for d equals 2.
That's our current goal, right? And then we'll look at more, even more general things.
So how would you do things if you had d equals 2? Just, it doesn't matter if you don't get our algorithm right.
We'll analyze it.
You propose something.
We'll analyze it.
And then we'll decide if we can do better or not.
Yeah.
Go ahead.
AUDIENCE: Drop the ball from the halfway point, say 64.
If it breaks there, then you can use the bottom half of what we are using.","How is binary search not the answer??? Drop a ball at 64. If it breaks, drop a ball at 32. If it doesn't break drop a ball at 96. Is it a rule that you have to pick an interval k? If so, I didn't pick up on that. With binary search, it the maximum drops would be log base 2 of n + 1. That would be 8 in the case of 128."
"And as you can see Y31 increased a little bit, so we're going to now stop this iteration of the algorithm and we're at A3 B1, which we think at this point is our upper tangent, but let's check that.
Start over again on my side B1 to B4, what happened? Well Yij decreased.
So I'm going to go back to B1.
And then Eriks going to try.
Hes going conterclockwise, he's going to go A3 to A2 and, well, big decrease in Yij.
Now Erik goes back to A3.
At this point we've tried both moves, my clockwise move and Eriks counterclockwise move.
My move from B1 to B4 and Eriks move from A3 to A2.
So we've converged, we're out of the while loop, A3 B1 for this example is our upper tangent.
All right.
You can have your finger back Erik.
So the reason this works is because we have a convex hull here and a convex hull here.
We are starting with the points that are closest to each other in terms of A1 being the closest to this vertical line, B1 being the closest to this vertical line, and we are moving upward in both directions because I went clockwise and Erik went counterclockwise.
And that's the intuition of why this algorithm works.
We're not going to do a formal proof of this algorithm, but the monotonicity property corresponding to the convexity of this subhull and the convexity of the subhull essentially can give you a formal proof of correctness of this algorithm, but as I said we won't cover that in 046.","I don't think this is correct. I think that if you keep track of points on the hull, you only need to check the ""open"" end (assuming that point is actually on the hull e.g. lowest x-value point which can be found in O(n)). This means the number of points to check decreases by 1 on every iteration"
"This made the problem feasible.
We ended up taking the product of the graph into all of these different subproblems, in fact.
But from the perspective of dynamic programming, we think of this as a subproblem constraint.
What we really want is the shortest s to v path, but that's hard.
So we're going to break it up into smaller pieces.
For each k between 0 and v, we're going to say, well, let's think about this problem restricted to use, at most, k edges.
And for simple paths, if there are no negative weight cycles, we only have to go up to v minus 1.
We proved that.
And we know we're-- then we're happy.
But that's sort of the last thing we want to solve.
So if we look down at the original problem we want, it's delta sub v minus 1 of s, v for all v.
So if we could solve these subproblems, as we saw with Bellman-Ford, we can solve the original problem-- unless there are negative weight cycles.
And we use delta sub v of s, v to check whether they're negative weight cycles.
I don't want to repeat that, but that's the all in the original problem.
And then we can take this relation that we wrote for DAG shortest paths and just port it over here.
So remember, for a general graph, this has cycles, so we can't use it, because we're just referring to arbitrary delta of s, vs, so there's no reason to expect no cycles there.
In fact, the call graph is exactly the graph G, so it has a cycle if and only if G does.
But now, over here, I'm running exactly the same formula for this min, but I added a sub k here and a sub k minus 1 here, the observation being, if I've already guessed what the last edge u, v is for this path, then if this whole thing has length at most k, then this piece has length at most k minus 1.",Why bellman ford base case k is equal to 0. Shouldn't it be 0 and positive values?
"And that's because S and E of S are the same size, because all I do is, if I had a bottleneck there, I would take this set T along with S and this set orange along with E of S, and that would be a bottleneck.
Because now, if you look at the size of T, I've added the same amount to the size of T as I've added to that orange set, because E of S and S are the same size.
So if this orange part was smaller than T, then orange part along with E of S is smaller than T union S.
And that defines a bottleneck in the whole graph.
So again, if there was a bottleneck in here caused by restricting images to the complement of E of S, it means there was a bottleneck in the whole graph.","what is S?
What is a bottleneck?
|S| <some condition> for all sets S that are a subset of L(H). Each S is some subset of the left side of the graph (as racun correctly stated), and we are considering all such subsets. Basically we want to analyze every subset S of the left side of our graph and see if it to conforms to the condition that its size is is less than or equal to the number of nodes it is connected to on the other side; |S| <= |E(S)|.
A bottleneck is where |S| > |E(S)| for some subset S  G. This violates Hall's matching condition."
"Now, the way that we actually go about doing this relies on kind of some knowledge of what's called an information bottleneck.
And we'll actually be covering information bottlenecks on Monday next week.
So I don't want to go too much into it right now.
But the intuition behind what we'll be doing is we're essentially just going to be adding noise to theta.
And when you add noise to theta, you're removing information from what's in theta.
You're increasing the entropy of that random variable.
And so by doing so by basically trying to minimize training loss under a noisy-- minimize meta-training loss under a noisy version of theta that means that you're going to be trying to, basically telling it to be able to do meta-training with less information in theta as compared to previously.","I didn't fully get why we couldn't maximise the mutual information between D_tr and y_ts like this: min loss_meta_training - mutual_information (D_tr, y_ts). Is it correct to say that it is better to minimise the mutual info between theta and y_ts instead because D_tr distribution is very complicated?"
"But no reason to get more complicated than what I have up there.
So find next cell to fill makes sense? We good with that? All right.
And generally with exhaustive search the key procedure is always do you have a valid solution or not? And you may not have a complete solution.
Think of it as a partial configuration.
So this is a partial configuration that is valid.
It's not a complete solution to the Sudoku puzzle.
It's a partial configuration that's valid in the sense that it satisfies all of the constraints.
You know, if I put another eight in here it would not be a valid partial configuration.","Do there exist sudoku puzzles (specifically, steps of these puzzles) that can only be solved by the ""guess and backtrack"" method?
I've been solving a lot of sudokus lately, and one thing I've noticed that this brute force + implication method doesn't account for is a puzzle with the potential for a ""uniqueness"" problem. It's entirely possible that a sudoku with a unique solution might have a valid position along the way that presents a choice where one option would eventually lead to the correct solution and one (or more) of the other options would lead to a set of solutions where you could swap/rotate certain cells' values and still have a valid position by the algorithm's standards. With this naive approach, I wonder how you could build in a feature that detects uniqueness problems and discounts them from the set of valid positions in the solving path.
Isn't it also important to state that each sudoku has exactly 1 solution?
Interesting. Are you saying that if a puzzle requires guessing then it's essentially ""indeterminate"" and would have more than one solution? I mean, if you had a sudoku board with only one number populated, you'd have a large number of solutions that would be possible, and then if you had a board with another number added, you'd have a smaller set, and so on until you would have one that would still have multiple solutions until you constrained it with a final number.
Why does he use range 09 for i and j (in general, and especially in the routine to find the next empty cell) ? The positions i and j can only have values 19, and what he says about using 0 to indicate an empty cell doesnt apply. Or am I missing something?
why? Which part are you talking about? It's an honest question because I truly want to understand whether we can solve sudoku with an algorithm purely based on deductive rules, rather than search"
"Here's what we do.
Like everybody else, we start off with a score from zero to 100.
But then we say to ourselves, what score would you get if you had a thorough understanding of the material? And we say, well, for this particular exam, it's this number right here.
And what score would you get if you had a good understanding of the material? That's that score.
And what happens if you're down here is that you're following off the edge of the range in which we think you need to do more work.
So what we do is, we say that if you're in this range here-- following MIT convention with GPAs and stuff, that gets you a five.","are search algorithms (depth-first, breadth-first, uniform-cost, A*) included in course 6.01 or is that just something all MIT students just know...? How do I learn them"
"Which means that R of Jason is that set of two courses that he's associated with or that are associated with him-- that he's registered 6.042 and 6.012.
So at this point, we've applied R to one domain element-- one student Jason.
But the interesting case is when you apply R to a bunch of students.
So the general setup is that if x is a set of students-- a subset of the domain, which we've been showing in green-- then if I apply R to X, it gives me all the subjects that they're taking among them-- all the subjects that any one of them is taking.
Let's take a look at an example.
Well, another way to say it I guess is that R of X is everything in R that relates to things in X.
So if I look at Jason and Yihui and I want to know what do they connect to under R-- these are the subjects that Jason or Yihui is registered for.
The way I'd find that is by looking at the arrow diagram, and I'd find that Jason is taking 042 and 012.
And Yihui is taking 012 and 004.
So between them, they're taking three courses.
So R of Jason, Yihui is in fact 042, 012, and 004.
So another way to understand this idea of the image of a set R of X is that X is a set of points in the set that you're starting with called the domain.
And R of X is going to be all of the endpoints in the other set, the codomain, that start at X.
If I said that as a statement in formal logic or in set theory with logical notation, I would say that R of X is the set of j in subjects such that there is a d in X such that dRj.
So what that's exactly saying that dRj says that d is the starting point in the domain.
d is a student.
j is a subject.
dRj means there's an arrow that goes from student d to subject j.
And we're collecting the set of those j's that started some d.
So an arrow from X goes to j is what exists at d an X.
dRj means-- written in logic notation-- it's really talking about the endpoints of arrows, and that's a nice way to think about it.","""D  inverseR(J),"" states __the set of all students__ (denoted by D) happens to be a subset of __the set of subjects that have registered students__ (denoted by inverseR(J)). It's NOT true because the set of all students (that is, D) includes/contains the element Adam but the set of subjects that have registered students (that is, inverseR(J)) does not include/contain some subject that has registered the student Adam -- D  inverseR(J) is {Jason, Joan, Yihui, Adam}  {Jason, Joan, Adam} and is False.
Why is the label for that group called stuDent when no course registers him?"
"And so if we're able to maximize that quantity, that means that when we change D-train y will actually change.
And that means that it will actually rely on D-train rather than completely ignoring it.
Yeah.
[INAUDIBLE] fine tune with the training data set? Sorry.
Can you-- I missed like one word in the first part, is theta the-- [INAUDIBLE] which has the general information about shared tasks and then we're trying to fine-tune with the training data set that we have.
Yeah.
So you can think of theta as the shared information.
Although in this case, I'm actually just referring to it as the meta parameters.
Like the parameters that you're optimizing during the meta training process.
But yeah, when you think of it as a random variable, sometimes it can be helpful to think of it also in terms of the shared structure.
But here really I'm referring to it as the meta-training parameters.
Cool.
So it'd be awesome if we could maximize the mutual information 'cause then we'd be telling it to rely on D-train to basically make different predictions for different D-train.","I didn't fully get why we couldn't maximise the mutual information between D_tr and y_ts like this: min loss_meta_training - mutual_information (D_tr, y_ts). Is it correct to say that it is better to minimise the mutual info between theta and y_ts instead because D_tr distribution is very complicated?"
"How many are saying it doesn't change? Okay.
Almost as many say it doesn't change.
Okay.
That's interesting.
So we'll answer this question, but you know, keep on thinking about that in your back of your head.
And one thing I'll say is that, you know, I shouldn't- you shouldn't expect to necessarily find the right answer here just by kind of intuiting things.
And one of the points of making things codified in a Bayesian network is that you don't leave anything up to a, kind of, vagueness.
It's- you- it's- there's actually a correct answer that we can derive.
Okay.
So, um, let me talk about how to go about, uh, modeling this as a Bayesian network.
So with this core example, so there's four steps.
Um, the first step is defining what the variables are.
Okay.
Variables.
Um, so what are the variables here? Yeah.
Burglary.
Okay.
So there is a burglary- Earthquake.
Earthquake.
And alarm.
And alarm.
Okay, great.
So these are the three things that we don't know about that are mentioned.
Okay.
So the second step is, um, you draw some edges.
Okay.
So these are gonna be directed edges, that correspond to notions of influence.
Um, and if you- if you want cause- causality.
But causality is a very, uh, more philosophical thing which we don't really need for this class.
Um, so, but I'll- but I'll use it anyway.
So what causes what? So this alarm cause burg- burglaries, no.","No idea. I feel like it should just be 0.5. Both mathametically and intuitively, if there is an alarm and we have no other information, and the probability of an earthquake = probability of burgalary, then it should be equally as likely for it to be an earthquake that triggered the alarm as it is a burgalary to trigger the alarm."
"Doesn't really matter.
What I can do is, I can move from here to there-- has the same in-order traversal order, but it's got a different shape.
And in particular, subtree heights have changed, which means it can help us re-balance the tree.
And that's the whole point of AVL.
Does that make sense? AUDIENCE: That's the right rotation? JASON KU: This one is-- this is a right rotation.
This is a left rotation.
Any other questions? Yeah? AUDIENCE: [INAUDIBLE] JASON KU: As I'm walking up the tree, every node I might have to fix with a re-balance, but that re-balance does at most two rotations, and there is at most log n ancestors that I have, because my tree was height balanced at 2 log n or something like that.",what is the item in AVL tree  and  in problem4-3 ? i dont understand that what is good point of cross-linking.
"If you type a string, ""Hello,"" it'll say, oh, primitive data structure, string.
And it'll print out that string.
So the idea is one of the features of Python that makes it easy to learn is the fact that it's interpreter based.
You can play around.
You can learn by doing.
Now, of course, if the only thing it did was simple data structures, it would not be very useful.
So the next more complex thing that it can do is think about combinations.
If you type ""2 + 3,"" it says, oh, I got it.
This person's interested in a combination.
I should combine by the plus operator two ints, 2 and 3.
Oh, and if I do that, if I combine by the plus operator two and three, I'll get 5.
So it prints 5.
So that's a way you know that it interprets ""2 + 3"" as 5.
Similarly here, except I've mixed types.
""5.7 + 3,"" it says, oh, this user wants me to apply the plus operator on a float and an int.
OK, well I'll upgrade the int to a float.
I'll do the float version, and I'll get this, which is its representation of 8.7.
So the idea is that it will first try to interpret what you're saying as a simple data type.
If that works, it prints the result to tell you what it thinks is going on.
It then will try to interpret it as an expression.
And sometimes, the expressions won't makes sense.
In particular, if you try to add an int to a string, it's going to say, huh? And over the course of the first two weeks, we hope that you get familiar with interpreting this kind of mess.
That's Python's attempt to tell you what it was trying to do on your behalf and can't figure out what you're talking about.
OK, so that was simple.
But it already illustrates something that's very important, and that's the idea of a composition.
So the way Python works, the fact that when you added 3 to 2 it came out 5, what we were doing was composing complicated-- well, potentially complicated (that was pretty simple) -- potentially complicated expressions and reducing them to a single data structure.
And so that means that, in some sense, this operation, 3 times 8, can be thought of as exactly the same as if the user had typed in 24.","At ~44;00 when he is talking about lists and [-1] being the last, how does he arrive at 8 instead of 7? Isn't 7 the first item produced by y[-1][1]? Thanks!
I might be late but the last example... y[-1][1], and it gives 8. Shouldnt it be 7? Because if -1 represents the last list within that list, and youre asking for the 1st element (or number) within THAT list, you would see 7. So what? Also, why is -1 used to represent that element. Shouldnt it be 3 since its the 3rd list within that list
why does y[-1][1] get 8?"
"And you could get by with working with lists of things as long as they're finite.
But they very quickly get out of hand when you have to talk about, say, a set of lists.
Then it's not clear how to make a list out of those, and you wind up making sets again.
So sets, in fact, are an unavoidable kind of idea.
So another basic thing to understand about the notion of a set is that an element is either in a set or not in a set.
So if I write down 7, pi/2, 7, this is the same description of the same set 7, pi/2.
I'm just telling you the same thing twice here.
That 7 is in the set, and the 7 is in the set again.
So no notion of being in the set more than once.","Why is null set IMPROPER subset of all set?
Aint u jus repeating what the professor saying? :) What I am not understanding why u don't care? What kinda weird messy explanation is this? ...One part is false then u don't look at other one and jus say the whole thing is true. That rule comes out from some sorta bible or what... that whatever written there is true? : I perfectly understand why a null set can be assumed a subset of every set without knowing all this mess... but this kinda explanation I don't understand. Do we devise things for easier explanation or to complicate things to nonsense level? Why don't just assume null set has null element (nothing) and so any other set can be assumed to contain that... no? Isnt that how we learned it in prep school? ... Now we doing higher maths and it all goes ZOOOMMMMM! :) Isnt x+ 0 = x? that we know since elementary times.....So can be set .... Any Set + null element = Any Set ... I mean this could be another way of seeing it. "
"STUDENT: Yeah, so if you subtract the left with the right [INAUDIBLE].
PATRICK WINSTON: No.
No, sorry.
This expression here is 1 plus b.
Trust me it works.
I haven't got my legs all tangled up like last Friday, well, not yet, anyway.
It's possible.
There's going to be a lot of algebra here eventually.
So this quantity here, this is miracle number three.
This quantity here is the width of the street.
And what we're trying to do is we're trying to maximize that, right? So we want to maximize 2 over the magnitude of w if we're to get the widest street under the constraints that we've decided that we're going to work with.","Hi everyone, can i ask why when professor substitute for w, he used sub i and sub j when the there is only sub i after taking the derivative with respect to w. He kinda explained a little bit. But maybe because i'm not a native speaker, i kinda didn't get the intuition behind it?
very nice course. But i m confused, why min ||w|| is equal to min 1/2||w|| square. Can someone explain? thx
He didn't really explain why after substituting the `w = \sum y_i x_i \alpha_i` into L, we changed from minimizing L to maximizing L.
Can anyone please explain why we are maximizing the width here?
why did he take wx+b>=1? why 1? it could be anything right?
anyone have doubt how he got width=2/|w|,ask me i help you
Can anyone tell if we are trying to maximize L or minimize L
why did he multiply it by 1/2 and square it?
I have a small question. How can we assume that the conditions for the points outside the gutters would be this - y(w.x + b) >= 1. I mean why is this term greater than equal to 1 and not some other constant.
Anyone can explain to me why we set w.x+b>=1 for positive points here instead of set it as w.x+b >=0.5 for example. Thank you in advance.
1-b - (1+b) = -2b ....is what he did wrong??
thank you for replying. I get your first sentence. Just to clarify, it would be equal to 1 if b=0, right? As for your second sentence, I don't get it because he wrote less than or equal to -1. Also, if say b=5 then the first inequality is ok but the second gives -1+5<=-1 which is not true. What am I misunderstanding?
he even doesn't explain where the max(L) coming from. max(min(L)) == min(max(L)) in this case. He just skipped that part. Do u guys really think u understand his explanation?
Why is width equal to the first equation?
I dont get how he is findeing the width of the street! x1- x2 =1-b-(1+b)= -2b!? correct me if i am wrong!
nice one....I understand more easily than before this...but one doubt I satisfied mathematical convenience of Yi = + or - 1. But I didn't understand mathematical convenience of 1/2 ||w|| ^2
Can anyone answer the question the student was posing around the 20 minute mark which the professor didnt answer properly? He says hes subtracting 1+b from 1-b but what he actually does is add them!
""now what are we trying to do here? i forgot""
Why minimizing w and square of w are the same ? The Lagrangian used for minimization would result in different conditions if we use magnitude of w instead magnitude w squared. Any help would do
Still don't get how (1 - b + 1 + b).w/||w|| = 2/||w|| is it because (1 - b + 1 + b) = 2 and 2*unit vector w = 2 therefore 2/||w|| ?
Don't we have to minimize the lagrange term and not maximize it?
I don't understand it, too. How can 2*w/||w||=2/||w||. w = vector. 2*w = vector, 2*w/||w||= vector != skalar = 2/||w||. I think there must be a mistake
why are we partially differentiating w.r.t b, isn't it supposed to be a constant?
In this Equation: Margin Width= ((x+) - (x-)) dot with Unit vector of W Why ((x+) - (x-)) dot with Unit vector of W = Margin Width Can anyone help me?
I believe the length of the projection is w.u/|w| isn't it?
And how do I compute 'b'?
Can anybody explain me why the differentiation of 1/2 * ||w||^2 wrt w is just w and not ||w||?
wx + b >= 1 or <=1 , this means we are assuming that perpendicular distance between the two support vectors will always be 2 , how wx+b>=0 or <=0 makes sense but >=1 and <=1 how this is gonna hold ?
Great explanation, BUT I thought we were looking for the minimization of the Lagrange equation while the Professor keeps saying ""the maximization"". P.S. I know in general we need to find the maximum width
At https://youtu.be/_PwhiWxHK8o?t=1757 , I remember that the formula of Lagrange Multiplier is partial differential with a_i, why is b on the video? Can anyone help me, thanks.
why minimizing (|w|^2)/2 is a convenience rather than |w|?
Can someone explain me why he compares two equations against >=+1 and <=-1 where the original equation was >= 0 and <= 0? Wouldn't the sample to be positive W*Xplus + b should be >= 0 and W*Xminus + b should be <= 0? i don't get that part
at 9.33 ,he mentioned that w.x(+)+b >= 1 how exactly is that before this he mentioned it as 0 actually can anyone clarify my doubt
Did anyone help me to understand how min(L) turns to max(L)?? by no lagrangian duality??
I dont't understand how he uses Lagrangian multipliers! It made sense that he differentiated with respect to the different variables but then he talked about maximizing L, the lagrangian function!? i thought you use the equations you derive to solve a system of equations and get a solution. What I'm also missing are complementary slackness equations since is constraints are inequalities. I expected there to be a ton of equations, depending of the number of x's and a solution for w and b that gives you your hyperplane...
Did you figure out why? I have the same question
so you want to say, actually it will be x.w/|w| but he wrote w.x, right?
Bhullar, I can not get your logic. How come you use the value of a non-expanded variable on the same variables expanded values where if you expand the non-expanded variable, the outcome will completely change? Also, how come you say that, he did not expanded the the w(vector) in first equation and expanded it in second equation where he did say that at 16.39 ""w(vector) is a normal and what i can do is take the w and divide by the magnitude of w and that will make it unit normal "" ? So it was not unit normal, he made it unit normal by dividing with magnitude to get the width of the street. can you prove me wrong? I am just trying to understand. May be I am wrong. Also I am agree with Sebouth's last question.
i also do not understand the same thing. please somebody answer
He is replacing value of 'W' in equation that finds L. the term contains w^2
+ranvideogamer I see he made a mistake: x(pos) = 1 - b while x(neg) = -1 - b so x(pos) - x(neg) = 1-b - ( -1-b ) = 2. I also have another question as to why it's just 2 over the length/magnitude of w, what happened to the vector w on top of the magnitude?
This is really helpful. Ive only got one small question, at 21 minutes why is min(w) = min (1/2 w)?
Nice Tutorial Question : 1) At the time of calculating the width of the street, why the unit vector component is multiplied ?
Haebichan Jung But why did he subtract SUM of the constraint function instead of just the function in the Langrangian?
***** beause (x+) - (x-) is not perpendicular to the median line. For calculate the width of the street, you need a perpendicular vector. When you multiple it with the unit vector, it becames perpendicular.
+Murat Bykaksu How do we know multiplying the perpendicular vector by the difference vector gets us the perpendicular width between the boundary samples? The way I'm thinking of the width between the margin is as the opposite side of a triangle. The projection of x- onto x+ is the base, and the difference vector is the hypotenuse. Is there an error in my way of thinking?"
"That won't matter so whatever, whichever order you want to think of it.
And each of the ki's here I guess is between 0 and m minus 1.
So far so good.
So with this perspective, the base m perspective, I can define a dot product hash function as follows.
It's going to be parametrized by another key, I'll call it a, which we can think of again as a vector.
I want to define h sub a of k.
So this is parametrized by a, but it's a function of a given key k as the dot product of those two vectors mod m.",I am confused when the dot product is performed on the Universal Hashing algorithm. Why there we are considering k_d - k'_d = 0 and is not (mod m) of that is also 0.
"And you didn't even use it over here! If you're familiar with C++ or Java, self is a lot like this.
Self is an implicit argument passed in here.
Even though you specify zero arguments, it's considered the first argument.
And you'll probably see a lot type errors when you're first programming in object oriented programming that say that you've either passed in too many or too few arguments.
It has to do with this definition, with self.
Self says, I am talking about myself.
That's not particularly intuitive.
But I'll try to explain a little bit more.
When kpugh calls .sayHi(), sayHi() always has an implicit reference to whatever called it.","Hello, This lecture series is quite useful. I have a few questions. 1) How does an object come alive or a method start existing in memory ? 2) In C++, I have seen usage of malloc() and realloc(), could you help in explaining it in the context of python? 3) I would like to understand it with a different point of view ?"
"So you'd like the spread of the data to sort of be the same in all the dimensions.
So is there anything we can do to arrange for that to be true? Sure, we can just normalize the data.
So we can borrow from our statistics course and say, well, let's see, we're interested in x.
And we know that the variance of x is equal to 1 over n times the sum of the values, minus the mean value squared.
That's a measure of how much the data spreads out.
So now, instead of using x, we can use x prime, which is equal to x over sigma.","If you are talking about ""x prime"", that's not the derivative of x. It's a new random variable. More precisely, it's a random variable transformed from the original x. With the definition of ""x prime"", you can calculate its variance by plugging it in the formula. You will get 1."
"We're going to freeze the input layer and train the output layer using the sigmoid curve and see what happens when we do that.
Oh, by the way, let's run our test samples through.
You can see it's not doing anything, and the output is half for each of the categories even though we've got a trained middle layer.
So we have to train the outer layer.
Let's see how long it takes.
Whoa, that was pretty fast.
Now there's an extraordinarily good match between the outputs and the desired outputs.
So that's the combination of the autocoding idea and the softmax idea.
[? There's ?] just one more idea that's worthy of mention, and that's the idea of dropout.
The plague of any neural net is that it gets stuck in some kind of local maximum.
So it was discovered that these things train better if, on every iteration, you flip a coin for each neuron.
And if the coin ends up tails, you assume it's just died and has no influence on the output.
It's called dropping out those neurons.",How did the professor trained the Neural Net...any idea??
"That's how we designed the cut.
The cup was designed not to cross, not two separate any of these connected components.
So all the edges that we've added to T, those are OK.
They're not related to the edges that cross this cut.
But we may have already considered some lower weight edges that we didn't add to T.
If we didn't add an edge to T, that means actually they were in the same set, which means also those are-- I'm going to use my other color, blue.
Those are extra edges in here that are inside a connected component, have smaller weight than e, but they're inside the connected component.","in what situation is w(T') < w(T* - e)? Isn't w(T') = w(T* - e)?
onwards, if an edge(red) from 1st part ends in 2nd part, then the graph would not be an MST...but we have already supposed it to be a MST with e being an edge.... Please clarify this doubt....!"
"This element being t at position zero.
So you have to sort of wrap your mind around how this is working.
So if t is going to be this tuple element right here, then t at position zero is going to be this blue bar here.
So it represents the integer portion of the tuple.
So as we're going through the loop, this nums is going to get populated with all of the integers from every one of my tuple-- inside tuple objects.
So that's basically what this line here is doing.
At the same time, I'm also populating this words tuple.
And the words tuple is a little bit different, because I'm not adding every single one of these string objects.","I have a confusion, If tuples are immutable how does nums = nums + (t[0],) work?
it's concatenating an empty tuple to a singleton tuple (and on and on as the for loop runs). Consider; nums = 'walk' nums = nums + 'ing' print(nums) will output walking. 'Walk' and 'ing' are immutable as strings, but they can be concatenated together. In the same way a tuple can be concatenated to a tuple. nums = (1,2) nums = nums + (3,4) print(nums) will output (1, 2, 3, 4) What it's not doing is changing an element within a tuple, as tuples are immutable."
"So what we would like to do is for every relation type, we would like to split it between, ah, training message edges, uh, training supervision edges, validation edges, and test edges.
And then we would like to do this for every of the relation types and then kind of merge, uh, all of the message edges for the- for the training will, er, take all the training supervision edges for each separate relation type into the training supervision edges and then same for validation edges and for test edges, right? So kind of the point is that we wanna independently split edges from each relation type and then merge it together, uh, these splits.","When choosing negative edges for the training, do you also need to make sure that negative edge is not a validation edge?"
"And now, we've got to have a summation over all the constraints.
And each or those constraints is going to have a multiplier, alpha sub i.
And then, we write down the constraint.
And when we write down a constraint, there it is up there.
And I've got to be hyper careful here, because, otherwise, I'll get lost in the algebra.
So the constraint is y sub i times vector, w, dotted with vector x sub i plus b, and now, I've got a closing parenthesis, a minus 1.
That's the end of my constraint, like so.
I sure hope I've got that right, because I'll be in deep trouble if that's wrong.
Anybody see any bugs in that? That looks right.
doesn't it? We've got the original thing we're trying to work with.
Now, we've got Lagrange multipliers all multiplied.
It's back to that constraint up there, where each constraint is constrained to be 0.
Well, there's a little bit of mathematical slight of hand here, because in the end, the ones that are going to be 0, the Lagrange multipliers here.
The ones that are going to be non 0 are going to be the ones connected with vectors that lie in the gutter.
The rest are going to be 0.
But in any event, we can pretend that this is what we're doing.
I don't care whether it's a maximum or minimum.","Yeah it's just a slopy notation actually. It doesn't really make sense to derive a Lagrangian (or anything really) by a vector, what you're doing is deriving components by components. So for instance the first term in the Lagrangian is .5*(w_1^2 + w_2^2 + ... + w_n^2) where n is the dimensionality of the vector space. If you then derive with respect to the i-th component w_i, then you just get that component back, and so if you do it for each components, then you get the vector back. That's what is meant by this notation but I understand that it is very confusing (and actually wrong strictly speaking) if you've never seen it before
Why we add alpha with the constraints when doing langrangian???
Can someone explain why will Lagrange multipliers alpha(i)'s be zero in the case of the points lying outside the gutter and non-zero when the point lies in the gutter.
He didn't really explain why after substituting the `w = \sum y_i x_i \alpha_i` into L, we changed from minimizing L to maximizing L.
Can anyone tell if we are trying to maximize L or minimize L
How to calculate alpha the langrange multipler in it?
one more doubt I have when we found w vector then why we are keeping it in auxiliary function..As per langrange we should keep it in constraint
I don't follow the constraints - he only retains the equality type constraints in the Lagrange's Multipliers step. This (equality) happens only for the few samples at the gutters. What about the non-gutter samples? The constraints for these are greater than or equal to zero type, hence Lagrange's method can not be used. Aren't those getting ignored therefore? What am I missing here?
he even doesn't explain where the max(L) coming from. max(min(L)) == min(max(L)) in this case. He just skipped that part. Do u guys really think u understand his explanation?
How is the a_i in the Lagrange multiplier calculated?
Don't we have to minimize the lagrange term and not maximize it?
Might be a stupid question since I'm a newbie in ML, but does Lagrange function he made purposefully look like a (basic) cost function for SVM, cause it seem to me that every optimisation objective in ML I have covered could be represented by Lagrange function?
Great explanation, BUT I thought we were looking for the minimization of the Lagrange equation while the Professor keeps saying ""the maximization"". P.S. I know in general we need to find the maximum width
At https://youtu.be/_PwhiWxHK8o?t=1757 , I remember that the formula of Lagrange Multiplier is partial differential with a_i, why is b on the video? Can anyone help me, thanks.
at 9.33 ,he mentioned that w.x(+)+b >= 1 how exactly is that before this he mentioned it as 0 actually can anyone clarify my doubt
Did anyone help me to understand how min(L) turns to max(L)?? by no lagrangian duality??
I dont't understand how he uses Lagrangian multipliers! It made sense that he differentiated with respect to the different variables but then he talked about maximizing L, the lagrangian function!? i thought you use the equations you derive to solve a system of equations and get a solution. What I'm also missing are complementary slackness equations since is constraints are inequalities. I expected there to be a ton of equations, depending of the number of x's and a solution for w and b that gives you your hyperplane...
How do we calculate alphas?
Very good explanation but where do all alphas come from? I know the answer is ""from Lagrange multiplier"" but how we calculate them?
The alpha itself is the Lagrange Multiplier (usually used with lambda instead of the alpha symbol). It's the proportionality constant, what makes the derivative of the function (1/2norm(W)^2) proportional to the derivative of the constraint (y(w*x+b)=1). The function usually goes f (function) = *g (constraint). This means that both gradients are perpendicular to their contour lines/curve, and that there is a point of tangency between them. It's worth keeping in mind that Lagrange Multipliers show that the answer to a constrained optimization can be found when the contour line of the function being maximized/minimized is tangential to the constraint curve.
Haebichan Jung But why did he subtract SUM of the constraint function instead of just the function in the Langrangian?
in Lagrangian fuction how can we detect constraint?"
"One part is roughly the policy that you want your system to enforce.
This is roughly the goal that you want to achieve.
Like well, maybe, only I should be able to read the grades file for 6.858.
Or maybe the TAs as well, and all the co-lecturers, et cetera.
But there is some statement about what I want my system to be able to do.
And then, if you want sort of think about what kinds of policies you might write, typical ones have to do with either confidentiality of data, so the grades file is only accessible to the 6.858 course staff.
Another example of a security policy has something to do with integrity.
For example, only the course staff can also modify the grades file.
Or only the course staff can upload the final grades to the registrar's office.
That'll be great.
Then you can also think about things like availability.
So for example, a website should be available, even if the bad guys try to take it down and mount some sort of a DOS-- Denial of Service-- attack on it.
So this is all well and good.
So these are the policies that we might actually care about from a system.
But because it's security, there's a bad guy involved.
We need to understand, what are we thinking the bad guy is going to do? And this is typically what we call a threat model.","Can you provide for reference sources citing the three folded security approach (policy, tm and mechanisms)? Regarding the threat model, is there a generally accepted methodology you could mention (preferably free from product bias) specifically advised for system protection endevors ?"
"The reason why the second one is important is because-- let's look at our condition here.
So while n is less than five.
If you didn't have this line here, you would never increment n.
So every time through the loop, you just keep printing zeros.
And you would have an infinite loop.
I do want to show, though, what-- if you do have an infinite loop, it's not the end of the world.
So I can say something like-- so while true, print zero.
So this is going to give me an infinite loop in my program.
And-- whoop.
OK.
So notice it's just printing the letter p over and over again.
And if I let it go any longer, it's going to slow down the computer.
So I'm going to hit Control-C or Command-C maybe.
And it's going to stop the program from printing.","the number you mention in parenthesis in for loop doesn't printed on the screen
How come the infinite loop didnt work with 0 but worked with p"
"Benefit of this is it keeps your code organized and it keeps your code coherent.
So functions are going to be used to achieve decomposition and to create structure in our code.
We're going to see functions today in this lecture, and in a few weeks, you're going to actually see-- when we talk about object oriented programming-- how you can achieve decomposition with classes.
And with classes you can create your own object types like adding some floats.
You can create your own object types for whatever you want, but that's later.
OK so decomposition is creating structure in your code.
And abstraction is the idea of suppressing details.
So in the projector example, remember, abstraction was you didn't need to know exactly how the projector worked in order to use it.
And it's going to be the same idea in programming.
So once you write a piece of code that does a little task, you don't need to rewrite that piece of code many times.
You've written it once, and you write this thing called a function specification for it, or a docstring.",Can anyone ELI5 what it means to provide abstraction? She says that writing the docstring provides abstraction and I'm just not familiar with that meaning.
"That roughly make sense? What's going on in gzip? All right.
So I guess one thing we asked for the homework is how do you actually use Capsicum in OKWS? So what do you guys think? Would it be useful? Would the OKWS guys have been excited and switched to FreeBSD because this was much easier to use? Or is this a wash? So what do you think? How would you use Capsicum in FreeBSD? Would this be much different? Yeah.
AUDIENCE: So it means you can get rid of some of the jailing [INAUDIBLE].
PROFESSOR: Yeah.
That's true.
So truth seems to be completely superseded by this plan of having directory file descriptors and capabilities.
So that's great.
So you don't need the chroots setting it up.
That seems messy.
And this is much more precise, also.
Because you can-- instead of having a chroot with lots of little things in there, you have to maybe set the permissions on there carefully.
You can just open exactly the files that you need.
So that seems like a plus.
Any other benefits? Yeah.",What is OKWS?
"So this definition of NP is what I'll stick to.
It's this sort of-- I like guessing because it's like dynamic programming.
With dynamic programming we also guess, and guessing actually originally comes from this world, nondeterminism.
In dynamic programming, we don't allow this kind of model.
And so we have to check the guesses separately.
And so we spend lots of time.
Here, magically, you always get the right guess in only constant time.
So this is a much more powerful model.
Of course there's no computers that work like this, sadly, or I guess more interestingly.
So this is more about confirming that your problem is not totally impossible.
At least you can check the answers in polynomial time.","If we can't prove P = NP, can we prove P < NP?
Is there anything between polynomial and exponential? Is there anything that grows faster than n^C for every C, but slower than e^(n^) for every >0?
If problem A can be converted to problem B(which is NP complete) in polynomial time then, what can we say about problem A? Is problem A, NP then? I think that nothing can be said about A. Can someone please confirm?
Hello Prof. Erik Demaine, Is RSA algo in P or NP ?
If we have a problem A that is in NP and we reduce it to B then we know that B is NP. if we dont know if A is NP and we know that B is in NP are we sure that A is in NP?Could'nt be an algorithm that do not use this reduction but solves the A problem? is the second statement in https://youtu.be/eHZifpgyH_4?t=1426 correct?
Could someone please help me understand what did he mean by, ""Computer would guess an option from polynomially many options""? I am not able to think(and relate) of an example when an algorithm guesses while solving a decision-based problem. e.g. while determining a chess move. His next statement only adds to my confusion, ""The guess is guaranteed to be a good one"" :(
should NP be the set of decision problem that can be solved in polynomial time with Non-deterministic machine, or, the set of decision problems whose ""TRUE"" instances can be solved in polynomial time with ND machine. if the former, how is it different from co-NP
Whether something is in P or NP, that something must be a decision problem. It does not make sense to talk about whether an algo is in P or NP. For example, we can talk about the problem ""Is 73642387 a prime?"" and ask whether it's in P or NP. However, we can't talk about whether a MergeSort algo is in P or NP becus it's an algo, not a decision problem. Here's an explanation by Alon Amit regarding this confusion: https://www.quora.com/Is-finding-prime-numbers-in-P-or-NP Hope this helps :)
See what you mean. But this is not what the guy says. He is talking about a ""good guess which leads to a yes answer"", which implies that a guess is a satisfying assignment for a set of variables. There are an exponential number of such guesses. In your interpretation he should have talked about about a set of guesses, one for each variable. I stick to my point ;-) But thanks for pointing this out to me. It explains why the confusion emerged....
This may clear things up: a non-deterministic Turing machine can solve a problem with an exponential number of options in polynomial time, and it can solve a problem with polynomial options in constant time. In his definition, of NP he's referring to the later case, where a deterministic machine would have to check through each of N^k possibilities, but an non-deterministic machine could guess which of the N^k possibilities is the right one. You can think of this process as occurring at each branching of an exponential problem, where there could be up to N^k branches to check. The non-deterministic machine knows which branch to take (and makes this choice N^k times moving down the tree), whereas the deterministic one has to check every branch."
"e are going to generalize what we talked about, uh, last time, uh, which will be all about generalizing and mathematically formalizing, uh, graph neural networks.
The idea for today's lecture, is to talk about deep graph encoders and mathematically formalize them, and also show you the design space, the idea, the diversity of what kind of design choices, uh, do we have when we are making, uh, these types of, uh, decisions, uh, building these types of architectures, right? So what we want is we wanna build a deep graph encoder that takes the graph, uh, as the input, and then through a series of non-linear, uh, transformations, through kind of this deep neural network, comes up with a set of, uh, predictions that can be at node level, can be at the level of sub-graphs, pairs of nodes, um, and so on.","How come a common W and B works for all the node embeddings, like in example some has 2 no of input others have 3 additionally, since the order of the weights are also different wouldn't that also impact the contribution of each input. How are we sure that it would be okay using the same W and B for all nodes? Kindly educate if i understood it wrong"
"The only thing that Kye did here was scan horizontally.
And you can imagine that-- so Kye did not use, in order to imply the eight-- and so this word implication, imply, is something that we're going to use in a more technical sense as well when we write our code, but an implication essentially says these rules imply the location of the eight.
Right? And we didn't do a vertical scan.
We did not use the fact that-- in this particular implication, we did not use the fact that a column needs to have all numbers on it, and therefore all of the numbers on a column have to be unique.","I wish prof don't mix up imply and infer.
Why does he use range 09 for i and j (in general, and especially in the routine to find the next empty cell) ? The positions i and j can only have values 19, and what he says about using 0 to indicate an empty cell doesnt apply. Or am I missing something?"
"So you basically are rotating the thing into place.
So you move the parent down into the underfull node, and you replace the parent by the leftmost thing here.
Everyone see why that preserves everything? And the child is also shifted.
Make sure you see that.
So the child which was in this subtree is now in this subtree.
But then you can have the situation where you don't have a nice sibling to take care of your problems.
So in this scenario, the sibling is barely full.
It has three things, and it can't donate anything to you.
So what do you do in that case? So then you do something which is a parallel of the split operation.",can someone explain why in the last bit we chose to rotate 17 on the left over 30 on the right? thxxxxx in advance!
"So the system is at rest, which means that all the delays start out with output equal to 0.
So at rest means this delay has an output of 0.
Well, if I tell you the output is 0 times 0 for this delay, then it's a simple matter of stepping through what is the output for each corresponding input? So if I-- the special value of the delta function is that it's 1 at the time 0.
So at the time 0, there's 1 coming in.
That 1 makes it through the adder, adds to 0.
Well, this is 0 because it was at rest.
So the 1 adds to 0 and the output becomes 1.
Notice that the 1 also goes down this path, and goes through the gain of minus 1 to give me minus 1.
But I'm in step 0.
So at step 0, the output of the delay is 0, not minus 1.
So the output then, for time equals 0, is y equals 1.
Just like we solved for the difference equation-- after all, I'm hypothesizing that those two systems are the same, they better give me the same answer.
So then at the next instant, as the time index goes from 0 to 1, two things happen.
The input goes from 1 to 0, and the delay box gets updated.","The labels on the x axis are not the value of the x's but the time steps. The definition of the delta signal is that x[t]=0 for every t different from 0 so x[2]=0. The term x axis is confusing here, it would better be called time axis or just horizontal axis."
"ROFESSOR: The idea of congruence was introduced to the world by Gauss in the early 18th century.
You've heard of him before, I think.
He's responsible for some work on magnetism also.
And it turns out that this idea, after several centuries, remains an active field of application and research.
And in particular, in computer science it's used significantly in crypto, which is what we're going to be leading up to now in this unit.
It's plays a role in hashing, which is a key method for managing data in memory.
But we are not going to go into that application.
Anyway, the definition of congruence is real simple.
Congruence is a relation between two numbers, a and b.
It's determined by another parameter n, where n is considered to be greater than one.
All of these, as usual, are integers.",Why does he say 'Equivalent' instead of Congruent?
"All right, any other questions? Now we can get to the paper for today.
So reading for today, the author has basically proposed a bunch of factors that can be used to evaluate these authentication schemes.
And what's really cool about this paper, I think, is that it basically tries to say, look, a lot of us in the security community are fighting just based on aesthetic principles.
Like, we should pick this because I just like the way that the curly braces look in the proof.
We should pick this because it uses a lot of math mode.
And so what they say is, look, why don't we try to establish some type of criteria? Maybe some of the criteria are a little bit subjective.
Let's just try to have this taxonomy of ways to evaluate the authentication scheme.
And let's just see how these various schemes stack up.
And so the authors basically proposed three high level metrics for evaluating these schemes.
And so, the first metric is usability.
And so, the base idea here is how easy is it for users to interact with this authentication scheme.",What is the paper the teacher is referring ?
"So those are our six rules that we can use to understand Jeremy's world of the Harry Potter universe.
And we also start off with four assertions.
Let me not underestimate the value of always looking to the assertions.
It's one of my white star ideas that are up here on the board.
Let me see.
This.
Perfect.
Always check the assertions before using a rule.
This really tripped people up last year, and you'll see why, because we're doing last year's problem.
Our four assertions that we start.
With assertion 0, Millicent lives in Slytherin dungeon.
Assertion 1, Millicent is ambitious.
Assertion 1 is what tripped people up, so remember that Millicent is ambitious.
Assertion 2 Seamus lives in Gryffindor tower.","Suppose two assertions matches rule 0 i.e., assertion 0 and assertion 3. At first we fire rule 0 and assertion 0. What do we do next? Do we fire rule 0 with assertion 3 or rule 'X(any other that matches except 0)' with assertion 1?
For the part 3 of the question 1. can we say that someone can live in 2 places at the same time as an assertion ?
Would adding this assertion answer quesion 1 ""Milicent is S's friend""?"
"And as of now, we're just going to say you're going to pick some element x belonging to s.
And this choice is going to be crucial.
But at this point, I'm not ready to specify this choice yet, OK? So we're going to have to do this cleverly.
And then what we're going to do is we're going to compute on k, which is the rank of x, and generate two sub arrays such that I want to find the fifth highest element.
I want to find the median element.
I want to find the 10th highest element.
So I have to keep track of what happens in the sub problems.
Because the sub problems are going to determine, depending on how many elements are inside those sub problems, which I can only determine after I've solved those sub problems.
I'm going to have to collect that information and put it together in the merge operation.
So if I want to find the 10th highest element and I've broken it up relatively arbitrarily, it's quite possible that the 10th highest element is going to be discovered in the left one or the right one.",how do you get the medium of mediums?
"And then, we'll apply it to size.
So the idea with subtree augmentation is that each node in our binary tree can store a constant number of extra fields.
Why not? And in particular, if these fields are of a particular type-- maybe I'll call them properties.
I'm going to define a subtree property to be something that can be computed from the properties of the nodes' children.
So I should say this is of a node.
So children are node.left and node.right.
You can also access constant amount of other stuff, for example the node itself.","Question: If height can be the property that can be stored in each node , then why not depth can be stored ? It's written that Index and depth we can't store in node bcz they are global properties
Just think about a local property, as that depends only on the node's subtree. The depth depends on what happens above the node, so it cannot be local."
"And we can show- I'm gonna, ah, just, um, skip the math.
So, um, this we showed using Hoeffding's inequality, and you can apply the union bound for unioning across all H.
Except we can- first we're going to limit ourselves to, um- all right.
So let me start over.
So we got this bound for a fixed H.
Right? But we are interested in getting the bound for any possible H.
Right? So that's our next step.
Right? And the way we're gonna, gonna extend this pointwise result to across all of them, is gonna look different for two possible cases.
One is a case of a finite hypothesis class, and the other case is gonna be the case for infinite hypothesis classes.
So what does it look like? So, [NOISE] so let's first consider finite hypothesis classes.
So first we're gonna assume that the class of H has a finite number of hypotheses.
The result by itself is not very useful, but it's gonna be like a building block for, for the, for the other case.
So let's assume that the number of hypotheses in this class is some number K.
Right? Ah, we can show that- I'm not gonna go over the, the derivation, but I'm just gonna, um, write out the result.",Why do we calculate hoeffding's equality in application?
"We are going to basically back prop from all these events all the way to the beginning-- to the beginning of time, to the firr-- to the first node in the graph to update the parameters of the RNN.
And then, how about at the test time, at the generation time? We are basically going to sample the connectivity based on the predicted distributions, predicted probabilities, and we are going to replace the input at each step by the RNNs own prediction or own output.
So here we are going to flip the coin, the coin will say what it will say, and it will go as an input to the next one.
And I think, here, we are going to do the coin, whatever is the output here, should be the input to the next step, so it should be a 0 here, it's a mistake.
OK.
So that's the idea.
So the summary is we have costed the problem of graph generation as a problem of sequence generation.
Actually, a problem of a two-level sequence generation, node level sequence, and an edge level sequence, and we use recurrent neural networks to generate the sequences.
And what I want to discuss next is, how do we make the RNN tractable? And how do we evaluate? ","For teacher forcing, why are we even computing for loss when we are anyways replacing the predicted probability with the real/expected value?"
"But the exact same number, or that there have to be two different subsets of 90 numbers, of 25 digits, that have the same sum.
But we will take a much more modest application of the pigeonholing principle.
Namely, if I have a set of five cards that I have to have at least two cards with the same suit, why? Well, there are four suits-- spades hearts, diamonds, clubs-- indicated here.
And if you have five cards, there's more pigeons cards than suits holes.
So if you're going to assign a pigeon to a hole, again, the pigeons are going to have to crowd up.","i dont know what total injection is
Okey, I perfectly understand the principle but in what circumstances we use this, what is the benefit that this principle provides us? In which part of my life I am gonna need this ? Why should I thank this principle ?
I am a student of computer engineering, can you give me an example of this princable in programming issues?"
"So what are attributes? Attributes are going to be data and procedures that belong to the class, OK? Data are going to be the data representations and procedures are going to be ways that we can interact with the object.
The fact that they belong to the class means that the data and the procedures that we write are only going to work with an object of this type.
OK.
If you try to use any of the data or the procedures with an object of a different type, you're going to get an error because these data and these attributes will belong to this particular class.
So the data attributes is, what is the object, right? What is the data that makes up the object? So for our coordinate example, it's going to be the x and y values for coordinate.","I understand what classes are.But what are the use of classes? What are the things that can be done in class but not in a normal function?
Why should coordinate be an object. It has no behaviour in itself, it's just a tuple of two ints. Just data. And a function to compute euclidian distance is just a function that takes two coordinates, four integers or two arrays, depending on how you decide to represent coordinates as data."
"But what this says is that if, for example, c doubled, then you are saying that your number of levels is order 4 log n.
I mean I understand that that doesn't make too much sense, but it's less than or equal to 4 log n plus a constant.
And that 4 is going to get reflected in the alpha here.
When the 4 goes from 4 to 8, the alpha increases.
So the more room that you have with respect to this constant, the higher the probability.
It becomes an overwhelming probability that you're going to be within those number of levels.
So maybe there's an 80% probability that you're within 2 log n.
But there's a 99.99999% probability that you're within 4 log n, and so on and so forth.
So that's the kind of thing that with the high probability analysis tells you explicitly.
And so you can do that, you can do this analysis fairly straightforwardly.
And let me do that on a different board.
Let me go ahead and do that over here.
Actually, I don't really need this.
So let's do that over here.
And so this is our first with high probability analysis.
And I want to prove that warm-up Lemma.
So usually what you do here is you look at the failure probability.
So with high probability is typically something that looks like 1 minus 1 divided by n raised to alpha.
And this part here is the failure probability.
And that's typically what you analyze and what we're going to do today.",Any idea how can I understand how log n is derived?
"For loops, while loops, they naturally lead to what we would call iterative algorithms, and these algorithms can be described as being captured by a set of state variables, meaning one or more variables that tell us exactly the state of the computation.
That's a lot of words, let's look at an example.
I know it's trivial, but bear with me.
Suppose I want to do integer multiplication, multiply two integers together, and all I have available to me is addition.
So a times b is the same as adding a to itself b times.
If I'm thinking about this iteratively, I could capture this computation with two state variables.",How do we initialise variables in recursion in Python so that it continues values in every local scope without it's initial value in global scope?
"So that's going to be the probability of the statue appearing.
So I'll just check off those four boxes there.
And it looks like the probability of the statue appearing is about 0.355 in my model.
I don't think it's quite that frequent, but this is a classroom exercise, right? So I can make up whatever numbers I want.
Now, I could say, well, what's the probability of a statue occurring given that there's an art show? Well, I can limit my tallies to those in which art show is true, like so.
And in that case, the probability has just zoomed up.
So if I know there's an art show, there's a much higher probability that a statue will appear.
And if I know there's a hack as well as an art show going on, it goes up higher still to 0.9.
We can also do other kinds of things.
For example, we can go back to the original table.
And instead of counting up the probability we've got a statue, as we just did, we're going to calculate the probability that there is an art show.",What does a hack in this context even mean ?
"And these will tell us how much improvement we're getting by making a little movement in those directions, right? How much a change is given that we're just going right along the axis.
So maybe what we ought to do is if this guy is much bigger than this guy, it would suggest we mostly want to move in this direction, or to put it in 1801 terms, what we're going to do is we're going to follow the gradient.
And so the change in the w vector is going to equal to this partial derivative times i plus this partial derivative times j.
So what we're going to end up doing in this particular case by following that formula is moving off in that direction right up to the steepest part of the hill.
And how much we move is a question.
So let's just have a rate constant R that decides how big our step is going to be.
And now you think we were done.
Well, too bad for our side.
We're not done.
There's a reason why we can't use-- create ascent, or in the case that I've drawn our gradient, descent if we take the performance function the other way.","why does he say starting with the same weights, they would stay the same? They won't have the same derivative since one of them goes through an extra sigmoid function.
I don't quite get the last point: the computation with respect to width is w^2 (width squared).Can someone explain?"
"So I have good reason to believe that the mean I'm computing is close to the true mean, because my confidence interval has shrunk.
So that's the really important concept here, is that we don't just guess-- compute the value in the simulation.
We use, in this case, the empirical rule to tell us how much faith we should have in that value.
All right, the empirical rule doesn't always work.
There are a couple of assumptions.
One is that the mean estimation error is 0.
What is that saying? That I'm just as likely to guess high as gas low.
In most experiments of this sort, most simulations, that's a very fair assumption.
There's no reason to guess I'd be systematically off in one direction or another.","Under what conditions is ""conformity to expectation"" distinct from ""regression to the mean"" ? When are these phrases used equivalently ? by whom , and why ? In what ways does the use of statistically derived results differ between the population of typical social engineers and the population of physically science theorists , and ""Why?"" ?
What if the mean is zero?
So we cannot calculate confidence interval when empirical rule does not hold?
Can someone explain when he calculated the confidence interval on the mean, why did he use the empirical rule instead of the central limit theorem? Since he has 20 means shouldnt he have divided z,alpha/2*sigma by sqrt(20)?
The computer program is no really random as the random samples are generated using psuedo-random generators.
I understood almost everything except for assumptions underlying emprical rule. It says the mean estimation error should be 0. What is the mean estimation error ? Does he mean standart error of the mean?
Hello, thank you for your lecture on Monte Carlo. I have a question, I dont understand why it is better to run the simulation rather than just use the mean? For the roulette example, were providing the code the 1/36 probability and using a simulation to show that well win 1/36 times. But of course we know that already, and so why run the simulation? My question related to why Im researching Monte Carlo. I have a real world problem where I am trying to forecast unplanned shutdowns for machinery. I have a detailed history of unplanned shutdowns for the past 10 years. Say I have 1005 machines and I have on average 7 breakdowns per month. Can I use Monte Carlo to forecast how many machines will breakdown per month going forward? Or should I just assume Ill have 7 breakdowns per month? What would be the benefit of each? Im happy to self learn, Im hoping someone can point me in the right direction. Thank yoy"
"And still others are interpreted as fixing the control algorithm by which the creature operates.
So you see how that roughly goes? Would you like to see a film of that in action? Yes.
OK.
[? Solo ?] always likes to see the films.
STUDENT: How would you measure diversity in that graph? PATRICK WINSTON: The question is, how do I measure the diversity of the graph? I did it the same way I measured the fitness.
That is to say, I calculated the distance-- the actual metric distance-- of all the candidates for the next generation from all of the candidates that had already been selected.
I summed that up.
And from that sum, I could rank them according to how different they were from the individuals that were already in the next generation.
It's like giving a rank, and then from the rank, I use that kind of calculation to determine a fitness, ie, a probability of survival, and then I just combine the two kinds of probabilities.
STUDENT: So you always kept-- every time that you selected something, you cached those.
And you kept everything that you've ever selected [INAUDIBLE].","Hey guys, I might be a bit thick here, but what does the professor mean when he says near the end of the lecture -> ""We were amazed by the SPACE of solutions ... and not by the GENETIC algorithms'? Any further explanation is welcome :)
Why would i need to change the values (mutate them) to random values if i the original values were created randomly too?
Hey guys if we remove the diversity factor at some instance i.e. generation then that generation could be perfect?????
This is very helpful for me. But I have a question. What is Pc ? And how much is it. I watch the screen ,find the rank probability is 0.05. (1-Pc) equals 0.95,so 0.95^39 always more than 0.05,if Pc equals 0.05. I think I need some help.
I watched your lecture with great interest. I'm teaching myself Python by coding a GA. Often, when selection and reproduction are discussed, the biological model of two parents are combined into one offspring. I have a different idea. Say you have a starting population of 200. You apply your fitness function to score each member and then the grim reaper function to kill the bottom half in terms of fitness. You have a population of 100 members. Why not combine each member with every other member? (think nested loops). 100 * 100 (crossover) produces 10,000 new members. apply a mutation function randomly against the population and against each cell in the DNA string. Then reduce the population by 99% by fitness back to the original level of 100. In effect producing the next generation from the top 1 percent of the current generation. Have you considered such an approach? Can you give me your opinion? Thank you!
John David Deatherage How are you sure you won't take out the other top ones when you reduce the population?Newbie here
outstanding teacher. Thanks a lot. But can anybody explain it's real life application."
"So I need this one, that one, and that one.
So 4 plus 4 plus 4, plus 1 plus 5 plus 7 equals 25.
And so this implies elimination, because what I've done is found the max flow, which is strictly less than 26.
Which means that I have not saturated all of the edges that come out of s.
I have not been able to play all the games.
And so if I'd played all the games, I'd be exceeding some capacity constraint.
And if I push more flow in there, I'd be exceeding some capacity constraint on this side.
And that would imply that Detroit is eliminated, because some team would have 77 wins.
All right, so hopefully you've got the gist of it.
I'll put this in the notes.
Take a long, hard look at it.
That particular values aren't important.
The framework of translation is important.
And you can certainly ask questions of your TAs on Friday.
And I'll stick around here for questions.
","The last example ,from what I understood allows team NY to win only one game w.r.t the games it plays with the other 5 teams ,but can there be a case where the team wins a game against a team we have not considered and hence not allowed team 5 to still make it ? Please correct me if I am wrong
The capacities of inner edges (between the ""team-pairs"" layer and ""teams"" layer) are irrelevant and so are set to infinity. Each of these edges indicates the number of games a particular team won when playing against some other team (for example, flow through edge ""1-2"" -> ""1"" shows how many times Team 1 won while playing against Team 2). Clearly, the flow through each of these edges is bounded by the number of games each team-pair has to play. Since these numbers are already specified as capacities of source-edges and sink-edges, they effectively restrict max flow that can go through inner edges. Thus, there is simply no need to put additional restrictions on inner-edge capacities."
"But anyway, let's say if we can say that profs should not teach anyone one that they're advising.
Well, if we were saying that in logical notation, what we would say is that for every professor and subject, it's not the case that the professor has an advisee in subject j and the professor is teaching subject j.
So that's how you would say it in logic, but there's a very slick way to say it without all the formulas and the quantifiers.
I could just say that T, the relationship of his teaching, intersected with the relationship of has an advisee in the subject is empty.
There is no pair of professor and subject that is in both T and in R of V.
And this bottom expression here gives you a sense of the concise way that you can express queries and assertions about the database using a combination of relational operators and set operators.","""D  inverseR(J),"" states __the set of all students__ (denoted by D) happens to be a subset of __the set of subjects that have registered students__ (denoted by inverseR(J)). It's NOT true because the set of all students (that is, D) includes/contains the element Adam but the set of subjects that have registered students (that is, inverseR(J)) does not include/contain some subject that has registered the student Adam -- D  inverseR(J) is {Jason, Joan, Yihui, Adam}  {Jason, Joan, Adam} and is False."
"Dependency parsing, if you're keen on assignment 3, lots of data on the Universal Dependencies website.
There are now several websites that are collecting a lot of data sets.
So Huggingface has also just recently actually announced Huggingface Datasets, which is sort of an index of datasets.
And the Paperswithcode people also have Paperswithcode Datasets, so you can look at those.
There are many more.
Yeah, here's just a quick summary of thinking about a research project.
So this is just one example, right? That suppose you think summarization.
So that's going from a longer piece of text to a short summary of it.
So what you have to do, you need to find the data set.",Could anyone please tell which research paper were they required to read for first part of the project component?
"So again, they're not crossed.
So they don't cross the cut, rather.
So e is basically the first edge that we're considering that crosses this cut, because otherwise we would have added that other edge first.
So here, we have to do sort of the greedy argument again, considering edges by weight and e is going to be the first edge that crosses this particular cut, which is this connected component versus everyone else.
So e has to be the minimum weight edge crossing the cut, so the greedy choice property applies.
So we can put e in the minimum spanning tree, and this algorithm is correct.
OK? So we've used that lemma a zillion times by now.
That's minimum spanning tree and nearly linear time.
","onwards, if an edge(red) from 1st part ends in 2nd part, then the graph would not be an MST...but we have already supposed it to be a MST with e being an edge.... Please clarify this doubt....!"
"So, the claim holds, if we have a negative infinite shortest-path distance, then V is reachable from a witness.
So it suffices for us to find all the witnesses, find all the vertices reachable from the witnesses, and then mark them as minus infinity.
Does that make sense? OK.
So, now we finally are able to get to our algorithm.
Bellman-Ford.
And what I'm going to show you today is a little different than what is normally presented as Bellman-Ford.
The original Bellman-Ford algorithm does something a little different.
And because it does something a little different, which we'll talk about at the end, it's a little hairier to analyze.","Why can there be vertices with - shortest-path weight that are not witnesses? An example will be really helpful.
I am pretty sure most of you are witness or a victim of a negative cycle when leaarning about Negative Cycle Witness proof. What the flying fox is the use of the Negative Cycle Witness? Jason this is the part that needs to be addressed for this lecture."
"So when I run the program, it's going to start executing in the main function.
And pretty quickly, it calls redirect.
And the debugger is now stopped at the beginning of redirect.
And we can actually see what's going on here by, for example, we can ask it to print the current CPU registers.
We're going to look at really low level stuff here, as opposed to at the level of C source code.
We're going to look at the actual instructions that my machine is executing because that's what really is going on.
The C is actually maybe hiding some things from us.
So you can actually print all the registers.
So on x86, as you might remember.
Well, on [INAUDIBLE] architecture, there's a stack pointer.
So let me start maybe drawing this diagram on the board so we can try to reconstruct what's happening.
So what's going on is that my program, not surprisingly, has a stack.
On x86, the stack grows down.
So it sort of is this stack like this.
And we can keep pushing stuff onto it.
So right now, the stack pointer points at this particular memory location FFD010.
So some value.
So you can try to figure out, how did it get there? One way to do it is to disassemble the code of this redirect function.",Can anyone explain what he is doing with the code? I don't understand what he is doing. Thanks
"So for this class in general the vast bulk of this class is working with text and doing various kinds of text analysis and understanding.
So we do tasks like some of the ones I've mentioned, we do machine translation, we do question answering, we look at how to parse the structure of sentences and things like that.
In other years I sometimes say a little bit about speech, but since this quarter there's a whole different class that's focused on speech that seem a little bit silly.
[INAUDIBLE] --focus more and more on speech [INAUDIBLE]..
I'm now getting a bad echo, I'm not sure if that's my fault or your fault but anyway, answer, so the speech class does a mix of stuff.","It seems this course is theory based, where can I learn to code these concepts and algorithms?
Is this course suitable for beginners?"
"How do I maximize my score if all I were given were these pins? Suppose the pins to the left of i didn't exist.
How would I maximize my score on the remaining pins? Or for this suffix, given these four pins, what would I do? And there's some weird sub problems here.
If I just gave you the last pin, what would you do? Nothing.
That's clearly different from what I would do globally here.
But I claim if I can solve all suffixes I can solve my original problem, because one of the suffixes is the whole sequence.
So let's do it.
Sort by for bowling.
So here is our dynamic program.
The sub-problems are suffixes.","Question: is the proposed algorithm optimal when pins are -1,-1,-9,1? In the first step it will choose to hit first and second pins -1 and -1. On the second step the algorithm will choose to skip -9. On the third step the algorithm will hit 1. Overall score will be 2 if we use the algorithm. But optimal solution is to hit the second and the third pins -1 and -9, and then hit the last, so the score would be 10. Or I misunderstood something?
Why would it choose to skip -9? It will take the maximum between three possible games: [[-1,-9]] points: 9 [[-1,-1]] points: 1 [[-1,-1],[-9]] points: -8 So the algorithm picks the first option."
"So here's two of them right here, here's Q1 and Q2, the same size as Q, and then you hit outputs XQ1 and XQ2.
And so you're effectively doing the same amount of computation as before, but now you're sort of doing, you have different attention distributions for each of the different heads, so this is pretty cool.
OK, so those are the main modeling differences, right? We did key, query value attention, that's how we got the key, queries and values from the X vectors, and we saw how to implement that in the matrices that we're looking at.
And then we looked at the multi-headed attention, which allows us to look in different places in the sequence in order to have more flexibility within a given layer.
Now we're going to talk about our training tricks.
These are really important, it turns out and so, Yeah, thinking about them I think is something that we don't do enough in the field and so let's really walk through them.","Good lecture, but I think there is some confusing usage of the K, Q, and V matrices. You introduce them by saying: (1) K*x_i = k_i, Q*x_i = q_i, V*x_i = v_i In this way you're saying that X, Q, and V operate on x_i column vectors from the left to yield another column vector. But then later, when you stack the transposes of the x_i to produce the X matrix for the tensor attention calculation, you multiple from the right with K: (2) softmax(XK(XQ)^T) = Attention weights I think both K and Q should be transposed in (2) as you're using them in order to be consistent with the way you define the matrix in (1).
Are matrices K, Q, V parameters to be learned?
Great lecture, very clear! Thanks for making it freely available! In multihead self-attention, each head is going to produce values vectors of dimension d/h whereas the single head self-attention produces value vectors of dimensions d. That allows the multihead self-attention to look at multiple places at the same time however reducing the dimension is going to discard some information. Could it be possible that single head self-attention works better in some cases than multihead self-attention because it doesn't reduce the dimensionality?
Agree. It seems that this lecture cannot even tell the reason of why we should use K, Q, V clearly..."
"Another thing you can do is neural architecture search.
Yeah, this.
Here we'll see a way to parameterize a neural network.
This is one way to do it, other works use other ways.
Here what they do is they have an RNN that outputs the parameters of your neural network.
I mean, yeah, the parameters of its size.
So it outputs things like number of filters, filter height, width, stride for a CNN.
And when they apply this to an RNN, it produces this very big thing which no person would come up with from first principles, but this seems to work better than things like LSTMs or GRUs.
And this was in 2017, so this is a long time ago, but at the time, they achieved state of the art results in terms of error rates on-- I think this is ImageNet.
I'm not sure.
So yeah, through these applications-- we don't really have time to go into depth on any of those, but hopefully, you're kind of convinced that large-scale meta optimization is possible.","Hi Yoon-ho, I have a question. If I understand correctly, we need these approaches to overcome memory burden as well as any kind of burden coming from the large scale. Is there any reason we restrict these techniques to large-scale meta learning? I mean, arent those approaches able to be applied to any sorts of large-scale single-task machine learning?"
"And we might go through that as an example.
So there are definitely issues that can come up in gadget assembly, but this one I'm not worried about, let's say.
Yeah? AUDIENCE: Did the original Super Mario Brothers allow you to go left? PROFESSOR: No.
In the original Super Mario Brothers, you cannot scroll the screen left.
Mario can go left.
So this is all one screen.
You always have to generalize something in your problem.
And if you say the size of your screen, it's 320 by 240, or whatever in the original is constant, then you can solve Mario in polynomial time by dynamic programming.
So that's not as interesting.
Mario 1.
Of course, any other Mario, you can go left, except sometimes it forgets the status of your monsters.
Again, if you have that, you can solve it in polynomial time by dynamic programming.
So we're in the sort of, we'll say, as intended model, which is you can have a big level.
4K is already happening.
Imagine the future, you have a giant screen, and you play a giant level.
AUDIENCE: [LAUGHING] PROFESSOR: Other questions? Now, one question is is Mario Brothers an NP conjecture? No, I think by now we might have a proof that is PSPACE complete.
But that's not published yet, or even written yet.
So it's certainly not guaranteed, but we think so.
OK.
Last proof is Rush Hour.
Before I get to Rush Hour, I'm going to tell you about another source problem.",It seems that rush hour is also in NP. You can easily check if a sequence of moves are valid and does move the desired block. What am I missing?
"Yeah.
So we also have materials for the TA lectures, so you can-- we're going to have three lectures on each of these topics-- programming, linear algebra, and probability to kind of review some of the backgrounds for you.
This is a mathematically-intense course-- at least, according to-- of course, depending on your backgrounds.
But kind of a good portion of students found that this course is mathematically intense.
So just kind of a heads-up.
So it's probably good for you to have at least, at two out of the three, like among the three things, probably, you need to know at least two of them relatively well so that you don't get kind of entangled issues when you do the homeworks.",If I dont know multivariable calculus and linear algebra then learning ml is not possible ?
"This is just like the induction step.
We have to prove that p n implies p of n plus 1.
So if you have a preserved invariant, then if the property holds at the start state, then it's obvious that the property will hold for all of the states that you can possibly get to.
That p of r will hold for all reachable states.
And you can prove this by induction on the number of states, but I think it's clear that if you have a property that you begin with, and it doesn't change making a step, it's never going to change.
And that's all that Floyd's invariant principle states.
So in particular, since the property p holds in all reachable states, if there is any final state which the machine reaches, then p is going to hold in that state.","Good day! Fast exponentiation is a good trick. However, how do I know what is preserved in state machines. Is it though a trial-and-error, or is there a good way to come up with what is preserved? Thanks in advance"
"The guess and check method works if you're able to check if your solution is correct.
So if your guess is originally 0, you can say, is 0 cubed equal to the cube of whatever I'm trying to find the cube root of? So if I'm trying to find the cube root of 8, is 0 cubed equal to 8? No.
So the solution is not correct.
If it's not correct, guess another value.
Do it systematically until you find a solution or you've guessed all the possible values, you've exhausted all of your search space.
So here's a very simple guess and check code that finds the cube root of a number.
So I'm trying to find the cube root of 8.","Cube root simple guess. Why does that code keeps running?
what about the other challenge (if the cube are negatives)?
Am I stupid? In Approximations code we set guess=0.0 and increment=0.01. How can our answer be different than increment times number of guesses??
What I get from the approximation algorithm is that it will only give you an answer if the difference between guess**3 and cube is equal to epsilon, if it manages to jump above epsilon, guess will increase in value until it is <= cube, and thus the message ""failed to obtain a value"" will appear. How can we make it so that if the difference does jump above epsilon that the value will get will be the value of guess at time - 1 of that moment.
For bisection I wanted to use this code to take direct input, but error is coming. #To find the cube root of a number using bisection method cube=float(input(""Put the number... "")) accuracy=float(input(""Give accuracy limit... "") low=0.0 high=cube guess=((high+low)*0.5) iteration=0 while abs(guess**3.0-cube)>accuracy: if (guess**3.0)<=cube: low=guess else: high=guess guess=((high+low)*0.5) iteration += 1 print (guess, ""is close to the cube root of"", cube) print (""Total iteration="", iteration) **************************** syntax error in low=0 why?
can anyone tell me what am i doing wrong!! code: cube = int(input('type the no....')) for guess in range(cube+1): if guess**3 == cube: print(""Cube root of"", cube, ""is"", guess) else: print(""cube root does'nt exist"") output:- type the no....1(#1 was the input) cube root does'nt exist Cube root of 1 is 1
How do you do a bisection search for a cuberoot when x is a decimal? I cant be find an answer anywhere.
I know she said it is a simple if statement but I can't figure it out. What did you do to make it work? My thought is: if cube < 0: guess = -guess
What is the if statement for negative values in the bisection search
Any idea on the code for a negative integer? Tried a handful of if statements, but it kept turning my while loop into an infinite loop."
"So actually I'll leave this one up here.
So you guys can continue to see that.
All right.
So let's do our proof.
So what we're gonna wanna do now is we're gonna wanna try to prove something about, um, the regret but before and how quickly it grows for the upper confidence bound algorithm.
But before I prove that, I'm gonna try to argue to you that, um, we're gonna look at, sort of, the probability of failure of these regret bounds, of these confidence bounds.
So what I said here is that [NOISE] we're gonna define these upper confidence bounds like this in terms of the empirical mean for that arm so far plus this term that depends on the number of times we pulled that arm.
And what I wanna convince you now is, what is the probability, what is the probability that on one step, that on some step, step the confidence bounds will fail to hold.
Why is this bad? Okay.
So we wanna bound the probability that on any step, excuse me, as we're running our algorithm that our confidence bounds fail to hold.
Why? Because if they all hold, we can guarantee we're gonna be making some, um, we're gonna have some nice properties.
Okay.
So note, if all the confidence bounds hold, like, on every step then we can ensure the following.
So if all confidence hold, bounds hold then Ut of at, this is the actual arm we selected, is gonna be greater than q of a star.","Li Yep, I was referring to that moment. I think I got It wrong when I wrote that (using unions with probabilities is still terrible, they should use sums and mention dependence and how we get rid of it). She calculates is the probability of choosing the wrong arm at some step. Effectively when all the bounds are met, Q(a*) < Ut(at) (I got that reversed the first time). Therefore, Q(a*) > Ut(at) implies that some bound does not hold and thus, Q(a*) > Ut(at) is less likely than the failure of some bound."
"I'd have to define log to the base 2 in the base case, and that's easy to do.
What about in the constructor case? Well, the log to the base 2 of x, y is equal to the log to the base 2 of x plus the log to the base 2 of y for all the x, y's that are positive powers of 2, and so I have defined a log of the constructor x, y in terms of the function log of x and the function log applied to y.
It conforms to the standard definition of a recursive function on a recursively defined data type, [INAUDIBLE] positive powers of 2.
Now, this looks OK.
Well, let's just check it out.
So log of 4 is log of 2 times 2, which is by the definition of the log of 2 plus the log of 2, which is 1 plus 1, which is equal to 2, and guess what? That's correct.
The log of 8-- well, 8 is 2 times 4, so by the recursive definition, that's the log of 2 plus the log of 4, which we previously figured out log of 4 was 2, so we get 3 and the answer comes out right.
Now, remember, you're not supposed to in this reasoning use the properties that you know that log to the base 2 has because we're defining this function which we're calling log to the base 2 and implicitly claiming that it's right.","I dont understand the length vs depth proof. Suppose I have this string. [][][], the length is 6, and the depth is 1. 6 > 2^(1 + 1) = 4 I'm misunderstanding something here, could anyone enlighten me?
The way I understand it, by the definition of M, the added brackets for d(r) can only enclose s, so the depth being max(d(s)+1, d(t)) should be correct see this video: https://www.youtube.com/watch?v=TXNXT3oBROw"
"But now you have a name for it.
Once you have a name for something, you get power over it.
You can start to talk about it.
So I can say, if you're doing a generate and test approach to a problem, you better build a generator with certain properties that make generators good.
For example, they should not be redundant.
They shouldn't give you the same solution twice.
They should be informable.
They should be able to absorb information such as, this is a deciduous tree.
Don't bother looking at the conifers.
So once you have a name for something, you can start talking about.
And that vocabulary gives you power.
So we call this the Rumpelstiltskin Principle perhaps The first of our powerful ideas for the day.
This subject is full of powerful ideas.
There will be some in every class.
Rumpelstiltskin Principle says that once you can name something, you get power over it.
You know what that little thing is on the end of your shoelace? It's interesting.
She's gesturing like mad.
That's something we'll talk about later, too-- motor stuff, and how it helps us think.
What is it? No one knows? It's an ag something, right? It's an aglet, very good.
So once you have the name, you can start to talk about.
You can say the purpose of an aglet is pretty much like the whipping on the end of a rope.
It keeps the thing from unwinding.
Now you have a place to hang that knowledge.
So we're talking about this frequently from now into the rest of the semester, the power of being able to name things.
Symbolic labels give us power over concepts.
While we're here I should also say that this is a very simple idea, generate and test.
And you might be tempted to say to someone, we learned about generate and test today.
But it's a trivial idea.","I didn't understand the gyroscope example
why 2^4 possibilities in F Fx G Gn problem?
What does marshalling the resources of the perceptual apparatus going down means?
what was that gyroscope concept... can someone please explain, I didn't get it ."
"So I have to model the boards that I see in front of me, because those are the boards that I have control over that I can maximize.
I cannot would be Vi plus 1 j in there, simply because I don't quite know what that is, because of what I get eventually is not something I control.
It's going to be the board after my opponent has moved.
So all I'm going to do is I'm going to say the range becomes-- range is i plus 1 j.
And I'm going to say something else in a second here.
The range is i j minus 1.
And in both of these cases, the opponent moves.",Why do we need to minimize opponents move. If we just consider all possible move opponent can play and just maximize shouldn't we still get the correct answer? Unless we specifically demand the opponent to optimize as well.
"AVL tree sort is great and then it gets n log n time, probably more complicated than merge sort and you could stick to merge sort.
But neither merge sort nor set AVL tree sort are in place.
And so the goal of today is to get the best of all those worlds in sorting to get n log n comparisons, which is optimal in the comparison model, but get it to be in place.
And that's what we're going to get with binary heaps.
We're going to design a data structure that happens to build a little bit faster-- as I mentioned, linear time building.
So it's not representing a sorted order in the same way that AVL trees are.
But it will be kind of tree-based.
It will also be array-based.
We're going to get logarithmic time for insert and delete_max.
It happens to be amortized, because we use arrays.
But the key thing is that it's an in-place data structure.
It only consists of an array of the items.
And so, when we plug it into our sorting algorithm-- priority queue sort or generic sorting algorithm-- not only do we get n log n performance, but we also get an in-place sorting algorithm.
This will be our first and only-- to this class-- n log n in-place sorting algorithm.
Cool.
That's the goal.
Let's do it.
So what we're going to do, because we're in place, basically we have to have an array storing our end items.
That's sort of the definition of in-place, just using n slots of memory exactly the size of the number of items in our structure.","Is my understanding correct of why we prefer to use an array over the pointers approach? Since our implementation of insert, delete (and by extension, build) all comes down to insert_last or delete_last (with a bunch of value-swapping), each of which an array can support in constant time, it makes the pointer implementation less desirable because at each node we would need to store both a value and a pointer, which would effectively make our memory allocation double that of an array. (are there any other benefits that I missed?)"
"And we'll talk about those.
And then, in part, that will then motivate coming up with a more advanced recurrent neural network architecture.
So the first problem to be mentioned is the idea of what's called vanishing gradients.
And what does that mean? Well, at the end of our sequence, we have some overall loss that we're calculating.
And well, what we want to do is back propagate that loss.
And we want to back propagate it right along the sequence.
And so we're working out the partials of J4 with respect to the hidden state at time 1.
And when we have a longer sequence, we'll be working out the partials of J20 with respect to the hidden state at time 1.","Regarding vanishing gradients: h are not parameters - W are. Right? And for W, those gradients sum up, i.e., no vanishing. It seems unclear why dJ/dh should be important. Aren't we only interesting in updating W?"
"And so you can think of the residual layer norm as coming after each of the interesting things we're doing, right? We're doing one interesting thing here, multi-headed masked self attention, cross attention after each one, do residual and layer normal, help the gradients pass, et cetera, et cetera.
And then the output of this residual and layer norm is the output of the transformer decoder.
OK, and so the only thing so far that we really haven't seen in this lecture, is the multi-head cross attention.
And I want to go over it, it is the same equations as the multi-headed self attention, but the inputs are coming from different places.","Great lecture, very clear! Thanks for making it freely available! In multihead self-attention, each head is going to produce values vectors of dimension d/h whereas the single head self-attention produces value vectors of dimensions d. That allows the multihead self-attention to look at multiple places at the same time however reducing the dimension is going to discard some information. Could it be possible that single head self-attention works better in some cases than multihead self-attention because it doesn't reduce the dimensionality?
Agree. It seems that this lecture cannot even tell the reason of why we should use K, Q, V clearly..."
"What happens now if I interrupt a series of heads and produce a tail? STUDENT: [INAUDIBLE].
PROFESSOR PATRICK WINSTON: What's that? STUDENT: [INAUDIBLE].
PROFESSOR PATRICK WINSTON: The black one will go to 0.
What else happens? By the way, the blue one is the one with the highest probability of being a head.
[INAUDIBLE] Boom! That blue one shot up.
Not going up slowly.
It shot up.
Because now the preponderance of evidence with all those heads is that I've flipped the coin with a bias of 0.75 towards heads.
So let's clear this.
Pick any probability you want.
0.25, 0.5, and so on.
I don't know, let's pick 0.25 since we've been at the upper end.
So orange is 0.25.
And sure enough, the probability that I've selected the 0.5 coin is going up and up and up and up after the original irregularity.
The Law of Large Numbers is setting in.
And a probability that I've got that 0.25 coin in play is pretty close to 1.
All right.
So that's cool.
Now you say to me, that's awfully nice but stop.
Awfully nice, but not very real-world-ish.
So let me give you another problem.
It's well-known that you are, with high probability, of the same political persuasion as your parents.
So if I wanted to figure out which party a parent belongs to, I could look at the party that their children belong to, right? So it's just like flipping coins.","Thank you Prof.Winston for this lecture. starting minute 38 the professor shows that multiplying the probabilities of heads and tails will get us to a point where the probability of the fair coin is bigger (if we get an equal number of heads and tails). However, on each new sample this number will get smaller and smaller since we are multiplying fractions. however, later on in the demonstration (5 coins) i noticed that the probability of the right coin is getting increased till it hits 1. I didn't understand this part, if anyone can explain what happened i would appreciate it?"
"They branch in a binary sense, but you could have it generalized to any constant branching factor.
But for our purposes, binary's fine.
And what we said was that there were at least n outputs-- really n plus 1, but-- at least order n outputs.
And we showed that-- or we argued to you that the height of this tree had to be at least log n-- log the number of leaves.
It had to be at least log the number of leaves.
That was the height of the decision tree.
And if this decision tree represented a search algorithm, I had to walk down and perform these comparisons in order, reach a leaf where I would output something.
If the minimum height of any binary tree on a linear number of leaves is log n, then any algorithm in the comparison model also has to take log n time, because it has to do that many comparisons to differentiate between all possible outputs.
Does that make sense? All right.
So in the sort problem, how many possible outputs are there? What is the output of a sorting algorithm? AUDIENCE: [INAUDIBLE] JASON KU: What? What's up? A list-- in particular, given my input-- some set of items A that has size n-- what I'm going to give you is some permutation of that list.",I don't understand how n/2 is greater than (n/2)^(n/2) in 17 min of video
"You called square of a plus 2 in the environment of E1.
So it figures out what did you mean by a plus 3.
Well, you were in the environment E1.
So it means whatever a plus 3 would have meant if he had just typed a plus 3 in that environment.
So you evaluate a plus 3 in the environment E1, and you get 5.
So then, this new environment, E2, that is set up for this procedure, square, associates 5 with x.
Now it's ready to run the body.
So now, it runs this procedure, return x times x.
But now, what it's trying to resolve its variables, it looks it up in E2.","So can an instance run a procedure in other classes which is in the same environment as the instance? In the final example he showed that pat is in the same global environment as staff601 but when he was asking for pat.salutation was the computer looking into the attributes of staff601 which is in the same class with pat?
why does y[-1][1] get 8?
Why did he make another environment E2 x=5, when (a+3) =5 (a=2) . Wouldn't it just be simple to evaluate that expression in it's current environment?"
"So, cool so far, everyone? Good questions from everyone.
These are all questions that are things that often trip people up.
So, our next thing, Millicent is ambitious.
We sort of-- the cat is out of the bag because there is a question mentioning that there is an assertion that says that.
But I direct you guys to our favorite rule, rule two.
Shouldn't we use rule two to prove that Millicent is ambitious? The answer is if you want to lose four points or however many points, then yes.
But if you don't want to lose points, then this is already correct by virtue of assertion one.
So I guess I should write, also, that this over here, Millicent studies a lot rule, this rule is rule four.","Would adding this assertion answer quesion 1 ""Milicent is S's friend""?"
"All right, this is v1 here.
This is a directed path, v1, 2, 3, 4, all the way up to k.
There are k edges in this graph.
I claim to you that any path from v0 to vk, any shortest path from v0 to vk remains a shortest path after I reweight everything in this way.
So let's say this is path pi, and so it has weight w pi, which is really just the sum of all of the edge weights from vi minus 1 to vi, for i equals 1 to k.","doesn't connecting all vertices in V to a supernode s with 0 weight edges makes all delta(s, v) to 0, (and makes all h(v) = 0)? Or is the s in the Johnson's algorithm another s different from the supernode"
"Why do we care? Because let's suppose we had an algorithm to solve B.
That would be this arrow.
So let's say if we can solve B, then we can solve A by this diagram.
Take an instance of A, convert it into an equivalence of B, solve B, and then that solution is equal to the solution to A, so we solved A.
This is as hard as.
So what we say is B is as hard as A.
That's a definition of this hardness.
In general, depending on your definition of reduction, you'll get a different notion of as hard as, but we will stick primarily to polynomial time.","Nice video and good explanations! However, I believe reductions should lead to the result ""at least as hard as"" instead of ""as hard as""."
"Um, I wasn't going to talk about it in this class.
If you want to learn more, uh, ah, I think at the start of the class, I mentioned I was working on this book, Machine Learning Yearning.
So that book is finished.
And if you go to this website, you can get a copy of it for free.
Uh, uh, that talks about that.
Uh, and I also talk about this more in CS230 which, which goes more into the big data.
But you can, you can go, go and learn machine- you can also read all about it in, in Machine Learning Yearning.
Um, if the train and test sets are a different distribution.
Uh, yeah, but random shuffling would be a good default if you think you're training dev test on two different, right? All right.
Just one last thing I want to cover real quick which is, um, feature selection.
And so, um, so let me just describe what- so sometimes you have a lot of features.",Is there a chapter in the main notes correspond to this video?
"So for example, in this case, if we would want to say what is the neighborhood overlap between nodes A and E, because they have no neighbors in common, they are more than, um, uh, two hops away from each other.
Then the- if only in such cases, the return- the value of that it will be returned to will always be, zero.
However, in reality, these two nodes may still potentially be connected in the future.
So to fix this problem, we then define global neighborhood overlap matrix.
That is all of this limitation by only, uh, focusing on a hop- two hop distances and two-hop paths between a pairs- pair of nodes and consider, um, all other distances or the entire graph as well.
So let's now look at global neigh- neighborhood overlap type, uh, metrics.
And the metric we are going to talk about is called Katz index, and it counts the number of all paths, uh, of all different lengths between a given pair of nodes.","An important detail wasnt mentioned Beta generally cannot be any number in the interval (0, 1). In order for the infinite sum to converge, must choose beta in the open interval (0, max eigenvalue of A).
A^K approach counts paths that may contain the same edge more than 1 time. E.g., a path 1-3-4-3-1 will be considered as a valid part of length 4. It's ok since it still reflects graph topology, however it may be more noisy compared to # of simple paths between two vertices. At the same time, I'm not aware of a polynomial algorithm to calculate the number of simple paths. It might not been found - and that may be the reason to use the A^k approach.
hi~ i'm wondering should we ignore the diagonal values of A^(k)? because this value doesn't make sense in the link prediction or it allows a link connected to itself?
It depends on the interpretation of your graph (can you befriend yourself?), but generally speaking, we set the diagonals of A to be 0 in graphs meaning there are no self loops (simple graph)."
"So let's go from here over to here and say that the probability of a whole bunch of things-- x1 through x10-- is equal to some product of probabilities.
We'll let the index i run from n to 1.
Probability of x to the last one in the series, conditioned on all the other ones-- sorry, that's probability of i, i minus 1 down to x1 like so.
And for the first one in this product, i will be equal to n.
For the second one, i will be equal to n minus 1.
But you'll notice that as I go from n toward 1, these conditionals get smaller-- the number of things on condition get smaller, and none of these things are on the left.","I'm thinking about the exact same thing, It doesn't make sense for i to start at n, it should start at 1."
"It just prints x and crashes.
But in other situations, you might have other pieces of code in your program that are doing interesting stuff that you really do want to execute.
And that's sort of called return to libc attacks for, again, somewhat historical reasons.
But it is a way to bypass the security measures.
So in the context of buffer overflows, there's not really a clear cut solution that provides perfect protection against these mistakes because, at the end of the day, the programmer did make some mistake in writing this source code.
And the best way to fix it is probably just to change the source code and make sure you don't call getS() very much, like the compiler warned you.
And there's more subtle things that the compiler doesn't warn you about.
And you still have to avoid making those calls.",Can anyone explain what he is doing with the code? I don't understand what he is doing. Thanks
"So the property that distinguishes the weak from the strict is this property of reflexivity.
A relation R on a set is reflexive if every element is related to itself, if and only if a R a for all little a in the domain capital A.
And what we can observe immediately is that the path of the walk relation-- G star-- which includes walks of length zero is reflexive because, by definition, there is a length zero walk from any vertex to itself.
So if you're going to play with axioms, then you can reformulate asymmetry-- the idea of asymmetry except for elements being related to themselves.",what is D*?
"Um, and this is also true if you use batch gradient descent.
[NOISE] If you use batch gradient descent [NOISE] then the update rule is this.
Um, Yeah, right, [NOISE] okay, alright.
And so it turns out you can derive gradient descent for the support vector machine learning algorithm as well.
You can derive gradient descent optimized W subject to this and you can have a proof by induction.
You know that no matter how many iterations you run during descent, it will always be a linear combination of the training examples.
So that's one intuition for how [NOISE] you might see that assuming W is a linear combination of the training examples, you know is a- is a reasonable assumption.","Is it really hard to prove that w is a linear combination of the training samples? The training samples represent some set of vectors in a vector space. They might not span all of the space, and in fact they can't span more than N where N is the number of training samples (and they might span less, if there's any linear dependence among them). But they span some subspace of dimension M. The vector w defines a hyperplane that divides that space into classes. Well, the training samples span that subspace, and therefore any vector in that subspace is a linear combination of them. That leaves the non-spanned dimensions to consider, but your training sample conveys no information whatsoever about those, so there's nothing to do about them. I hope linear algebra was a prerequisite for this class."
"If I get excited, I'll start talking all kinds of nonsense.
So this will focus me a little bit more on the class, and you'll see how long I last, all right? So what we're going to do in this first three sections of the class, first three lectures is kind of build up increasingly sophisticated machine learning models.
And what you're going to see is that they are very, very similar to a model that you probably already know and love, which is linear regression.
If you don't know linear regression, don't worry.
Today's lecture is effectively going to be talking about linear regression with slightly fancier notation and some little bits around the algorithm, but it's basically just fitting a line, OK? It's really hopefully going to be something that you've seen and you can grab on to.",Apologize in advance if I sound rude but I would like to know what sets out this course different from Andrew NG's Machine Learning course on Coursera?
"So for this type of general recurrence, we don't have a very good way.
Instead, what we'll do is just take a random guess, and then see if it is correct.
So I don't really need to guess in this case, because I know it's O of n.
So let's just assume our expectation of Tn is theta of n.
What does that mean again? It means I can find some constant, such that this expectation is bounded by a constant times n.
So far so good? Now, we can use induction, assume that this holds for everything up to n minus 1.
And we're going to show this also holds for n.
Then we're done, right? Now, we'll just plug that in.
The expectation of T n will be less or equal than this sum from half of n to n.","seems missing the probability Pr(j) term
why does the summation of j from ceiling(n/2)-1 to n-1 equal n^2 times a constant??? Is it because we're replacing the summation with the closed form of the summation?
how did he get 2/n as the probability of picking a number? He said that numbers are repeated, can anyone explain this?
Every number has a 1/n probability. The summation goes from 1 to N. However, the probabilities of the left half and the right half are symmetric, so its simplified such that the summation goes up ceil(N/2) and each weight is doubled, hence 2/n."
"So, OK, the question is, if I want to-- we'll get there in a second-- but if I want to multiply, and it's so easy to do in sample land, why don't I just sample a and b and then multiply them? That's right, but [? effect ?] sampling is not so easy.
It takes quadratic time.
Let's go there now.
Because we have n samples to do, each one will cost linear time.
Remember, to evaluate a polynomial takes linear time.
If you want to think of it in a matrix-- let's enter the matrix-- then we get a big matrix.
So we're given the xi's and we just want to evaluate a given polynomial whose coefficients are given by a0, a1, a2, and minus 1.
Our goal is to compute the yi's-- y0, y1, y2, to yn minus 1-- and if you know a matrix-vector product, you take this row with that column.
You take the dot product that multiplies corresponding entries.
You get y0.
That is the definition of the polynomial evaluation.
I'm just going to write a bunch of these rows so you get the pattern.
Pretty simple.
This is called the Vandermonde matrix.
I'll call it V.
And in general-- I don't have room for it, so let me go-- In general, if we look at V ij, it's just-- sorry.","if you want to multiply two polynomials, why not just evaluate both of them, and then just multiply de results??
That would only result in a singular value. The goal is to multiply two polynomials to output another polynomial.
is there a mistake ? we know V.A = Y ( V - vandermond matrix, A - coefficien matrix, Y - samples matrix) (multiplying by V inverse i.e. V^(-1) both sides) => V^(-1).V.A = V^(-1).Y => A = V^(-1).Y So to go from samples matrix to coefficient matrix we need to do V^(-1).Y right ??"
"F is first applied to every, uh, element of the multi-set summed up and that that is passed through the function Phi and this way, uh, the- the injectivity, uh, is preserved.
And because of the universal approximation theorem, we can model f and Phi with a multi-layer perceptron, and now we have an injective, uh, aggregation function, because MLP can learn any possible function, and, uh, um, and, uh, the other MLP also can learn any function, meaning, it can learn the function f as well.
So, uh, what is now the most expressive graph neural network there is? The most expressive graph neural network there- there is, is called Graph Isomorphism Neural Network or GIN for short.","Thanks for the lecture, it's really amazing how GIN is related to the previous WL kernel. I have a question about the MLP_phi in the GIN layer (P.62). Since the function could be injective relying on MLP_f and is determined by the summation of MLP_f(x), and the final transformation MLP_phi should not change or affect the injectivity, so are MLP_phi necessary?
Are there applications where one would prefer GCNs or GraphSage over the most expressive GINs (Graph Isomorphism Networks)?"
"So the plan for today is we're first going to motivate large scale meta optimization, what it is and why we should do it.
We're going to look at some applications of this.
And then we're going to look at two approaches in particular that can handle large scale settings.
And by the end of the lecture-- we won't have time to go into deep detail into most of this stuff.
But at least the goal is to scenarios where large scale stuff makes existing meta learning approaches fail.
And broadly understand some techniques for handling those types of scenarios.
Yeah, so I think we can roughly summarize a lot of the meta learning approaches as doing direct back propagation.
So this is the black box model that you learned in homework one, I believe.
And it's just the network that can take all of your support data and your query data and outputs predictions.
And in the same way, MAML also-- MAML and all the optimization-based approaches here to you just-- it's one big computation graph that takes in your source and query data.","Hi Yoon-ho, I have a question. If I understand correctly, we need these approaches to overcome memory burden as well as any kind of burden coming from the large scale. Is there any reason we restrict these techniques to large-scale meta learning? I mean, arent those approaches able to be applied to any sorts of large-scale single-task machine learning?"
"Okay.
So I'm gonna write some concepts on a board.
There's gonna be a bunch of, um, concepts I'm going to introduce, and I'll just keep them up on the board for reference.
So feature vector is again an important notion and it's denoted Phi, um, of x on input.
So Phi itself- sometimes, you think about it, er, you call it the feature map which takes an input and returns, um, a vector, and this notation means that returns in general, ah, d-dimensional vector, so a list of d numbers.
And, um, the components of this feature vector we can write down as Phi_1, Phi_2, all the way to Phi_d of x.
Okay.
So this notation is, eh, you know, convenient, um, because we're gonna start shifting our focus from thinking about the features as properties of input to features as kind of mathematical objects.
So in particular, Phi of x is a point in a high-dimensional space.
So if you had two features, that would be a point in two-dimensional space, but in general, you might have a million features, so that's a feature, ah, it's a point enough, a hundred- ah, uh, million dimensional space.
So, you know, it might be hard to think about that space, but well, we'll see how we can, you know, deal with that in a later in a, in a bit.
Okay.
So- so that's a feature vector, you take an input and return a list of numbers.
Okay.
Um, and now, the second piece is a weight vector.",Why phi(x) is an array in each point value?
"And sort of, as a result, this is very much an iterative process.
And the thing you end up realizing at every iteration is, well, here's the weakest link into my system.
Maybe I got the threat model wrong.
Maybe my mechanism had some bugs in it because it's a software and it's going to be large systems.
They'll have lots of bugs.
And you sort of fix them up.
You change your threat model a bit.
And you iterate and try to design a new system, and hopefully, make things better.
So one possible interpretation of this class-- well, one danger-- is that you come away thinking, man, everything is just broken.
Nothing works.
We should just give up and stop using computers.
And this is one possible interpretation.
But it's probably not quite the right one.
The reason this is going to come up or you're going to think this way is because, throughout this class, we're going to look at all these different systems, and we're going to sort of push them to the edge.
We're going to see, OK, well, what if we do this? Is it going to break? What if we do that? Is it going to break then? And inevitably, every system is going to have some sort of a breaking point.
And we'll figure out, oh hey.
This system, we can break it in if we push this way.
And this system doesn't work under these set of assumptions.
And it's inevitable that every system will have a breaking point.
But that doesn't mean that every system is worthless.
It just means you have to know when to use every system design.
And it's sort of useful to do this pushing exercise to find the weaknesses so that you know when certain ideas work, when certain ideas are not applicable.
And in reality, this is a little more fuzzy boundary, right? The more secure you make your system, the less likely you'll have some embarrassing story on the front page of New York Times saying, your start up company leaked a million people's social security numbers.",Is this course useful for someone who has not taken any sort of computer science course and has little/no experience in coding? Or is there another lecture series I should start with?
"So you have to do the insertion appropriately.
So that's what we have to do next.
But any questions about that complexity that I have up there? All right, good.
I want a canonical example of a list here, and I kind of ran out of room over there, so bear with me as I draw you a more sophisticated skip list that has a few more levels.
And the reason for this is it's only interesting when you have three or more levels.
The search algorithm is kind of the same.
You go up top and when you overshoot you pop down one level, and then you do the same thing over and over.","Skip list is kind of like van Emde Boas Tree, right?
It's difficult to update a Skiplist efficiently without backpointers"
"For example, the search space of passwords had better be large or a cracker can just search through them all to find one that works.
And finally, we'll talk about discrete probability theory, which is simply a version of probability theory, where we can get by with sums instead of getting into the complications of integrals.
So here's a quick sanity check, or vocabulary check.
Do you know what discrete means? And I'll give you a hint.
It doesn't mean discreet.
If you don't know, this is a good moment to stop the video and look it up.
","What the prerequisites to this course? I've skipped a lot of math while studying at school, so I'm just trying to figure out what level of math do I actually need to follow along this course? Is general understanding of algebra will be enough?
kindly tell the difference between the COURSE CONTENT 6.042 and 6.042J .(later one is class recording and earlier one is slides)
First of all, thanks for another series of awesome content. Leaving the more interactive nature aside, what is the difference between this course and then one taught by professor Leighton? In other words, at MIT, is there a reason (aside from staffing) as to why the same course is taught by two different professors? For example is this one more geared towards a set of people whereas the 2010 one is geared towards others? Thanks in advance.
I notice there are 18.01 A,B,C. three parts. Are all three parts prerequisite or just the 18.01A? thanks."
"And the central thing that's a bit-- as before, we're going to take our loss and we're going to back propagate it to all of the parameters of the network, everything from word embeddings to biases, et cetera.
But the central bit that's a little bit different and is more complicated is that we have this WH matrix that runs along the sequence that we keep on applying to update our hidden state.
So what's the derivative of Jt of theta with respect to the repeated weight matrix Wh? And well, the answer to that is that what we do is we look at it in each position and work out what the partials are of Jt with respect to Wh in position 1 or position 2, position 3, position 4, et cetera, right along the sequence.","Regarding vanishing gradients: h are not parameters - W are. Right? And for W, those gradients sum up, i.e., no vanishing. It seems unclear why dJ/dh should be important. Aren't we only interesting in updating W?"
"We instead use what's called stochastic gradient descent.
And stochastic gradient descent is a very simple modification of this.
So rather than working out an estimate of the gradient based on the entire corpus, you simply take one center word or a small batch like 32 center words and you work out an estimate of the gradient based on them.
Now that estimate of the gradient will be noisy and bad because you've only looked at a small fraction of the corpus rather than the whole corpus.
But nevertheless, you can use that estimate of the gradient to update your theta parameters in exactly the same way.
And so this is the algorithm that we can do.
And so then if we have a billion word corpus we can if we do it on each center word.
We can make a billion updates to the parameters we pass through the corpus once rather than only making one more accurate update to the parameters once you've been through the corpus.
So overall, we can learn several orders of magnitude more quickly.
And so this is the algorithm that you'll be using everywhere, including right from the beginning, from our assignments.
Again, just an extra comment of more complicated stuff we'll come back to.
[AUDIO OUT] the gradient descent is a sort of performance hack that lets you learn much more quickly.
It turns out it's not only a performance hack.
Neural nets have some quite counter intuitive properties.","While using Stochastic Gradient descent, if we choose a corpus of 32 center words, how do we make updates to the outside (context) words that surround it. Because these words will show up when we compute the likelihood and if out corpus doesn't include them then how does their probability of occurring get updated? Thanks!"
"In fact, in this case, she'll actually be compatible with these two boys.
But all we need is one boy.
Since there's no bottlenecks, if I pick a girl, she's got to be compatible with some boy.
And I'm just going to arbitrarily match her with that boy-- match g with b.
Now, the consequence of that is that, if I now remove g and b from the graph, I am left with a graph in which at worst I've shrunk the set E of S by that girl that I've matched with B, which means that E of S, which used to be strictly greater than S has gone down by at most one.",what is S?
"So these diagonal edges are exactly the cases where the letters match.
Here H equals H.
I equals I here, so I draw this diagonal edge.
That's that first case, where the letters are equal, and so I recurse by increasing i and j.
That's why I get a diagonal edge.
There's also one over here, where T equals T.
So for those, we're getting a 1 plus.
You see this 1 is 1 larger than the 0.
This 2 is 1 larger than the 1.
This 1 is one larger than the 0.
And for every other vertex, we are rehearsing this way and this way.
We see what those two numbers are, and we take the max.
So this whole diagram is just filled in by-- for each position, where they're not equal.
We look at the guy below.
We look at the guy to the right.
Those are the slightly smaller substrings.
We look at those values.
We take the max.
As long as you compute this in a generally right-to-left and bottom-up fashion, whenever you're trying to compute a guy, you will have the-- its predecessors already computed.
That's the topological order of this graph.
And then, at the end, we get our answer, which is 2.
And now, if we pay attention to where we came from-- for example, this vertex had to come from this direction-- 2 is the max of 2 and 1, so I highlight the 2 edge.
And if I follow this path, there should be a unique path to some base case.
We don't know which one.","Is the topology reversed when you go from top down vs bottom up? I'm a bit confused by the topology. Are suffix subproblems always have topology of decreasing i ?
Hard time understanding relation between graphs and answer of problems"
"Different way of thinking about it, and the reason this is really nice is a method called memoization, is if I call fib of 34 the standard way it takes 11 million plus recursive calls to get the answer out.
It takes a long time.
I've given you some code for it, you can try it and see how long it takes.
Using the dictionary to keep track of intermediate values, 65 calls.
And if you try it, you'll see the difference in speed as you run this.
So dictionaries are valuable, not only for just storing away data, they're valuable on procedure calls when those intermediate values are not going to change.
What you're going to see as we go along is we're going to use exactly these ideas, using dictionaries to capture information, but especially using recursion to break bigger problems down into smaller versions of the same problem, to use that as a tool for solving what turn out to be really complex things.
And with that, we'll see you next time.
","Would inclusion in the lecture of the 'call stack' or Python's symbol table concept help explain recursion? As you recursively call the function object's return value, the frames get 'popped' off the stack (symbol table on Python I think?) Sorry, self-taught. Still learning every day.
Ken MacDonald Yeah I know but as n goes up, there will be hundreds of steps. So do they recurse on the other because the same rules still applied?
I'm a little confused on one point. I understand recursion, but from what I am reading online, most of the time it is more efficient in terms of processing times to write code using an iteration than a recursion. I get that some people might find one more readable than the other and that for many programs, most users wouldn't notice a difference, but wouldn't it be a best practice to normally iterate to make code as efficient as possible?"
"Last week, we saw a few ways to do sort.
Some of them were quadratic-- insertion sort and selection sort-- and then we had one that was n log n.
And this thing, n log n, seemed pretty good, but can I do better? Can I do better? Well, what we're going to show at the beginning of this class is, in this comparison model, no.
n log n is optimal.
And we're going to go through the exact same line of reasoning that we had last week.
So in the comparison model, what did we use when we were trying to make this argument that any comparison model algorithm was going to take at least log n time? What we did was we said, OK, I can think of any model in the comparison model-- any algorithm in the comparison model as kind of this-- some comparisons happen.","What if you have all collisions in counting sort? Wont that be n^2?
I don't understand how n/2 is greater than (n/2)^(n/2) in 17 min of video"
"Because at the edge of the unit circle, the distance from the origin is equal to 1.
If you made me guess, then I would look at the distance here and compare it to the distance at the next time step.
I realize this is a blackboard.
It's not entirely to scale.
But for the purposes of this demonstration, I'd like to say that the signal at this time step is 0.5 the signal from the previous times.
Likewise at the next time step, I would like to say that this signal is 0.5 the signal from the previous time step, and so on and so forth.
Therefore, I'm going to graph my pole right here.
Let's take a look at this graph.
I've drawn these squiggles to indicate that the unit sample response exceeds the bounds of the space that I gave for this graph.
So just assume that these values are much larger than I've drawn them.
The first thing that I notice about this unit sample response graph is the fact that not only am I increasing in a way that does not seem to change in any way-- we're going to end up diverging-- is that I'm actually alternating about the x-axis.",How do you get .07 from 1.6 and .09 from -.63? I have tried looking this up but can't figure it out.
"So for every vertex in the graph, we take a literal.
So if the number of vertices is n, we have n literals because it's corresponding to each vertex.
Also, we take a dummy literal.
Let's call it Z.
So now how do we get our clauses? So the general idea is that if a vertex is in the clique, you will assign it 1.
If it's not in the clique, we will assign it 0.
Everything outside of the clique is 0.
So this clause, not xi or not xj-- so what is the value of this clause normally? So let's say xi and xj are both outside the clique.","why do we have to create a dummy variable z???
What is the definition of Z? what's its role?"
"So one way to interpret the contrastive loss function is basically you're trying to classify between-- classify, kind of, one image from all the other images in the data set.
And you can sort of think of this, kind of, the denominator of this loss function is trying to sum over all the other examples in the data set.
You're trying to classify between one example and everything else.
And in the summation right here, the examples, the negatives, that have the smallest distance are really going to dominate this sum.
Because we're exponentiating here, and so something that has a distance of, say, like 0.01 versus a distance of 10.
Because we're exponentiating this, this is going to play a much like e to the -0.01 versus e to the negative 10.
This is a much, much larger number.
And so this is going to play a much larger role in the summation right here.","Also, why did the professor use a negative sign for the distance? The paper clearly doesnt do that. I think this is corrected but it makes it confusing"
"This is a conservative estimate.
And so in particular, what does that mean? It means that v is big O of E.
This is a case where we have to be quite careful about big O being an upper bound, right? In this case, typically, v is much less than E-- well, depends how many edges, like if you have a really dense graph or not.
But in this case, what does that mean? That means that mod v plus mod E is really just big O of mod E, right? Because this is big O of mod E plus big O of mod E.
And that means that our problem really runs in v times E time, which is what we wanted in our problem.
Are there any questions from our audience on part a here? Cool.
AUDIENCE: I don't quite understand why-- where you went from the first statement to the second statement there.
At least one edge implies v is less than or equal to E over 2.
JUSTIN SOLOMON: Oh, yeah.
So I guess there's sort of two things that matter here, right? Every vertex is adjacent to one edge at most.
And every edge-- yikes.
Every edge is adjacent to two vertices.
I guess, actually, it's the second one that matters.","In problem 1, the condition for |V| and |E| in a connected graph seems to be incorrect: |V| <= |E|/2. Consider a graph with 3 vertices and 2 edges; it is connected but |V| <= |E|/2 does not hold. I think the condition should have been |V| <= |E|+1 (by induction; one edge connects two vertices and each next edge adds at most one vertex to the connected component). But as the next statement, |V|=O(|E|) still holds, it does not invalidate the rest of the proof."
"And because we are keeping them in order in this sequence, I'm appending to the end.
Then, when I go and read off the different things, then I'm returning them in a stable way in the way that I want them to be.
Does that makes sense.
And it's not overwriting the work I did on the lower significant digits.
So how long does this take? This also only takes order n plus u, because I'm instantiating this thing of size u.
And then, how big are these data structures? Well, maybe I'm storing one, a constant amount for each index.
So that's a u overhead.
And then I'm paying 1 for every item I'm storing.
These things are only the lengths.",what if there is a three digit number such as 456 in that list? what is the most significant and least significant in this number?
"I guess there are conditions in which a coin flip is better than a-- it is a weak classifier.
If the two outcomes are not equally probable, than a coin flip is a perfectly good weak classifier.
But what we're going to do is we're going to think in terms of a different set of classifiers.
And we're going to call them decision tree.
Now, you remember decision trees, right? But we're not going to build decision trees.
We're going to use decision tree stumps.
So if we have a two-dimensional space that looks like this, then a decision tree stump is a single test.
It's not a complete tree that will divide up the samples into homogeneous groups.
It's just what you can do with one test.","Why is a coin flip a weak classifier if p1>p2 with p1+p2=1? 0.5p1+ 0.5p2 still is 0.5.
Could be a biased coin. You are assuming p1 = p2, but there could be a coin that is heavier on one side and so has a higher probability of landing on one side. That's all he meant.
Hi I didnot understand this concept,can you share some references where I can better understand the concept?"
"Not the least of which, by the clever name that we chose.
But this is obviously just a little function.
It runs through a loop-- sorry, a for loop that takes i for each element in a list L.
And it's checking to see is i equal to the element I've provided.
And when it is, I'm going to return true.
If I get all the way through the loop and I didn't find it, I'm going to return false.
It's just saying is e in my input list L? How many steps is this going to take? Well, we can certainly count the number of steps in the loop.","That unsorted list, I didn't understand how is the complexity (1+4n+1) ?
for i in range(x+1): x from 0 to x , doesn't this have (x+1) times ops?
top coder, I know that there is two loops in the second half of the code, in which ""e in res"" is an implicit loop. But when the teacher says ""I'm only looping over tmp, which is at most going to be len(L1) long"", that is false. To prove my point, imagine L1 and L2 to be filled with all 1's. (L1 = [1,1,1,1,1], L2 = [1,1,1,1,1]). In that case, the line if e1 == e2 will always be true and execute with every iteration of its inner loop, thus making tmp the size of len(L1)*len(L2) (and if len(L1)==len(L2) then tmp will be len(L1)^2). I agree though that in such case O(n^2) is still applicable to the lower code half aswell as all of the code."
"And now, we have several techniques that we can use to draw some decision boundaries.
And here's the same problem.
And if we drew decision boundaries in here, we might get something that would look like maybe this.
If we were doing a nearest neighbor approach, and if we're doing ID trees, we'll just draw in a line like that.
And if we're doing neural nets, well, you can put in a lot of straight lines wherever you like with a neural net, depending on how it's trained up.
Or if you just simply go in there and design it, so you could do that if you wanted.
And you would think that after people have been working on this sort of stuff for 50 or 75 years that there wouldn't be any tricks in the bag left.","How did they come up with the kernels?
What about multiclass classification? (i.e non binary classification)
We have got historical reports and we need to find the score (whether report is effective or useful or not) based on supervised learning. While doing the supervised learning process, we have to upload the words lists that are available in the reports. We have to assign the weightage for the word lists and run through all the reports to find the number of occurrences. Now I am not able to understand how these word lists can be used to find the usefulness of the report? Based on your experience can you please explain how this word lists can be used as a supervised data in finding the score of the report?"
"And if you put all n queens down, closed your eyes put n queens down and then ran a conflict check, and then did that over and over, over and over, it would be n raised to n.
But that's not what we're doing.
What we're doing is if you look at the code, we are saying if-- this is an important statement, actually.
Thanks for the question.
That is pruning your search, and that is making it significantly less than n raised to n.
A nice exercise.
Remember what I said about opportunity for homework, when I get questions.
A nice exercise is to go to 8 raised to 8 and then go to the eight queens problem and figure out what that number is in terms of the number of times you're actually checking for no conflicts.","Back in last lecture, the 4 x 4 example, we were talking about we'd possibly have to place one of our previous queen in a different row to solve the problem. Here we are assuming that once a queen is placed in a column, we don't have to worry about it again. I understand that for each new queen we are placing in the new column, we only have to worry about it conflicting with the existing queen(s), but what if we weren't able to place a new queen in the n-th column without conflicting with some of the existing queen(s), and the possible solution could be achieved by placing one of the previous queen differently?"
"So modeling improvements of both kinds are really, really important.
So it's good that we're using self attention which is this cool thing that had these properties.
But if we couldn't train it, it wouldn't be useful.
OK, so here's how the transformer builds the key, query, and value vectors.
We have x1 to xt, the input vectors to our transformer layer.
OK, and we're gonna go ahead and put the transformer encoder here.
So we have one of these vectors per word, you can say.
And again, each xi is going to be a vector in dimensionality d.
And here's how we compute the keys, queries and values.
We're going to let each key ki, which we saw before, be equal to some matrix k times xi where k is d by d, right? So this is a transformation from dimensionality d to dimensionality d.
We're going to call this the key matrix k, and we're going to do the same thing for the queries.
OK, so we're going to take the xi, multiply it by a matrix, get the query vector, and we'll do the same thing for v.
OK, so you can just plug this in, right? Now instead of saying that all the k, q and the v are all the same as x, they all are slightly different because you apply a linear transformation.
What does this do? Well, you can think about it as like well, the matrices k, q and v can be very different from each other, right? And so they sort of emphasize or allow different aspects of the x vectors is to be used in each of the three roles, right? So we wrote out the self attention equations with the three roles to indicate the different things are being done with each of them.
So maybe k and q are helping you figure out where to look, and so they should be a certain way, they should look at different parts of x.
And then v the value, maybe you want to pass along a different information than the thing that actually helps you access that information.
So this is important.
How do we do this? In practice, we compute it with really big tensors.
So we had our X vectors, which is what we've been talking about sort of word by word, is where you had the sequence xi, x1 to xt.","Good lecture, but I think there is some confusing usage of the K, Q, and V matrices. You introduce them by saying: (1) K*x_i = k_i, Q*x_i = q_i, V*x_i = v_i In this way you're saying that X, Q, and V operate on x_i column vectors from the left to yield another column vector. But then later, when you stack the transposes of the x_i to produce the X matrix for the tensor attention calculation, you multiple from the right with K: (2) softmax(XK(XQ)^T) = Attention weights I think both K and Q should be transposed in (2) as you're using them in order to be consistent with the way you define the matrix in (1).
Are matrices K, Q, V parameters to be learned?"
"o the algorithm that allows us to identify high modularity score communities is- it's called uh, the Louvain Algorithm, from the University Louvain in Belgium.
That's why, uh, this name.
So it is a very scalable and very popularly- very popular algorithm.
Kind of it's a de facto thing you would use if you want to partition the network into a set of clusters.
So Louvain algorithm is a greedy community detection algorithm that uh, scales uh, um, ah, n log n, where n is the number of nodes in the network so it can uh, um, scale to large networks.
It supports weighted graphs, uh, and it provides us hierarchical communities.
So it doesn't only provide us clustering at one level, but it provides us kind of clustering of clusters.
So we get this notion of a tree or a dendrogram, how individual nodes join into groups and how these groups can then be further joined into the super groups uh, and so on.
Um, and as I said, it is very widely used, there are fast implementations available, uh, It works very quickly, ah, and works well in practice.
It finds uh, clusters, communities that have a high modularity.
The algorithm operates in two phases, where basically we wanna greedily assign the nodes to communities to maximize modularity.",What exactly modularity is?
"But now that I mentioned it, I expect it's a familiar idea.
And it's pretty obvious too if you think about it for a minute.
Here's a way to think about it.
Given an nonempty set of integers, you could ask, is 0 the least element in it? Well, if it is, then you're done.
Then you could say, is 1 the least element in it? And if it is, you're done.
And if it isn't, you could say 2, is 2 the least element? And so on.
Given that it's not empty, eventually you're to hit the least element.
So, if it wasn't obvious before, there is something of a hand-waving proof of it.
But I want to get you to think about this well ordering principle a little bit because it's not-- there are some technical parts of it that matter.
So for example, suppose I replace nonnegative integers by nonnegative rationals.
And I asked does every nonempty set of nonnegative rationals have a least element? Well, there is a least nonnegative rational, namely 0.
But not every nonnegative set of rationals has a least element.
I'll let you think of an example.
Another variant is when, instead of talking about the nonnegative integers, I just talk about all the integers.","I am quite disappointed that when introducing the Well Ordering Principle, he does not at least mention the Axiom of Choice (AC). I understand AC to be the foundational axiom leading to the Well Ordering principle, and the subject of a fascinating historical and current debate.
can you please help me to make 3 analogies for the well ordering principles (WOP) and well ordering axiom or WOA
What is an example of a non-empty set of nonnegative rationals that does NOT have a least element??? can't wrap my head around that
But a set of negative integers also has the smallest number in it! Like that: -1; -8; -12, the least number here should be -12. What am I missing?
How can there be an empty set of non-negative integers? What's the difference between ""non-empty set of non-negative integers"" and ""a set of non-negative integers?""
It is interesting to know that zero is considered nonnegtive Would assume it is negative due to the fact zero = 0 the lack of any real number? Is there any proof on this?
he did not finish his proof of squared root of 2 is irrational.can anyone explain the remaining part? just dont understand how we can prove by wop
I don't see where the square root of 2 comes into play here. Explanation seems to drift away to the least element theroem without ever coming back... ? Am I missing something ? Thanks in advance
Consider a nonempty subset of nonnegative integers such as S = {0}. Does S have a least element? If it had couldn't we also say that it has a greatest one too? So, the least element would be identical to the greatest element which somehow sounds nonsensical! Dear Professor Meyer, please, could you share your thoughts on my given example?
I reviewing the fundamental knowlodge for computer science, but it's the second time that square of 2 is not clear to me how was proved that is rational, I know what is rational, but I didn't undestand this explanation of prove. I will look for anothers proves or people explaning in another way.
Consider the following set: {x: x  (0, ), x is rational} Can you tell me which is the least element in this set; 1/100 or 1/1000? Whichever element you decide is the least, you can always come up with a less element.
I think it was suggested that with N integers you always know that the smallest number is 0 but the smallest number with Z can go to -
A finite set would, but the set of all negative integers is a set of negative integers and has no least element.
There is in This set but not in ""EVERY"" set. e.g. {1, 0, -1, -2, ...} this dotdotdot implies there's no least element.
but this does not prove irrationality unlike proof by contradiction. Please elaborate if i am wrong"
"So how do we know that if you give it-- ERIK DEMAINE: This function.
STUDENT: --random variables, it won't prefer certain numbers over others? ERIK DEMAINE: So this function may prefer some numbers over others.
But it doesn't matter.
All we need is that this function is independent of our choice of ad.
So you can think of this function, you choose all of these random-- actually k and k' are not random-- but you choose all these random numbers.
Then you evaluate your f.
Maybe it always comes out to 5.
Who knows.
It could be super biased.
But then you choose ad uniformly at random.","Hey can you elaborate on Fubini? I just got this: Pr{a_d = -(k_d-k'_d)^{-1}*a_i*(k_i-k'_i)} = (independence of a_d from a_i) = Pr(f(a_d))*Pr(f(k,k',a_i not d)) = E_{a_i}[Pr{a_d=f(...)]. Would appreciate any insight!"
"Oh, I just want to mention, when I'm getting the rid, I'm actually using this cool zfill() function here, or method, which actually pads the beginning of any number with however many zeros in order to get to that number here.
So the number 1 becomes 001 and so on.
So it ensures that I have this nice-looking ID type thing that's always three digits long.
So let's try to work with this Rabbit object.
Let's define what happens when you add two rabbits together, OK-- in this class, not in the real world.
OK.
So if I want to use the plus operator between two rabbit instances, I have to implement this __add__ method, OK? So all I'm doing here is I'm returning a new Rabbit object, OK? Whoops, sorry about that.
And let's recall the __init__ method of the rabbit, OK? So when I'm returning a new Rabbit object, I'm returning a new Rabbit object that's going to have an age of 0.
Self-- so the Rabbit object I'm calling this method on is going to be the parent of the new rabbit.
And other is going to be the other parent of the new rabbit, OK? So if we look at the code, and I run it, this part here, I'm creating three rabbits, r1, r2, and r3.
Notice this class variable is working as expected, because the IDs of each of my rabbits increments as I create more rabbits.","What is the point of naming the class variable self.rid? She never used it, I don't understand that. If she directly used Rabbit.tag on the last line what was the point of creating that instance variable? Can someone please answer this? Thanks
Excuse me but I don't quite understand why the _eq_ method will be called over and over again when comparing objects directly. Thanks in advance~~^^
why 1 == 2 won't get you the ERROR msg? They are integers, not from the rabbit class."
"And the integral from 1 to n of log of x plus the last term, which is log of n.
In case you don't remember from first term calculus, the integral of log of x is, in fact-- has the indefinite integral is x log of x over e, which you can easily check by differentiating x log x over e.
ln means natural log, remember.
In computer science, L-O-G, log means log to the base 2 unless you explicitly put some base on it like log, L-O-G, sub 10.
So ln is the natural log from calculus.
And plugging in this value for the indefinite integral of log of x and using the bounds 1, n, what we come up with is that the sum of the logs is bounded between n times log n over e and n times log n over e plus log of n.","Please what is ""e"" stands for? on n/e . How can I find e number?"
"You'd only have one data point for every state.
There would be no repeating.
So, it's really hard to learn because um, they're- all states are different.
Um, and in general if we wanna learn how to do something, we're gonna either need some form a generalization or some form of clustering or aggregation so that we can compare experiences, so that we can learn from prior similar experience in order to what to do.
So, if we think about assuming that your observation is your state, so the most recent observations that the agent gets, we're gonna treat that as the state.
Then we- the agent is modelling the world is that Markov decision process.
So, it is thinking of taking an action, getting observation and reward, and it's setting the state, the world state- that the environment state it's using to be the observation.
If the world- if it is treating the world as partially observable um, then it says the agent state is not the same, um, and it sort of uses things like the history or beliefs about the world state to aggregate the sequence of previous actions taken and observations received, um, and uses that to make its decisions.
For example, in something like poker, um, you get to see your own cards.
Other people have cards that are clearly affecting the course of the game.
Um, but you don't actually know what those are.
You can see which cards are are discarded And so that's somewhere where it's naturally partially observable.","To put it simply, Markov's Assumption is ""that the future actions are influenced only by the present*, not the *past states"". In other words, Markov's Assumption emphasizes only the independencies across the time domain, NOT the independencies across different features. Hence, explaining the concept of Markov's Assumption by saying one feature, such as ""blood pressure"", is dependent on other features, such as ""exercise"", is missing the point on time domain dependency. The professor's other example, such as ""hot-or-not-outside"", is simply wrong. Blood pressure can be dependent on the temperature outside, but still satisfies Markov's assumption, as long as knowing current temperature provides sufficient information about blood pressure, regardless of the temperature in the past. In fact, Markov's Assumption is so widely applicable exactly because most natural processes are continuous in time, i.e. knowing the current state is often sufficient to ignore the past states. The common examples of processes that violate Markov's Assumption are human discretion and randomness."
"And then-- well maybe I'll stick to these colors.
Again, these two points only appear in this clause gadget.
These dots are actually these dots.
So there's one of these pictures for x1.
There's another one for x2, x3.
And so xi has one of these wheels.
I want this dot to be one of these dots of the wheel.
And then I want this dot to be one of the dots in the xj wheel with the false setting, one of the red dots.
I want this one to be xk true setting in the xk wheel.
So these things are all connected together in a complicated pattern.",i got lost once he started drawing the wheel. can anyone explain?
"So if you look at the math carefully, if you sort of treated the two vectors as the same, so if you use the same vectors for center and context and you say, OK, let's work out the derivatives, things get uglier.
And the reason that they get uglier is it's when I'm iterating over all the choices of context word, oh my God sometimes the context word is going to be the same as the center word, and so that messes with working out my derivatives.",Objective function seeks to maximise the probable likelihood of context word given center word. However should it also not try to minimise the probability of incorrect context words given center word?
"So that's practical application number one.
And that requires you to interpret your chromosome as an indicator of the steps in the plan.
Another example is drawn from a UROP project a student did for me some years ago.
He was a freshman.
He came to me and said, I want to do a UROP project.
And I said, have you taken 6.034? And he said no.
And I said, go away.
And he said, I don't want to go away.
I want to do a UROP project.
So I said, have you read my book? He said no.
I said, well, go away, then.
And he said, I don't want to go away.
I want to do a UROP project.
So I said, I don't have any UROP projects.",outstanding teacher. Thanks a lot. But can anybody explain it's real life application.
"?] So we have to make sure that if we have some unencrypted JavaScript code running on our browser, then we should assume that that could've been tampered with an attacker because it wasn't encrypted.
It wasn't authenticated over the network.
And consequently, we should prevent it from tampering with any pages that were delivered over an encrypted connection.
So the general plan for this is we're going to introduce a new URL scheme.
Let's call HTTPS.
So you often see this in URLs, presumably in your own life.
And there's going to be two things that-- well, first of all, the cool thing about introducing a new URL scheme is that now, these URLs are just different from HTTP URLs.",what is an attacker change jquery and create new hash?
"The recurrence will be something that's not particularly difficult to solve.
So I want to now make a more quantitative argument that the variable being n as to how many elements are guaranteed to be greater than x.
And essentially what I'm saying, which is I'm writing out what I have on that picture there, half of the n over 5 groups contribute at least three elements greater than x except for one group with possibly less than five elements, which is the one that I have all the way to the right, and one group that contains x.
So for all the other columns, I'm going to get three elements that are greater than x.
And so if you write that out, this says there are at least three n over 10, because I have half of all of those groups, minus 2.
And I'm not counting perfectly accurately here, but I have an at least.
So this should all be fine.",Where did 7n+7/10 + 7 come from in the end?
"You can have 0 inputs or as many as you'd like.
Function should have a docstring.
This is how you achieve abstraction.
So it's optional, but highly recommended, and this is how you tell other people how to use your function.
Function has a body, which is the meat and potatoes of the function-- what it does.
And a function's going to return something.
It computes its thing and then it gives back-- spits back some answer.
OK here's an example of a function definition and a function call.
Function definition is up here.
I'll just draw it here.
This is the function definition up here.
And this is the function call down here.
So remember, someone has to write the function that does something to begin with.",Can anyone ELI5 what it means to provide abstraction? She says that writing the docstring provides abstraction and I'm just not familiar with that meaning.
"Um, and then finally, by convention, we put a one-half there- put a one-half constant there because, uh, when we take derivatives to minimize this later, putting a one-half there would make some of the math a little bit simpler.
So, you know, changing one- adding a one-half.
Minimizing that formula should give you the same as minimizing one-half of that but we often put a one-half there so to make the math a little bit simpler later, okay? And so in linear regression, we're gonna define the cost function J of Theta to be equal to that.","why in cost function he did 1/2 and not 1/2*m ?
I didn't understand the linear regression algorithm is there any way to understand it better ??
why is it that the cost function has the constant 1/2 before the summation and not 1/2m?
I don't really understand what you mean by 1/2m. However, from my understanding, the 1/2 is just for simplicity when taking the derivative of the cost ftn the power 2 will be multiplied to the equation and cancellyby the half.
I have been wondering why we need such an algorithm when we could just derive the least squares estimators. Have you seen any research comparing the gradient descent method of selection of parameters with the typical method of deriving the least squares estimators of the coefficient parameters?"
"We'll see how to achieve this assumption in a little bit.
Let me first prove to you that this is enough.
It's going to be basically the same as the 006 analysis.
But it's worth repeating just so we are sure everything's OK.
And so I can be more precise about what we're assuming.
The key difference between this theorem and the 006 theorem is we get to make no assumptions about the keys.
They are arbitrary.
You get to choose them however you want.
But then I choose a random hash function.
The hash function cannot depend on these keys.
But it's going to be random.
And I choose the hash function after you choose the keys.","If you haven't yet found your answer: When you build the hash table and generate the hash function(s) (by choosing a random 'a') you store and re-use the 'a'. 'a' is not chosen randomly for each k-particular. You choose one 'a' for each hash function you use. The main idea is randomly choosing a hash function (by selecting a random 'a') to more likely get a random distribution of the keys across the bins, instead of assuming keys are random and having a static hash function.
Isn't u the number of all possible hash functions? all possible keys is different"
"But yeah, we printed this value.
And then it crashed.
Why did crash? Why do you think? What actually happens, right? So we jump to printf.
And then, something went wrong.
Yeah? Well, we changed the return address so that when we return from redirect, we now jump to this new address, which is that point up there, right after printf.
So where's this crash coming from? Yeah? AUDIENCE: Is it restricted because your i is supposed to be some sort of integer, but-- PROFESSOR: No, actually, well the i is like, well it's a 32-bit register.
So whatever's in the register, it'll print.
In fact, that's the thing that's in the register.
So that's OK.
Yeah? AUDIENCE: [INAUDIBLE] main returns.
PROFESSOR: Yes.
Actually, yeah.
What's going on is, you have to sort of-- OK, so this is the point where we jumped.
It's set up some arguments.
It actually calls printf.
printf seems to work.
printf is going to return.
Now actually, that's fine, because this call instruction put a return address on the stack for printf to use.
That's fine.
Then main is going to continue running.
It's going to run the sleeve instruction, which doesn't do anything interesting.",Can anyone explain what he is doing with the code? I don't understand what he is doing. Thanks
"Um, and let's think that, ah, all other, er, edges in the graph are- are the training message passing, ah, edges.
So we wanna use the training message-passing edges to predict, ah, the likelihood or the existence of this, ah, training, ah, supervision edge of interest.
What we also have to do in link prediction is that we have to create negative instances.
We have to create, ah, negative edges.
So the way we create negative edges is by perturbing the supervision edge.
So for example, if this is the supervision edge, then one way how we can do- do it is that we corrupt the tail of it, right? So we maintain the head, we maintain node E, but we pick some other node, ah, that node E is not connected to with the relation of type r_3.","When choosing negative edges for the training, do you also need to make sure that negative edge is not a validation edge?
RCGN doesn't use the node type, only the edge type, does it? Just making sure I didn't miss anything. Overall, the node type can often be derived from the relation type, e.g. in citation network. Thus the node type may be not important at all."
"Whereas by taking them as separate vectors that never happens, so it's easy.
But the kind of interesting thing is saying that you have these two different representations sort of just ends up really sort of doing no harm, and my wave my hands argument for that is since we're kind of moving through each position, the corpus one by one something-- a word that is the center word at one moment is going to be the context word at the next moment, and the word that was the context word is going to have become the center word.
So you're sort of doing the computation both ways in each case.
And so you should be able to convince yourself that the two representations for the word end up being very similar, and they are not identical both for technical reasons of the ends of documents and things like that, but very, very similar.
And so effectively you tend to get two very similar representations for each word, and we just average them and collect the word vector.
And when we use word vectors we just have one vector for each word.
That makes sense thank you.
I have a question purely out of curiosity.
So we don't know when we projected the vectors, the word vectors onto to the 2D surface we saw like little clusters of squares are similar to each other, and then later on, we saw that with the analogies thing, we kind of see that there's these directional vectors that sort of get like the rule of or the C of O or something like that.
And so I'm wondering, is there-- are there relationships between those relational vectors themselves such as, like is the rule of vector sort of similar to the C of O of vector which is very different from like-- makes a good sandwich with vector.","I am wondering how the two vectors (Uw, Vw) are determined for each Word?
Do areas of sparsity in the high dimensional word2vec space mean anything: For example, can you say - some word should exist here, but doesn't?"
"And that this is essentially a flow value, and we showed-- this is implicit summation notation, we showed, using a little bit of algebra, that this equals f of V t.
And that's going to hold because of flow conservation primarily.
So that was not particularly surprising.
Something that was a little more surprising, or is a little more surprising, is that you can have an arbitrary cut in the flow network, and a cut corresponds to any partition.
S, T, such that s belongs to S, the source belongs to S, and the sink belongs to T, and that's essentially a cut.
And so obviously there could be exponentially many cuts in a given flow network.
But the amazing thing is that you can show a lemma that says that, regardless of what the actual flow values are in any of these edges-- across any cut you're going to see this flow.
And this is true for any cut S, T.
The corollary to this lemma is that the flow is going to be less than or equal to the capacity of any cut.
And that simply comes from the edge constraints.
You know that the flow value corresponding to any particular edge has to be less than the capacity of that edge.","max flow |f| = some c(S,T) cut..... not any I think. some (S,T) cut may have a larger capacity which is okay since all we need id f < any (S,T) cut. here , he just showed that there is always some (S,T) cut out there such that |f| = capacity of that (S,T) cut. did I make sense?"
"We're going to inherit one more time and create a class of golden retrievers.
Once again, I've got my class definition and my indication that I'm going to inherit from Retriever.
I don't have any initialization or attribute assignments.
I only have a definition for greeting.
So what happens here? Well, the first thing we always do is look for an initialization method.
Golden doesn't have one, so it's going to check the Retriever class.
Retriever doesn't have one, so it's going to check the Dog class.
The initialization method is here.
So when it runs the initialization method, it's going to run this code.
The first thing that's going to happen is any code, or any attribute assignments, or method definitions here are going to be considered the canon, or the first thing that any Golden is going to reference.","So when she goes over that any child class will be applied, will the main parent class _init_ be applied if there was __init__in the child class? Or will that be just completely negated?"
"Our graph is connected.
And in particular, there's going to be a nice property of connected graphs, which is that the number of edges dwarfs the number of vertices here.
So really, if we have v plus E, in some sense, this is going to look like a constant factor times E plus another E here.
So this whole thing is going to be v times E time, yeah? So let's make that argument a tiny bit more formal here.
So in particular, we know that G is connected.
And every vertex-- so in particular, what can happen here is-- OK, unless my graph consists of one vertex, which is a case you could dispose of pretty quickly, what I can't have is a graph that looks like this, like one vertex and then an edge floating around there.","what if the graph is a 3D graph?
In problem 1, the condition for |V| and |E| in a connected graph seems to be incorrect: |V| <= |E|/2. Consider a graph with 3 vertices and 2 edges; it is connected but |V| <= |E|/2 does not hold. I think the condition should have been |V| <= |E|+1 (by induction; one edge connects two vertices and each next edge adds at most one vertex to the connected component). But as the next statement, |V|=O(|E|) still holds, it does not invalidate the rest of the proof."
"And it's a very useful infrastructure for proving PSPACE hardness results, for puzzles, originally motivated by sliding blocks, but we will get there in a moment.
You've seen it very briefly in Lecture 1.
I'm going to go through it again more slowly and clearly and define everything.
And then we'll see lots of hardness proofs based on that.
So we start with the notion of a machine.
We won't use this term too much.
But the idea is we start with an undirected graph.
Think of this as a model of computation.
So your computer is an undirected graph of red edges and blue edges.
Red edges have weight 1.
Blue edges have weight 2.
And then a configuration of that machine-- or a constraint graph in total-- is an orientation of that graph.","at 628, i cannt understand the point of picture meaning,blue is 2,red is 1. the point between blue and red means =+, but the point between blue and blue means =, what about point between red and red means += or =condition of points is different?"
"You have to divide them into groups and do, well, several recursive course.
And also, let me digress a little bit, there's a very interesting point regarding this worst case O of n algorithm.
Has anyone wondered why we use groups of five? Why not groups of three? Algorithms should work in the same way, right? If we take out this first row and this last row, we can still find the median, which is just a second element in each group and find the median of the median.
We still have a subproblem that looks like this.
Exercise.
And it turns out if we use groups of three, when we solve the recursion, it doesn't solve to O of n.
It solves to something else.
OK.
Now, end of digression.
Let's get back to the randomized version.","For randomized quicksort what happened to ET(n-j), why is it only ET(j-1)?"
"We don't know anything about it's length, yet.
Then, we also have some unknown, say, right here.
And we have a vector that points to it by excel.
So now, what we're really interested in is whether or not that unknown is on the right side of the street or on the left side of the street.
So what we'd what to do is want to project that vector, u, down on to one that's perpendicular to the street.
Because then, we'll have the distance in this direction or a number that's proportional to this in this direction.
And the further out we go, the closer we'll get to being on the right side of the street, where the right side of the street is not the correct side but actually the right side of the street.
So what we can do is we can say, let's take w and dot it with u and measure whether or not that number is equal to or greater than some constant, c.
So remember that the dot product has taken the projection onto w.
And the bigger that projection is, the further out along this line the projection will lie.","I have a question. At the start of the lecture, the professor says that we can imagine W to be a normal vector of any length and that the distance of a point from the hyperplane is the projection of any point onto W. But how can the dot product of WX be equal to the length of the projection of X onto W if W is any length? This can only work if W is a unit vector unless I'm mistaken. WX = ||W||*||X||*cos(theta) and the projection length of X onto W is ||X||*cos(theta). So WX can only equal ||X||*cos(theta) if ||W|| = 1. Otherwise ||X||*cos(theta) = WX/||W||.
dot product dont give projection we need to multipy it either by |x| or |y| to give us the projection im i right ?
Why is the projection originally defined as w dot x = c and not the unit vector of w dot x?
Why it is not the magnitude ?
I dont get how he is findeing the width of the street! x1- x2 =1-b-(1+b)= -2b!? correct me if i am wrong!
Okey, can someone please tell me what is the unknown vector U? And what should be the value of b?
why did we choose vector w perpendicular to the boundary ?. why wont w vector project on vector u when taken the dot product?
Isn't the projection of vector u on the vector w = (dot_product(u,w)/||w||) ? Why is he considering only the dot product? Can someone please explain it to me? If we consider value of ||w||, the width comes to be exactly 2. How can we find extremes of it? Am I interpretating anything in a wrong way?
In this Equation: Margin Width= ((x+) - (x-)) dot with Unit vector of W Why ((x+) - (x-)) dot with Unit vector of W = Margin Width Can anyone help me?
Why is he claiming and using the dot product to be the length of a vector projection onto another vector? Shouldn't he also divide by the other vector length??
I believe the length of the projection is w.u/|w| isn't it?
wx + b >= 1 or <=1 , this means we are assuming that perpendicular distance between the two support vectors will always be 2 , how wx+b>=0 or <=0 makes sense but >=1 and <=1 how this is gonna hold ?
Bhullar, I can not get your logic. How come you use the value of a non-expanded variable on the same variables expanded values where if you expand the non-expanded variable, the outcome will completely change? Also, how come you say that, he did not expanded the the w(vector) in first equation and expanded it in second equation where he did say that at 16.39 ""w(vector) is a normal and what i can do is take the w and divide by the magnitude of w and that will make it unit normal "" ? So it was not unit normal, he made it unit normal by dividing with magnitude to get the width of the street. can you prove me wrong? I am just trying to understand. May be I am wrong. Also I am agree with Sebouth's last question.
Nice Tutorial Question : 1) At the time of calculating the width of the street, why the unit vector component is multiplied ?
jupiter 6 Isn't the projection of vector u on the vector w = (dot_product(u,w)/||w||) ? Why is he considering only the dot product? Can you please explain it to me? Also, If we consider value of ||w||, the width comes to be exactly 2. How can we find extremes of it? Am I interpretating anything in a wrong way?
***** beause (x+) - (x-) is not perpendicular to the median line. For calculate the width of the street, you need a perpendicular vector. When you multiple it with the unit vector, it becames perpendicular.
+Murat Bykaksu How do we know multiplying the perpendicular vector by the difference vector gets us the perpendicular width between the boundary samples? The way I'm thinking of the width between the margin is as the opposite side of a triangle. The projection of x- onto x+ is the base, and the difference vector is the hypotenuse. Is there an error in my way of thinking?"
"So just out of curiosity, I'll take a poll.
Who thinks that the drunk doesn't much matter how many steps he takes, he'll be more or less the same distance away? And who thinks the more steps he takes, the further away he's likely to be? It seems to be a season where when you take polls, they come out almost tied.
Let's look at a small example.
Suppose he takes one step only.
Well, if he takes one step, and we'll assume for simplicity that he's not so drunk that he moves at random.
He either moves north or south, east or west.
These are all the places he can get to in one step.","If someone could help me, in the textbook there's another exemple of drunk: the EW Drunk, moving only in the horizontal axe (-1, 0) and (1, 0). However this drunk is also getting farther away from the origin. But why? If after n number of steps he has equal chance to step etiher W or E, wasn't he supposed to be back to the origin according to the law of big numbers? Isn't it the same as to flip n coins and count number of head and tails?"
"But notice, every other thing I do here, I've actually computed those values.
I'm wasting measures, or wasting time, it's not so bad with fib of 5, but if this is fib of 20, almost everything on the right hand side of this tree I've already computed once.
That means fibs very inefficient.
I can improve it by using a dictionary, very handy tool.
I'm going to call fib not only with a value of n, but a dictionary which initially I'm going to initialized to the base cases.
And notice what I do, I'm going to say if I've already computed this, just return the value in the dictionary.
If I haven't, go ahead and do the computation, store it in the dictionary at that point, and return the answer.","How do we initialise variables in recursion in Python so that it continues values in every local scope without it's initial value in global scope?
Ken MacDonald Yeah I know but as n goes up, there will be hundreds of steps. So do they recurse on the other because the same rules still applied?"
"But it means you get twice the number of digits of precision as you run.
It's wildly fast, OK, when it runs in terms of steps that it takes.
OK.
So this distance we want to call delta.
So theta 1 is going to be equal to theta 0 minus delta.
What is delta? How far do we step? Then we have an algorithm here, right? And then we'll repeat it, right, just to be clear.
We would go up here, we would compute another derivative, and so on.
And we would zoom in on this.
This would be our theta 1 or theta 2.
OK? So we have to solve this key step.
So how big is this? Well, if we look at it, f of theta 0 equals f prime of theta 0 times delta.",Quick question! In newtons method we find a point where lprime theta =o. But what if the equation is not quadratic which makes it not covex anymore?
"Another issue that's going to come up is programming languages use different data types.
And so, in Python, we have integer numbers like for example 3, and these are sometimes referred to just shorthand as ""ints."" So you hear somebody talking about an ""int"" and they're a programmer, they're talking about a integer value.
We have floating point numbers like say 3.14, these are numbers with decimal point in them.
These are sometimes referred to just as ""floats."" And then we have strings, which are used to represent text.
And the term ""string"" comes from the concept of a string of individual characters.
When we want to write a ""string,"" we're going to use quotes.",Why cant 12-6-6 be 12+(-6-6) or (12-6)-6? Is separating the - from the -6 allowed?
"Oh, another question.
Great.
AUDIENCE: So obviously after putting rule 4 there, shouldn't we first check the assertions-- check on the assertions instead of our conditions [INAUDIBLE] MARK SEIFTER: So the question is, once we put rule four in there, should we check to see if those antecedents of rule four are already in the assertions before checking other rules? The answer? You're not only correct, sir, you're exactly one step ahead.
So I'm going to assume I called on you for the very next thing.
That's exactly what we do once we draw it onto the tree.
You're exactly on the way.
Further question? Perfect.
All right, great.
So again, we're putting up Millicent studies a lot.
Millicent is a protagonist.
We're not putting up x.
Some people did that.
And of course it's an n node as we heard.","Suppose two assertions matches rule 0 i.e., assertion 0 and assertion 3. At first we fire rule 0 and assertion 0. What do we do next? Do we fire rule 0 with assertion 3 or rule 'X(any other that matches except 0)' with assertion 1?
For the part 3 of the question 1. can we say that someone can live in 2 places at the same time as an assertion ?
Would adding this assertion answer quesion 1 ""Milicent is S's friend""?
In rule P2, it states: IF ?x lives in SD THEN ?x is a villain ?x is ambitious. Why does the word AND not appear there? Shouldn't it say this: IF ?x lives in SD THEN AND(?x is a villain, ?x is ambitious) Or is adjacency meaning multiple things written next to each other automatically assumed to be conjunction? If that is the case, why is AND ever used in these rules?"
"So [? Silla ?] doesn't actually have to remember that anymore because going forward, she will never have to integrate anything personally in her life, she can just simulate the program.
So these go from polynomial form, back into trigonometric form.
So you have three of these heuristic transformations.
We've got four safe transformations.
Let's see if we can make any progress on our integration problem.
OK so keeping track of what we've been using, this is safe transformation number one, this is safe transformation number two.
What do we do next? We decided there were no more safe transformations that apply.
But now we can look at our heuristic transformations and behold, we see what? AUDIENCE: C SPEAKER 1: What? AUDIENCE: Applying transformation C.
SPEAKER 1: Transformation C suggests that we do x equals the sine y.
And now we get the integral of sine to the fourth y over cosine to the fourth y dy, right.","There is one question, that when they apply x=sin(y), there is a subtle constraint of the range of x applied, which means abs(x) cannot be bigger than 1. But the original formula does not have the constraint of x value range. Are we losing something here?!
Why did he chose those specific transformations as safe transformations?
I got lost when he apply C and got to sin^4y/cos^4y. can someone explain it in detail for me? thanks...
Out of curiosity: What level of mathematics should I have before watching these videos. I have never taken a calculus course, so I don't know why those transformations work. Do I need calculus, or can I get by without it? Or can I learn a certain calculus subjects without having to go into calculus in depth?
I don't understand the explanation why the denominator became Cos^4 y as opposed to cos^5 y. How does the derivative of sin being equal to cos explain this? If anyone can assist in my understanding I would appreciate it very much.
+James N How does the numerator become sin^4 y cos y, with direct substitution it shld be sin^4 y ?! Help !!"
"The observation that is interesting that then leads us to generalize, uh, this notion to the not- of clustering coefficient to the notion of graphlets, the observation is that clustering coefficient basically counts the number of triangles in the ego-network of the node.
So let me explain what I mean by that.
First, ego-network of a given node is simply a network that is induced by the no- node itself and its neighbors.
So it's basically degree 1 neighborhood network around a given node.
And then what I mean by counts triangles? I mean that now, if I have this ego-network of a node, I can count how many triples of nodes are connected.
And in this particular, uh, use case where, um, um, uh, the clustering coefficient of this node is 0.5, it means that I find three triangles, um, in the network, uh, neighborhood, in the, um, ego-network of my node of interest.
So this means that clustering coefficient is really counting these triangles, which in social networks are very important because in social networks, a friend of my friend is also a friend with me.","I have a doubt about GDV at Could you provide more clarification about when you are explaning about counting connected nodes, Why is every single node to node connection considered as a graphlet instance associated with (a) and in particular this question arises for me because when it comes to counting graphlet instance of (d) you are counting the instances even when there are links connecting to other nodes are present. Why does this difference occur ?"
"What would the decision boundaries look like if these were four different kinds of things, and we were using this kind of mechanism? And maybe there's a lot of samples all clustered around places like that.
What would the decision boundaries look like? Would they be the same as this? god, I hope not.
Why? Because what we're going to do is we're going to use a threshold on each axis.
So therefore, the decision boundaries are going to be parallel to one axis or the other.
So we might decide, for example-- Oh, shoot.
I think I'll draw it again, because it'll get confused if I draw it over the other one.
So it looks like this.
And that's how nearest neighbors does it.
But a identification tree approach will pick a threshold along one axis or the other.
Let's say it's this axis.
It's only got one choice there.
So it's going to put a line there.
And now, what's the next thing it does? Well, it still has these two different kinds of things to separate.",Why does he make the difference between Identification tree and decision tree?
"In other words, for every a and b, that gives me a line, and I could, therefore, compute this function, given the observed values and the predicted values, and it would give me a value, which is the height of the surface in that space.
If you're with me with the visualization, why is that nice? Because linear regression gives me a very easy way to find the lowest point on that surface, which is exactly the solution I want, because that's the best fitting line.
And it's called linear regression not because we're solving for a line, but because of how you do that solution.
If you think of this as being-- take a marble on this two-dimensional surface, you want to place the marble on it, you want to let it run down to the lowest point in the surface.
And oh, yeah, I promised you why do we use sum squares, because if we used the sum of the squares, that surface always has only one minimum.
So it's not a really funky, convoluted surface.
It has exactly one minimum.
It's called linear regression because the way to find it is to start at some point and walk downhill.
I linearly regress or walk downhill along the gradient some distance, measure the new gradient, and do that until I get down to the lowest point in the surface.","Thank you . I have looking into this more to better understand. Rhetorically, why are we being taught the range is 0-1? Is it just more practical? Admittedly, I am new to the field and only have a grasp of the basic concepts, but I can find many resources that I would find credible that state R^2 it is definitively 0-1. ""It's a proportion."" ""It's a squared term."", etc. Is this contentious? Are negative r^2 more theoretical and so rare they aren't worth discussing? Anyways, thank you for elucidating the point and setting me straight. I will try to understand this better."
"Actual cost is constant.
You have to also deposit one coin, which costs constant time so the amortized cost of the insert is still constant.
So that's good.
Still we don't know how to deal with deletions, but let me give you a kind of reverse perspective on the accounting method.
It's, again, equivalent in a certain sense, but in another sense may be more intuitive some of the time for some people.
It's actually not in the textbook, but it's the one I use the most so I figure it's worth teaching.
It's called the charging method.
It's also a little bit more time travel-y, if you will, so if you like time travel, this method is for you, or maybe a more pessimistic view is blaming the past for your mistakes.","I got the idea of amortization in general, but these coins are totally weird. Why the heck do we charge back in time only once per insertion?
Troy Whorten yeah I'm having a hard time understanding the concept of amortization. I guess I need to go through that chapter of the book couple more times "
"As you can see, this outline is fairly involved, talking about cuts in a network, residual networks.
And we'll, more or less, end the lecture with the statement, though not the proof-- we'll save that for next time-- of the mas-flow min-cut theorem, which is really an iconic theorem in the literature, and suddenly, the crucial theorem for flow networks.
And we'll take the max-flow min-cut theorem and use that to get to the first ever max-flow algorithm, which was due to Ford and Fulkerson.
And that should be, pretty much, at the end of today's lecture.
And next time, we'll talk about the proof of max-flow min-cut, talk about some of the issues with Ford-Fulkerson, and then use max flow as a hammer to solve interesting problems in bipartite matching, baseball playoff elimination, and things like that.
So just like shortest paths, can be used not just to compute the shortest distance from point A to point B, you can imagine that other problems, for example, scheduling problems, time problems, could be solved using Dijkstra-- and you've probably seen examples of that-- max flow is another algorithmic hammer that's being used to solve a wide variety of problems.
We won't really touch on that aspect today, but we'll spend a bunch of time on Thursday talking about that.","is it related to gomory hu's algorithm?
That's the proof I come to as well when I try to come up with the proof on my own. Any idea why he uses a different method?"
"So let me just kinda refresh.
Okay.
So I'm computing this at a distance between two strings and we're gonna define a recurrence that works on sub problems, where the sub problem is the first m letters of s and the first n letters of t.
And the reason I'm using integers instead of, um, strings is to avoid like string copying, um, implementation detail, but it doesn't really matter.
Um, so base cases.
So you wanna reduce your problem to a case where it's- it's trivial to solve.
Um, and then we have the last letter matches.
And then we have a letter doesn't match and you have to pay some sort of cost.
I don't know which action to take.
So I'm gonna take them, you know, minimum of all of them.
And then I call it by just calling, you know, recurse.
Okay.
So this is great, right? So now I have a working thing.
[NOISE] Um, let's try another test case.
So I'm gonna make this.
Um, so if I do times 10, this, uh, basically, uh, replicates this string 10 times.
So it's a- it's a long string-longer string.
[NOISE] Okay.
So now I'm gonna run it.
[OVERLAPPING] Maybe I shouldn't wait for this.
Is there a base case? Um, there is a base case, I- I think that it expanded- it's- what- what's wrong with this code? Very slow.
Um, yes, it's very slow.
Why is it slow? [BACKGROUND] Yeah, right? So- so I'm recursing.
[NOISE] Every point recurses three times.
So you kind of get this exponential, you know, blob.
Um, so there's kind of a- how do you solve this problem? [BACKGROUND] Yeah.
You can memo I think I heard the word memoize, which is another way to kind of think about.
Memorize plus, um, I guess, recurrences is dynamic programming, I guess.
Um, so I'm gonna show you kind of this, um, way to do it which is pretty, uh, uninvasive.
Um, and generally I recommend people.
Well, get the slow version working [NOISE] and then try to make it faster.
Don't try to be, you know, too slick at once.","We use cache after we do all computing ( after ""result = min(subCost, delCost, insCost)"" ) so how does it benefit to us?
You check the cache FIRST before running all the computation ""if (m,n) in cache => return cache(m,n)"" lines at the top before everything else. So basically if the result is already in the cache then there is no need to run 3 computations again, just return the result"
"And then when it's finally decrypted over here, because that's a malleable crypto scheme, if the same attacker is controlling this node as well, or if the attacker is observing it here, the attacker will see Alice, Alice, Alice, Alice, Alice XORed with a reasonable plain text and be able to use that to identify, ah, this is the stream that came from Alice.
So let's do a little more about how the protocol works.
Because it would be a shame to have everybody read the paper and then not talk about the stuff that the paper is focused on.
Again, I apologize for my blackboard technique.
Most of the time, I'm sitting at home on a desktop.
This is alien tech.
So here's a relay.
Here's Alice.
Here's another relay.
Here's Bob.
Now Alice wants to talk to Bob.
So first thing Alice has to do is build a circuit through these relays to Bob.
Let's say she's picked these two, R1 and R2.
So Alice first makes a TLS link to R1.
R1, let's say, already has a TLS link to R2.
First thing Alice does is she does a one-way authenticated one-way anonymous key negotiation.
The old one in Tor is called TAP.
The new one is called NTor.
They both have proofs.
They both even have correct proofs, although the original proof in the paper had a flaw in it.
But when that's done, she sends a create cell.
And she picks a circuit ID.
Let's say she picks 3, and says, create 3.",Did anyone understand the part where he talks about IP stacks and TCP stream contents?
"So what does it mean? Is that rather than, um, uh, evaluating, uh, the gradient over all the nodes- all the negative nodes, um, and- or all the neighbors in the neighborhood of a given node, and then make a- make a step, we are going to do this only for a, uh, for a given, uh, for a given node in the neighborhood.
So basically, the idea is that, you know, we'll sample node i, and then for all, uh, js that in the- in the, uh, in the neighborhood, we are going to compute the gradient, uh, and then make a step and keep iterating this.
Uh, we- of course, we'll get a stochastic estimates or kind of a random estimate of the gradient.
But we'll be able to up- to make updates much, much faster, which in practice tends, uh, to be much, uh, better.
So let me summarize.
We are going to run th- a short fixed-length random walks starting from each node on the graph.
Uh, for each node u, we are going to collect, um, its neighborhood and as a multiset of nodes visited on the random walks starting from node u.
And then we are going to optimize this embeddings used- using stochastic gradient descent, which means, uh, we are going to, uh, uh, find the coordinate Z that maximize, uh, this particular expression.","I didn't understand the part ""drawback of node2vec"", why we need to learn each node embedding individually ?
I had a quick question. Do you generate the negative samples every epoch, and then use them for each node, or do you generate new negative samples for every single node you sample in every generation?
What do you mean by the negative examples in negative sampling?
 I don't think that's entirely correct, as neighbouring nodes are positive samples, similarly to word2vec where context words are ""positive samples"". Negative sampling samples from the negative examples, in this case nodes that are NOT neighbors to the central node.
To my understanding: The loss has two terms: 1, (z_u, z_v) corresponding to the positive example (neighbor and central node's similarity) 2, (z_u, z_ni where z_ni are elements of the negative samples i = 1,...,k) corresponding to the selected k negative samples. The loss can be optimized by either increasing term 1 or decreasing term 2. Now, if k is increased, the sum in term2 will have more elements and that part will have more weight, resulting in stronger emphasis on the negative samples, that is, the mapping will be adjusted so that the similarity to the negative examples are smaller. This way there will be a higher bias for negative events, as the loss will punish greater values for similar non-neigbor nodes, that it will reward similar neighbor nodes."
"If I've got too hard 36 minutes, and I want to know how many hours that is? I can take the 236 use in integer division by 60, which will give me three.
And then I can use the modulus operator, which gives me 56.
And so I can say 236 minutes is 3 hours and 56 minutes.
OK, what about this power, the double asterisk Well, we're going to return to our discussion on binary here.
I'm sure some of you are thrilled great.
So what it's 2 to the eighth power or how many different combinations of bits can I have in 8 bits? OK, so we know that we have 2 do the 8 different combinations of zeros and ones in 8 bits.",Why cant 12-6-6 be 12+(-6-6) or (12-6)-6? Is separating the - from the -6 allowed?
"Well, that's kind of a bummer.
Because how can I get the standard deviation of the population without looking at the whole population? And if we're going to look at the whole population, then what's the point of sampling in the first place? So we have a catch, that we've got something that's a really good approximation, but it uses a value we don't know.
So what should we do about that? Well, what would be, really, the only obvious thing to try? What's our best guess at the standard deviation of the population if we have only one sample to look at? What would you use? Somebody? I know I forgot to bring the candy today, so no one wants to answer any questions.","Quick question, when do we NEED to have more than one sample? And when can we just use one?
I am a bit surprised he didn't comment on how the standard deviation of the sample has a bias as an estimator for the standard deviation of the population, and how you should divide by (n-1) instead of n (n being the sample size) when doing this estimation..."
"The first time you get an email from your teammates that has the word NIPS in it, your spam classifier will estimate this probability as 0 over 0 plus 0, okay? Now, apart from the divide by 0 error, uh, it turns out that, um, statistically, it's just a bad idea, right? To estimate the probability of something as 0 just because you have not seen it once yet, right? Um, so [NOISE] what I want to do is describe to you Laplace smoothing, which is a technique that helps, um, address this problem.
Okay? And, um, let's- let's- In order to motivate Laplace smoothing, let me, um, use a- a- a- Yeah, Let me use a different example for now.
Right? Um.
Let's see.
All right.
So, you know, several years ago, this is- this is all the data, but several years ago- so- so let me put aside Naive Bayes, I want to talk about Laplace smoothing.
We will come back to apply Laplace smoothing in Naive Bayes.
So several years ago, I was tracking the progress of the Stanford football team, um, just a few years ago now.
But that year on 9/12, um, our football team played to Wake Forest and, you know, actually these are all the, uh, all the stay games we played that year, right? And, um, uh, we did not win that game.
Then on 10/10, we played Oregon State and we did not win that game.
Arizona, we did not win that game.
We played Caltech, we did not win that game.
[LAUGHTER].
And the question is, these are all the away games- almost all the out of state games we played that year.
And so you're, you know, Stanford football team's biggest fan.
You followed them to every single out of state game and watched all these games.
The question is, after this unfortunate streak, when you go on- there's actually a game on New Year's Eve, you follow them to their over home game, what's your estimate of the chances of their winning or losing? Right? Now, if you use maximum likelihood, so let's say this is the variable x, you would estimate the probability of their winning.","A doubt : When talking about NIPS conference making zero probability in Naive Bayes ; in the first place, probability of word NIPS shouldn't come up in the calculation P(x /y=0) , as the the binary column vector of 10000 elements won't have this word in it as its not in the top 10000 words cuz it started appearing very recently."
"It's an offline analysis, it's not facing customers, so it's OK for it to be slow.
That might actually really help you to figure out oh, this data's getting into the keyboard buffer, it's getting into the x server, it's getting to wherever.
So even if it's slow, that can still be very, very useful.
So I just wanted to mention that briefly.
One interesting thing you might think about is the fact that as I mentioned, TaintDroid is nice because it constrains the universe of taint sources and taint sinks.
But as the developer, maybe you want to actually explicitly assert some more fine grain control over the labels that your program interacts with.
So now as a programmer, you want to be able to say something like this.
So you query some int, and let's say we call it x, then you associate some label with it.
Maybe the name of this label is that Alice is the owner of this data, but Alice permits Bob, or something labeled with Bob, to be able to see that.
TaintDroid doesn't let you do this, because it essentially controls that universe of labels.
But maybe as a programmer you want to be able to do a thing like this.
You can imagine that your program has various input channels and output channels, and all of these input and output channels, they all have labels, too.",I'm not sure if the purpose of TaintDroid is to protect users from malicious apps or help the developer to protect his users.
"And the way we're going to do that is say, well for this function f it's a function, it's got a derivative for a gradient.
So what we want to do is work out that local gradient dh / dz, and then that gives us everything that we need to work out ds/ dz because that's precisely we're going to use the chain rule.
We're going to say that ds / dz equals the product of ds / dh times dh / dz where this is, again, using Jacobians.
OK, so the general principle that we're going to use is the downstream gradient equals the upstream gradient times the local gradient.
OK, sometimes it gets a little bit more complicated.
So we might have multiple inputs to a function.",Question: How is f(z) Jacobian? My understanding: For a single neuron z is going to be a scalar. and f(z) output is also going to be scalar for a neuron. Can a Neuron ever output anything other than a scalar? Perhaps the jacobian holds for the overall network
"But that makes absolute sense.
If you don't know what the actual animal is going to be and there are a whole bunch of possibilities, you better just say no for everybody.
It's like when a biologist says, we don't know.
It's the most probable answer.
Well, but eventually, after about 160,000 iterations, it seems to have got it.
Let's run the test samples through.
Now it's doing great.
Let's do it again just to see if this is a fluke.
And all red on the right side, and finally, you start seeing some changes go in the final layers there.
And if you look at the error rate down at the bottom, you'll see that it kind of falls off a cliff.
So nothing happens for a real long time, and then it falls off a cliff.
Now, what would happen if this neural net were not quite so wide? Good question.
But before we get to that question, what I'm going to do is I'm going to do a funny kind of variation on the theme of dropout.
What I'm going to do is I'm going to kill off one neuron in each column, and then see if I can retrain the network to do the right thing.
So I'm going to reassign those to some other purpose.
So now there's one fewer neuron in the network.","How did the professor trained the Neural Net...any idea??
When it guesses the wrong thing (school bus on black and yellow stripes) isn't the ""real problem"" there that it doesn't have enough data or good enough data?
I wonder why it still works when shutting down some of the neurons and left only 2 of them."
"So- so we're actually by- by convention, we're actually going to assume that W I can be written right? So in- in this example this is plus minus 1 right? So um, this makes some of the math a little bit downstream, come out easier but it is- but it's still saying that W is- can be represented as a linear combination of the training examples.
Okay? So um [NOISE] let me just describe less formally why this is a reasonable assumption, but it's actually not an assumption.
The representer theorem proves that you know, this is just true at the optimal value of W.
But let me convey a couple ways why um, this is a reasonable thing to do, or assume I guess.","Is it really hard to prove that w is a linear combination of the training samples? The training samples represent some set of vectors in a vector space. They might not span all of the space, and in fact they can't span more than N where N is the number of training samples (and they might span less, if there's any linear dependence among them). But they span some subspace of dimension M. The vector w defines a hyperplane that divides that space into classes. Well, the training samples span that subspace, and therefore any vector in that subspace is a linear combination of them. That leaves the non-spanned dimensions to consider, but your training sample conveys no information whatsoever about those, so there's nothing to do about them. I hope linear algebra was a prerequisite for this class."
"Now, this JavaScript code is going to run as part of this page.
And now, because it's running in this HTTPS foo.com domain, it has access to your secure cookies for foo.com and any other stuff you have in that page, et cetera.
So it seems like a really bad thing.
So you should be careful not to.
Or a web developer certainly should be careful not to make this kind of a mistake.
So one solution is to ensure that all content embedded in a secure page is also secure.
So this seems like a good guideline for many web developers to follow.
So maybe you should just do https colon jquery.com.
Or it turns out that URLs support these origin relative URLs, which means you could omit the HTTPS part and just say, [INAUDIBLE] script source equals //jquery.com/ something.",what is an attacker change jquery and create new hash?
"It's not b.
Look at this one-- it is b.
I return yes.
Does that make sense? So this is an idea called chaining.
I can put anything I want there.
Commonly, we talk about putting a linked list there, but you can put a dynamic array there.
You can put a sorted array there to make it easier to check whether the key is there.
You can put anything you want there.
The point of this lecture is going to try to show that there's a choice of hash function I can make that make sure that these chains are small so that it really doesn't matter how I saw them there, because I can just-- if there's a constant number of things stored there, I can just look at all of them and do whatever I want, and still get constant time.
Yeah? AUDIENCE: So does that means that, when you have [INAUDIBLE] let's just say, for some reason, the number of things [INAUDIBLE] is that most of them get multiple [INAUDIBLE]..","he asked a question saying why we need to think of the minimum height of a binary tree with at least n+1 leaves. I wonder why that minimum height is significant for the hashTable that was discussed later. Please explain. A student asked a doubt at that time and the Professor forgot to reiterate his statement.
why is memory access constant time? are we assuming the bus is this wide to support the full memory address? Seems like a big assumption since we do not know how large the data set is
You could definitely add pointers to elements in a linked list. The problem though is how to use them. In order to know whether the pointer on element A still points to be middle of the list after an operation that might change the list's state, one would need to traverse the entire list, figure out where the ""middle"" was, and then update the pointer. The other problem is that just having a pointer to the middle element of a list doesn't communicate anything to the caller about what value they'll find at the other end of the pointer unless they traverse the pointer and evaluate the element. Jumping to the middle of the list might be helpful, but it might also not be helpful. It depends on the state of the list at that time. It's certainly possible to TRAVEL through a list by skipping elements, but you still need to examine each element to find out if you've reached your destination, or passed it."
"And then I have not the initial puzzle that was given to me, but something that I've kind of hacked in the sense that I've stuck a two in there.
And that may not correspond to the solution, because I just sort of put the two down there.
But now given the two, I'm going to try and do some implications.
And I'm going to try and see whether there's things that are valid or not.
The important thing is that, because I put a two down in an arbitrary way without using implications, the two could have been incorrect.
I mean that's exactly why we have all of these backtracks, correct? Because I've put down incorrect guesses and then I've had to backtrack.
So once I put a two down and then I fill in a bunch of things with implications.
You know, I may even put an eight up there.
I may put a six out here, et cetera, et cetera.
And I go deep in and then I realize, ooh, you know that two was a mistake.
The two really shouldn't have been in there.
Now I have to clean up everything.
I have to clean up all of the guesses that came after two and all of the implications that came after two.","I wish prof don't mix up imply and infer.
Interesting. Are you saying that if a puzzle requires guessing then it's essentially ""indeterminate"" and would have more than one solution? I mean, if you had a sudoku board with only one number populated, you'd have a large number of solutions that would be possible, and then if you had a board with another number added, you'd have a smaller set, and so on until you would have one that would still have multiple solutions until you constrained it with a final number."
"Hopefully, this makes sense.
So you find the biggest element between 0 and index i.
That's what we're going to call j here.
I swap that with the one in index i.
That's step 2.
And then step 3 is I still have to sort everything to the left of index i and that's that recursive call.
So if I want to justify the runtime of this particular technique, well, now let's call that t for time.
Well, what do I do? Well, for one, I call selection sort with index i minus 1.
So that incurs time that looks like this.
But I also call that prefix max function.
And how much time does that take? That takes order n time.
So at the end of the day, I have some relationship that looks like this.
Does that makes sense? So by the way, notice that this order n swallowed up the order 1 computations that I had to do to swap and so on.
So remember, there's this nice relationship, which you probably learned in your combinatorics class, which is that 1 plus 2 plus dot, dot, dot plus n.
OK.
I can never remember exactly the formula.
But I'm pretty sure that it looks like n squared.
So based on that and taking a look at this recursive thing, which is essentially doing exactly that-- n plus n minus 1 plus n minus 2, and so on-- I might hypothesize that this thing is really order n squared.
So if I'm going to do that, then again if I want to use the same technique for proof, I have to plug this relationship in, and then double check that is consistent.",Anyone else get confused by the mathematical part?
"So we- we, um, we take the output of one- one, um, simple model and feed that as input to, you know, another simple model and so on.
And the crucial, um, the crucial thing while composing them and composing them is having non-linearities, right? And non-linearities are crucial, uh, because if there were no non-linearities then the entire network can- could be represented as just a single linear, uh, single linear layer with one, uh, one matrix.
And also, um, neural networks and deep learning, they are- they are, um, non-convex, and it's important that we initialize the parameters randomly.
Whereas in the previous models that we saw, the simpler models, they were mostly convex and initialization did not matter.",I learnt a lot from your class Mr.Anand avati. I have a small doubt in NLP. If you know please help me. In transformers how model can distinguish from positional embeddings and word embeddings.
"And that's why they look like different formulas but they evaluate to the same thing so there's only one root.
The fix to that is to require the quantity by which the two root formulas, r1 and r2 differ to be non-zero.
And that's the quantity that we were taking the square root of, the discriminant it's called.
b squared minus 4ac needs to be 0 and then r1 and r2 will differ and we will get the two roots.
Now, there's still a complication.
It sounds like we've now verified that indeed our proof by calculation is correct now that we've put in these qualifiers, that a is positive and d is non-zero.","How these kind of things can be related to programming. I don't understand
""What about 0x^2 + 1x + 1""? The instructor claims that this is a 45 degree line, the y=x line. Isn't this the line x = -1 ? The vertical line that passes through (-1, 0) ? Why would this be the 45 degree line?
how can square root of a real number be negative sir? Is it even a sane thing?
There's nothing called square root of a negative number, but there exists a number which its square equals to -1
There's an assumption that the quadratic function is equal to y. Such as y = 0x^2 + 1x + 1 = x + 1, so y = x + 1, which is indeed the y = x line, shifted upwards by 1.
I lost you at ""Abstractly thinking you can not look at 1 and -1 as scalars, ..."""
"t is the optimum cost.
That's what I defined it as.
And this is a bound on k over t.
All right? Cool.
Any questions on this? OK.
So this approximation ratio gets worse for larger problems, just like this other approximation's algorithm that we didn't actually prove from the very first problem.
That also got worse.
We had a log k factor for that.
And as your problem size increased, obviously the approximation factor increased.
This is a little clearer as to what the increase looks like in relation to the original size of n.
So it's just natural logarithm of n plus 1.
So, so far, we've done approximation algorithms, a couple of different varieties.
We had a constant factor one, and then we had a row of n that actually had a dependence on n.
Now let's move, and we'll do one last example on partition, which it turns out has a trivial constant factor approximation scheme.
And this obvious thing, and we'll get to that in just a second.
But what is nice about partition is you can actually get a PTAS, right? Polynomial time approximation scheme, and FPTAS, fully polynomial time approximation scheme, that essentially give you with higher and higher run times.
They're going to give you solutions that are closer and closer to optimal, right? If you want to do the FPTAS, we'll do the PTAS.",how k/n > lgn becomes k/n <= lgn + 1
"It may sound difficult, but in fact, there's a pretty simple way, which are called AVL trees, that maintain balance in a particular way called height balance.
This is if we take the height of node.left-- actually, I'd prefer to-- node.right, minus height of node.left, so this thing is called skew of the node.
I want this to always be minus 1, 0, or plus 1.
So this is saying that if I have any node, and I look if its left subtree and its right subtree, I measure their heights-- remember, that's downward distance, maximum distance to a leaf-- I measure the height of this tree-- maximum height-- and I measure the maximum height of this subtree, I want these to be within 1 of each other.","His formula skew(node) = height(node.right) - height(node.left) is correct. But your calculation implicitly suggests that you thought height(NIL) = 0, which is wrong. In fact, the height (and depth) in his class (see Lecture 6) is defined as the # edges, not # nodes. So height(leaf) = 0 (1 node subtree has 0 edge), and height(NIL) = -1. The latter case for NIL must be defined this way to ensure that the recursive relationship height(node) = 1 + max{height(node.left), height(node.right)} holds even if node = leaf, i.e., height(leaf) = 1 + max{height{NIL}, height{NIL}} = 1 + max{-1, -1} = 1 + (-1) = 0. Now back to your example, the correct drawing with more details becomes: Z (height(Z): 2, skew(Z) = height(Y) - height(NIL) = 1 - (-1) = 2) \ Y (height(Y) = 1, skew(Y) = height(X) - height(NIL) = 0 - (-1) = 1) \ X (height(X) = 0, skew(X) = height(NIL) - height(NIL) = -1 - (-1) = 0) So Z is unbalanced, and we need to perform right_rotate(Z) to restore the balance.
skewed is which side is having larger height on the tree
I thought of that definition first, but view that when skew(y) = -1 in AVL 3rd case, left and right subtrees levels, which k-1 which k-2 didn't matter! I am not sure about the definition"
"There's a partial of p2, partial of a performance function with respect to z.
Now, all we have to do is figure out what those partials are.
And we have solved this simple neural net.
So it's going to be easy.
Where is my board space? Let's see, partial of p2 with respect to-- what? That's the product.
The partial of z-- the performance function with respect to z.
Oh, now I can see why I wrote it down this way.
Let's see.
It's going to be d minus e.
We can do that one in our head.
What about the partial of p2 with respect to w2.",I don't quite get the last point: the computation with respect to width is w^2 (width squared).Can someone explain?
"Down here we create, at most, one 3 node, which is when the split stops.
When the split stops, that's the only time we actually insert a key into a node and it doesn't split, because otherwise you split.
When you split, you're always making two nodes, and that's good.
At the very end when you stop splitting, you might have made one 3 node.
So in an insert, let's say the number of splits equals k, then the change of potential for that operation is minus k plus 1, because for every split there was a 3 node to charge to-- or for every split there was a 3 node that became two nodes, two 2 nodes.
So the potential went down by one, because you used to have one 3 node, then you had 0.
At the very end, you might create one 3 node.
That's the plus 1.
So the amortized cost is just the sum of these two things, and we get 1.
That's k minus k plus 1 which is 1.
Cool, huh? This is where a potential method becomes powerful, I would say.
You can view this as a kind of charging argument, but it gets very confusing.","Chapter 17 from CLRS? The potential method in particular was pretty fuzzy for me from just the lecture but the description in the book really brought it together.
Troy Whorten yeah I'm having a hard time understanding the concept of amortization. I guess I need to go through that chapter of the book couple more times "
"And there's nothing there.
And it'll crash.
That's the segmentation fault you're getting.
So let's just step up to it and see what happens.
So let's run next.
So we keep stepping through the program.
And we can see where we are.
OK.
We're getting close to the end of the function.
So we can step over two more instructions.
nexti.
And now we can disassemble again.
OK.
We're now just at the return instruction from this function.
And we can actually figure out.
So as you can see, at the end of the function, it runs this leave x86 instruction, which basically restores the stack back to where it was.
So it sort of pushes the stack pointer all the way back to the return address using the same EBP.
That's what it's basically for.
And now, the stack is pointing at the return address that we're going to use.
And in fact, it's all A's.
And if we run one more instruction, the CPU is going to jump to that exact memory address and start executing code there and crash, because it's not a valid address that's in the page table.",Can anyone explain what he is doing with the code? I don't understand what he is doing. Thanks
"So that tells me that T prime is contained in T star prime.
The new T star that I get when I apply the cut and paste argument, I modify T star potentially by removing one edge and putting e in.
And the edge that I remove was not already in T, which means I preserve this part, but I also get that my new edge e is in the minimum spanning tree.
And so that's how you prove by induction that at all times the edges that you've chosen so far are in T star.
Actually, to apply the greedy choice property, I need not only that e is cut-- sorry, that e crosses the cut, I also need that e is the minimum weight edge crossing the cut.
That's a little more argument to prove.
The rough idea is that if you forget about the edges we've already dealt with, e is the globally minimum weight edge.
OK, but what about the edges we've already dealt with? Some of them are in the tree.
The edges that are in these-- that are in T, those obviously don't cross the cut.","in what situation is w(T') < w(T* - e)? Isn't w(T') = w(T* - e)?
onwards, if an edge(red) from 1st part ends in 2nd part, then the graph would not be an MST...but we have already supposed it to be a MST with e being an edge.... Please clarify this doubt....!
Before he wrote that he said that T' is the minimum spanning tree of a graph with G/e nodes. So by definition w(T') <= w(T*-e), which is to say that the weight of a minimum spanning tree is smaller than or equal to the weight of any other tree defined on the same set of nodes (G/e). But yes, they are equal, the inequality just comes from the definition of MST."
"This is similar to the guess and check from before.
It's not similar.
Well this part is similar to the guess and check from before.
So we're going to take the guess to the power of 3 minus the cube, right? So that's how far away are we from the actual answer? And we're going to say, if that's not good enough-- so if we're still greater than or equal to the epsilon-- then keep guessing.
So we're going to be stuck in this loop, where we keep guessing values, until we've reached a guess that's good enough, so until we're less than epsilon.
And way we keep guessing is just with this line, right here, which says, increment my guess by increment, and increment being this really small value.","Why aren't we getting cube root of 27 and 8120601 in form of x.xx as we always increment guess by second decimal place.
Cube root simple guess. Why does that code keeps running?
what about the other challenge (if the cube are negatives)?
Am I stupid? In Approximations code we set guess=0.0 and increment=0.01. How can our answer be different than increment times number of guesses??
Can anybody explain me in Assignment 1 part C of the downpayment problem(The one where we calculate ideal monthly savings rate) Why am I getting an Infinite loop. I followed all instructions. Help would be appreciated. annual_salary = float(input(""Please enter your yearly salary: "")) monthly = annual_salary/12 portion_saved = 0.0 cost = float(input(""Please enter the cost of your dream home: "")) down_payment = 0.25*cost semi = float(input(""Decimal rate of increase of semi annual salary: "")) low = 0.0 high = 1.0 x = (low+high)/2 r = float(input(""Rate of interest yearly for savings:"")) current_savings = 0.0 month = 0 while abs(current_savings - down_payment)>= 100.0 : portion_saved = x*monthly while month <= 36: if month%6 == 0 and month != 0: portion_saved = (1 + semi)*portion_saved current_savings += r*current_savings/12 + portion_saved if current_savings < down_payment: low = x else: high = x x = (low + high)/2 print(x) print(""Your saving rate should be: "", x)
and guess**3<=cube is better choice ?
What I get from the approximation algorithm is that it will only give you an answer if the difference between guess**3 and cube is equal to epsilon, if it manages to jump above epsilon, guess will increase in value until it is <= cube, and thus the message ""failed to obtain a value"" will appear. How can we make it so that if the difference does jump above epsilon that the value will get will be the value of guess at time - 1 of that moment.
For bisection I wanted to use this code to take direct input, but error is coming. #To find the cube root of a number using bisection method cube=float(input(""Put the number... "")) accuracy=float(input(""Give accuracy limit... "") low=0.0 high=cube guess=((high+low)*0.5) iteration=0 while abs(guess**3.0-cube)>accuracy: if (guess**3.0)<=cube: low=guess else: high=guess guess=((high+low)*0.5) iteration += 1 print (guess, ""is close to the cube root of"", cube) print (""Total iteration="", iteration) **************************** syntax error in low=0 why?
can anyone tell me what am i doing wrong!! code: cube = int(input('type the no....')) for guess in range(cube+1): if guess**3 == cube: print(""Cube root of"", cube, ""is"", guess) else: print(""cube root does'nt exist"") output:- type the no....1(#1 was the input) cube root does'nt exist Cube root of 1 is 1
why are we writing range(cube+1) instead of *range(cube)*?
I know she said it is a simple if statement but I can't figure it out. What did you do to make it work? My thought is: if cube < 0: guess = -guess
What is the answer of cube = 27 on the exercise. I rewrote the code on the IDE but the execution never ended, why? Help, please
what is the meaning of epsilon here ???"
"So that's why you end up with a, this as the sum of squares of the, those error terms.
Okay.
And, um, if some of the steps don't quite make sense, really don't worry about it.
All this is written out more slowly and carefully in the lecture notes.
But I wanted you to have a sense of the, uh, broad arc of the of the big picture of their derivation before you go through them yourself in greater detail in the lecture notes elsewhere.
So finally, what we want to do is take the derivative with respect to Theta of J of Theta, and set that to 0.
And so this is going to be equal to the derivative of one-half X Theta minus y transpose X Theta minus y.
Um, and so I'm gonna, I'm gonna do the steps really quickly, right.
So the steps require some of the little properties of traces and matrix derivatives I wrote down briefly just now.
But so I'm gonna do these very quickly without getting into the details, but, uh, so this is equal to one-half derivative of Theta of, um.
So take transposes of these things.
So this becomes Theta transpose X transpose minus y transpose.","why in cost function he did 1/2 and not 1/2*m ?
Why is trace mentioned, it isn't used in the lecture notes
1.14.54 my answer is (X^T X )+(X^T ^T X)-(X^T Y)-(Y X^T) its same or my ans is wrong ?
why is it that the cost function has the constant 1/2 before the summation and not 1/2m?
I don't really understand what you mean by 1/2m. However, from my understanding, the 1/2 is just for simplicity when taking the derivative of the cost ftn the power 2 will be multiplied to the equation and cancellyby the half."
"?] PROFESSOR: Good.
Yeah.
In the case when I have to do this summary insertion, I know this guy was empty.
Cluster high of x was empty.
So this call is just going to do these two lines.
Because I optimized the case of empty-- when a structure is empty, I spend constant time, no recursive calls.
That means in the case when cluster high of x is empty, and I have to pay to insert into the summary structure, I know my second call is going to be free, only take constant time.
So either I do this, in which case this takes constant time, or I don't do this, in which case I make one recursive call.","recursion in insert operation is not necessary since we can do it in constant time, I don't understand why he use recursion during insertion operation"
"This is basically li squared/ 2.
And so this is at most 1/2.
It's a slightly less than li squared/2.
So this is at most 1/2.
And this is basically a birthday paradox in this particular case.
That means there is a probability of at least a half that there is zero collisions in one of these tables.
So that means I'm basically flipping a fair coin.
If I ever get a heads I'm happy.
Each time I get a tails I have to reflip.
This should sound familiar from last time.
So this is 2 expected trials or log n with high probability.
We've proved log n with high probability.
That's the same as saying the number of levels in a skip list is log n with high probability.","Why log n trail whp?
Also didn't get that. If Pr[total space of 2nd level linear] > 1/2 (for some constant we get to choose which does not depend on the absolute value of n), then I think that we'd have Pr[#trials until we get linear space > t] < 1/2^t, that is Pr[#trials > lg s] < 1/s (with s=2^t) which is whp on s which has nothing to do with n imho. I don't see how it would depend on n. What am I missing?
Can anybody explain to me, why n people with n^2 birthdays gives a collision probability of 1/2?"
"Uh, so the labels are, uh, in this one-hot vector where we have a vector that's filled with 0s except with a 1 in one of the places, right? And- and- and- and the way we're gonna- the way we're gonna, uh, um, think of softmax regression is that each class has its- its own set of parameters.
So we have, uh, Theta class, right, in R_n.
And there are k such things where class is in here, triangle, circle, square, etc, right? So in logistic regression, we had just one Theta, which would do a binary, you know, yes versus no.
Uh, in softmax, we have one such vector of Theta per class, right? So you could also optionally represent them as a matrix.","What is h(theta) for the last example of softmax regression?
correct. Im also wondering why wouldnt we train k different binary logistic classifiers instead of the softmax, especially that we cant train the model to take input that is not in any of the k classes (say we want to classify the input as either dog, cat, mouse, and we input a horse); in a binary classifier it would output 0 for each of the dog, cat and mouse classifiers, but for softmax p(y)=0 which makes the likelihood 0 no matter what, so we cant train.
what is h(theta) for the last example of softmax regression?"
"But then you have this dangling.
So this is not connected.
This is not connected.
So it's not a clique.
So let's say this is vertex i and this is vertex j, and this whole thing is V dash.
And let's say somehow you have this anomaly.
You have that this guy is not connected to all the vertices.
So let's take one such pair.
And let's just say we remove xi.
So we just take the xi and remove it from V dash.
What that equates to in the Max-k-SAT is setting xi to 0.
So we take the original satisfying assignment and change the value of xi.
So let's see what that does.
So we take and set xi equal to 0.
And xi was originally 1.
Let's actually write it down.
So xi, or rather, i, j is not an element of E, but-- so these i, j were in the supposed to be clique, but the i, j is not in the edge set, so it's not actually a clique.
So the way we resolve that is just we do this.
We say, OK.
Let's just forget about this vertex.
Let's say it's just not in the clique.
So we set our xi to 0.
And what does that do? Let's look at how that affects the number of sat clauses.","Since 2sat is in P, can anyone explain the difference between max2sat and 2 sat?"
"That would be a counter-example, so if there's any number that's not postal, then there's at least one m by the well-ordering principal, because the set of counter-examples is non-empty if some number is not postal, so there's at least one.
So what we know, in other words, is that this least m that's not postal has the property.
It's not postal, and any number less than it is postal.
See what we can figure out about m.
First of all, m is not 0-- 0 is postal, because 0 plus $0.08 can be made with a $0.03 stamp and a $0.05 stamp.
M is not 0, because m is supposed to be not postal.
As a matter of fact, by the same reasoning, m is not 1 because you can make 1 plus $0.08 with three $0.03, and m is not 2, because you can make 2 plus $0.08-- $0.10-- using two $0.05.
So we've just figured out that this least counter-example has to be greater than or equal to 3, because 0, 1, and 2 are not counter-examples.
So we've got that m is greater than or equal to 3, the least non-postal number, so if I look at m minus 3, that means it's a number that's greater than or equal to 0, and it's less than m, so it's postal, because m is the least non-postal one.","How did he choose 3 as a postal number, he explained 0, 1, 2 are postal then supposed m>3 is the least non postal?
How do we know j and k are products of primes? Isn't that presupposing what we are trying to prove?
What does ""nonproducts"" mean ?
I'm still confused on the stamp example
So sir please explain which 2 primes make up the product of the integer 2???
m=3 is also postal right then why did he choose m>=3
We have a pre-assumption that states that integers are products of primes. We arent supposing that all integers are non products of primes. We are supposing that there exists a set of such integers that are non products of primes and by well ordering principle we know that they will have a least integer. Now the least integer is greater than 1 and can be any value. Now comes the j and k and that are also greater than 1 but less than k but the j and k donot exist in the set our number ""m"" does so the numbers j and k are bound by the theorem and thus are product of prime numbers"
"So when I say what's the shortest length of any path that there could possibly be that goes from S to D.
We know it's at least 5.
But can we say something more about it? Especially, when we look at these airline distances, and note that this airline distance is 6, and that's a little more than 7, and that's a little more than 7.
So what do you think? So it's gone from S to B, and the question is what's the shortest path that could possibly be that had started out going from S to B? 11 right? Because we can't have a path that's shorter than the airline distance.
If there were a straight line road from A to G, its length would be 6.
But there isn't.
So that gives us a lower bound on the distance that we have along that path.
So we're using the accumulated distance, plus the airline distance, to give us a lower bound on the path that we've started off on that goes from S to B.
Once again, let's solidify a little bit by simulating the search and seeing how it turns out.
Not just I did last time, I'm going to forget that I've got an extended list.","What is airline distance? Is it just synonym for Euclidean distance?
Wouldn't BFS give us shortest path as well?"
"OK.
Cool.
There are more interesting examples you've probably seen in problem sets, like if you want in the arbitrage problem, you want to find a path that has the minimum product of all the values, to convert products into a min.
Some problem, you just take logs of all the values.
So the reduction is compute logs and all the weights.
So you've probably seen lots of things like that.
Some are more complicated than others.
What we're going to do in this class is reduce instead of-- so for algorithms, you want to reduce to something you know how to solve.
We're going to reduce from something we know we can't solve under certain assumptions.
So assuming P does not equal NP, if A is NP-hard and we reduce from A to B, then we know the B is NP-hard.","Nice video and good explanations! However, I believe reductions should lead to the result ""at least as hard as"" instead of ""as hard as""."
"We're going to head over to Retriever to see if there any additional methods or attributes that are a consequence of being a subclass of Retriever that we need to add to our definition.
Now, we just hit the pass.
On the other hand, Retriever inherits from Dog.
So once again, we have to jump over to a super-class and grab any attributes or methods that are defined there as well.
So all the way back over to Sidney.
When I call Sidney.greeting(), the first thing that happens is that I look in the most specific subclass, or whatever my object type is and see if there's a definition for the method.","So when she goes over that any child class will be applied, will the main parent class _init_ be applied if there was __init__in the child class? Or will that be just completely negated?"
"So what's the overall complexity? I'm going to play the same game.
Let's let t sub n capture the time it takes to solve a problem of size n.
Just temporarily, I'm going to let s sub n denote the size of the solution for a problem of size n.
How big is that thing, smaller? And what do I know? The amount of time it takes me to solve the problem of size n is the amount of time it takes me to solve the slightly smaller problem-- that's the recursive call to genSubsets()-- plus the amount of time it takes me to run over that loop looking at everything in smaller and adding in a new version, plus some constant c, which is just the number of constant operations, the constant steps inside that loop, OK? And if I go back to it, t sub n is the cost here.
t sub n minus 1 is the cost there.
s sub n is the size of this.
And then I've got, one, two, three, four, five constant steps.
So c is probably 5 in this case.
So what can I say? There's the relationship.
Because I know s of n minus 1 is 2 to the n minus 1.
There are 2 to the n minus 1 elements inside of that.
How do I deal with this? Let's play the same game.
What's t sub n minus 1? That's t of n minus 2 plus 2 to the n minus 2 plus c.
And I could keep doing this.","Inttostr method complexisty is O(n) it deal with lenght of string and reduce it only by 1 everytime not by 10 times so it O(n) and if it how it is it will be log10(n) if we deal with digits as big number to iterate over
it says ""turns out that the total cost to copy is O(n) and this dominates the log(n) cost due to the recursive calls"" I agree that the cost will be O(n) but not with the part that says ""and this dominates the log(n) cost due to recursive calls"". The following is implied: O(log(n)) + O(n) = O(n), but such addition can't be the case. Log(n) is the amount of recursive calls, which should be used as a multiplicative factor with the amount of steps inside one function call. As was mentioned correctly, the amount of steps in each consecutive function call decreases by factor of 2. Suppose that n = 32; then the amount of function calls is s = log(n) = log(32) = 5. The total steps in all function calls combined is (1 - 1/(2^log(n))) * n = (1 - 1/2)n = (1/2 + 1/4 + 1/8 + ... + 1/2)n = n/2 + n/4 + n/8 + n/16 + n/32 = 32/2 + 32/4 + 32/8 + 32/16 + 32/32 = 0.96875n = 31  n (for large n) --> O(n). I don't see how the addition of log(n) with n is logical in the proof for the code being O(n).
I don't get the exponential complexity code
may I ask what you meant by Tn?
Let's say the time required by a program for input of size n is n+100. If you now go from 10 to 20, for 10, it takes 110 secs. But for 20, it takes 120 secs. For 30, it takes 130 secs. But difference between times for 10 and 20 and between 20 and 30 is same (10). Observe that from 10 to 20, time taken doesn't double. But the difference remains same"
"Python also has a nice thing, I think, in its standard operations, which is divmod of K, n.
Is that right? Yeah.
So if you want to use that, you can.
OK, so how do we sort these tuples? These are tuples, right? You guys are, I'm sure, very familiar with tuples by now.
How do I sort these tuples? What's the most important digit of this thing? If I had to sort one of the digits and get something that's close to sorted, what's more important-- the 1's digit or the n's digit? OK, we have discrepancy here.
Who says 1? Who says n? Someone who said n tell me why.
Oh, you all think that way for no reason.
AUDIENCE: [INAUDIBLE] JASON KU: Yeah.
Sorry.
This is a little confusing.
This is the 1's digit.
This is the n's digit.
This is the n's digit.
This is the 1's digit in how I'm writing this.
Does that makes sense? Yeah? AUDIENCE: [INAUDIBLE] have a different ones digit inside of it.
So you could have [INAUDIBLE] but that only tells you where they are with regard to the specific n category they're in.","what if there is a three digit number such as 456 in that list? what is the most significant and least significant in this number?
I don't understand how n/2 is greater than (n/2)^(n/2) in 17 min of video
Why do we have to break our number up if u<n^2?
Can someone explain the reasoning behind sorting least significant first and then doing most signification? I saw the example from the lecture and it worked on that. But why does it work?"
"It's called the standard form in CLRS, also called the general form.
In some cases, we'll look at the standard form for LP.
And I want to pop up a level about this example and give you the general setting.
And we'll focus in on the general setting for the most part.
But what I have here is I can either minimize or maximize-- we had a minimization problem-- for the political problem, minimize the linear objective function subject to linear inequalities or equations.
And the variables, think of x as a vector, it's a column vector, or x1, x2, all the way to xn.
And the objective function is c times x, so that's c1x1 dot, dot, dot, cnxn.","Is there restriction on the number of decision variables and constraints used in an LPP?
When introducing general form of LP, shouldn't the objective function be x*c transpose? Since c is also a vector which means that it's a column matrix..."
"Insertion.
Let's start with insertion.
We already did searching.
So insertion is you bring in a new key K, and you want to insert it into the tree.
So what's the problem that could happen? You can find the location where you want to insert it, just like searching.
You just go down the tree and find where it should be placed.
But once you do place it, you have a problem.
What is the problem? The problem is that one of your nodes will become overfull.
Whatever.
It'll overflow, and that's not what you want.
So you want some way so you can manage this.
How do you manage this? So I have this lovely prop here, which I hope to demonstrate.
OK.
So here we have B equal to 4.
So let's first figure out the number of keys.
So what is the minimum number of keys, anyone for B equal to 4? AUDIENCE: Three.
PROFESSOR: Three, precisely.
So what is the maximum number of keys? AUDIENCE: Six.
PROFESSOR: 4 into 2 minus 3, yeah.
Correct.
3, 4.
It's not seven, there's a strictly less than sign somewhere.
Yes.
And you'll see why it's not seven in a minute.
[LAUGHTER] Oh.
Hypocritical of me.
All right.
So as you can see, 1, 2, 3, 4, 5, 6, 7.
So some insertion happened.
Is the writing clear? Can everyone read the numbers? 49 looks a little skewed.
Anyway, essentially these are all sorted.
This is the parent node.
Doesn't matter what's over here.
All that matters is 8, 56, and whatever's in between.","In the Cormen textbook, section 18.1, it is written that if a node has 2*B-1 keys then it is called ""full node"" and once the # keys become 2*B then split occurs. But in this lecture, split occurs at 2*B-1 itself as here max # keys should be strictly less than 2*B-1. So practically which one to prefer is the confusion here 
it is a good lecture for clearing the concept of B tree. But I have a doubt that value of maximum keys in a node is less than 2*B-1 or less or equal to 2*B-1 OpenCourseWare.
can someone explain why in the last bit we chose to rotate 17 on the left over 30 on the right? thxxxxx in advance!
I think the reason to use these trees is balancing! Near the beginning, he asks ""Why use B-trees over Binary Search Trees"". He then explains something about avoiding reading memory from disk. This is not the reason I was taught in my class. The reason I was taught, is that these trees will stay balanced. With a Binary Search Tree, the algorithm of insert could result in the tree being extremely deep on one branch and very shallow on others. This means you will not actually get log(n) performance, since most of the data is in longer branches.
how is the order of B tree related to the branching factor? Also how do you find the branching factor?
B is defined as the branching factor. By definition, this establishes a bound on the number of children: [B,2B) and hence a bound on the number of keys: [B-1, 2B-1]. You can what B is by taking the floor of lg(# of children in any branch)....(correct me if im wrong here...)
Any one please explain how its no of children B<= No children < 2B . When B=10 it can have macimum of 11 children . then ???
isnt his definition wrong though? in CLRS 3rd edition page 489 the definition for internal nodes of degree t is: t <= num children <= 2t"
"And so you could miss it.
A given r vector might miss it, and of course, if you keep generating the r's, you'd like to find it and declare that the matrices weren't multiplied correctly and that probability is what we have to compute.
So we want to get this result where we are analyzing the correctness in the case.
You've already analyzed the correctness in the case where AB equals C, but now we have to analyze the correctness in the case where AB is not equal to C.
Right? And so the claim is that if AB is not equal to C, then the probability of ABr not equal to Cr is greater than or equal to half.","What is the collection of r vectors? Doesn't the 1-1 mapping argument require that your set of vectors (the support of the randomization) is close under addition by v. But without knowing v ahead of time, this might require an infinite set of vectors, in which case a bijection is not enough to guarantee equal measure between sets.
Take A=[[1, 0], [0, 1]], B = [[1, 1], [0, 0]] and C = [[1, 1], [1, 1]] AB not= C, but Frievald's alg returns YES for r=[1, 1]. The proof shows, however, that there exists an r'=[1, 0] such that Frievald's alg returns NO."
"And then the last one is a bit tricky.
And we will have an app that requires this way at the end.
But this is infeasible given h of x to produce h of x prime, where x and x prime are-- and it gets a little bit fuzzy here-- are related in some fashion, right? So a concrete example of this is, let's say that x prime is x plus 1.
So this is a reasonable example of this.
So what this says is you're just given h of x.
It doesn't actually say anything about one-wayness yet.
But you could assume, for example, that if this was a one-way hash function, that it would be possible to get x from h of x, correct? And let's keep that though.
Hold that thought, all right? We're going to get back to it.
So if I'm just given the hash through some computation, it may be possible for me to create another hash, h of x prime, such that there's some relationship that I can prove or argue for between the strings that created the hashes, namely x and x prime, OK? That's what malleability is, right? Now you might just go off and say here's an x, here's a y, here's h of x, and here's h of y.","Consider simpler examples to get the idea: If h(x) =x^2 (square of x) Clearly h(x) is not OW... x=sqrt( h(x) ) However, there is no collisions CR & TCR is satisfied . -A reverse example, let H(x) = XOR(xis) {I mean some kind of XORing of different bits of x in some manner You see here the reason that makes H(x) OW (hard to retrieve x after the mangling) is almost the same reason it is NOT CR & NOT TCR (It's not that we can't find an x given h(x), it is there r so many valid Xs) . and conceptually, there r applications that require OW not TCR (like storing pswds hashes to not reveal them), and other applications that cares more about TCR not OW (like file signatures where the file is not a secret, u care only that it has not been modified by an opponent or a virus for example by storing its hash... here ur worry is that they can modify F to F' where h(F) =h(F')...note that even if F' doesn't make sense and u won't be deceived by it the original F will be lost)"
"So really all you're trying to do is minimize this negative sum of losses of your children, okay? So let's move to the next board here.
[NOISE] So I started to find this misclassification loss.
Let's get a little bit into actually why misclassification loss isn't actually the right loss to use for this problem, so, okay? And so for a simple example, let's pretend- So I've sort of drawn out a tree like this, Let's pretend that instead we have another setup here where we're coming into a decision node.
And at this point we have 900 positives and 100 negatives, okay? So this is sort of a misclassification loss, of 100 in this case because you'd predict the most common class and end up with 100 misclassified examples.",What is the loss function for Decision Tree?
"So previously, it was just all weights were 1, so maximum cardinality was what we wanted.
But now we want to schedule a subset of requests with maximum weight.
Someone give me an argument as to whether the greedy algorithm earliest finish time first is optimal for this weighted case, or give me a counter example.
Yep, go ahead.
AUDIENCE: Oh, well, you know like your first example you have your first weight of the first interval, it took the whole time, [INAUDIBLE] would have three smaller ones? Well, if the weight of the first one was 20 and then-- SRINIVAS DEVADAS: Exactly, exactly right.","The interval scheduling proof makes no sense. I don't understand what l' is attempting to show. My understanding of l' is the set of compatible requests after the first shortest end time is selected. Then what? How does this tie into the proof of showing that the greedy algorithm produces the optimal solution?
Why can greedy algorithm solve the interval scheduling problem? Is there any limitation to the type of problems greedy can solve? And how to develop an intuition to guess the best strategy, such as the earliest finish time? on the first sight, the minimum conflict strategy totally makes sense, although one counter example proves its invalid. My other guess is to a strategy which concurrently picks smallest tasks and minimizes time waste (intervals between tasks) ."
"So back to our observations.
So the first thing we observed is that we have eliminated phi of x completely, right? That's, that's the, uh, most important, um, most important step.
The, uh, second observation that we make is to make a prediction, we need to remember all our test examples, right? So for prediction, we need training examples to be stored in memory, okay? And this is probably the most distinctive feature from linear regression that we've seen.
In linear regression, we started with a training, training set, with x's and y's.
We learned the theta vector by performing, say, the normal equations or gradient descent, and once we obtain the theta vector, we could discard our entire training set and only carry forward theta- the theta vector from that point on.","If I understand correctly, we are no longer estimating parameters like theta and instead are storing the values of Beta and all the training examples. Since in this example we would have a tendency to overfit, how would we regularize it? Can we apply Ridge/Lasso on Beta instead of Theta?"
"Um, and- but, you know, let's, let's go with natural language for now.
So here- here's an example of how you can draw inferences using natural language.
Okay? So a dime is better than a nickel.
Um, a nickel is better than a penny.
So therefore, a dime is better than a penny, okay? So this seems like pretty sound reasoning.
Um, so what about this example, a penny is better than nothing, um, nothing is better than world peace.
[inaudible].
Therefore, a penny is better than world peace, right? Okay.
So something clearly went, uh, wrong here.
And this is because, languages- natural language is kind of slippery.
It's not very precise, which makes it very easy to kind of make these, um, these mistakes.
Um, but if we step back and think about what is the role of natural language, it's really- language itself is an ex- mechanism for expression.
So there are many types of languages.
There's natural languages, um, there's programming languages, which all of you are, you know, familiar with.
Um, but we're going to talk about, a different type of language called logical languages.
Um, like programming languages, so they're gonna be formal.
So we're gonna be absolutely clear what we mean by when we have a statement in a logical language.",I have a few doubts from this lecture: 1) can we say A <-> B based on one w alone (egs: A=0 & B=0 in w) ? 2) Can any provide an example (interms of truth values) where A -> B doesnt imply A <-> B
"What's missing? AUDIENCE: The cache.
PROFESSOR: The cache, exactly.
Well, you fixed your own little error there.
It was a trick question.
So there's no recursion-- I'm sorry, there's recursion here, but no memoization.
So this is exponential complexity.
You will recur.
In fact, the recurrence for that is something like T of n equals 1 if n equals 1, and 2T n minus 1 if n greater than 1.
And this would be 2 raised to n minus 1 in complexity.
Now there's a single line of code, and you all know this, that would fix this.
And that single line of code is simply something that says, right here, look at the lij-- and I'm writing this differently.
I'm calling this now a 2D array.
So that's why I have the open brackets and close brackets.
So I'm overloading l here.
But it's a 2D array that is going to essentially be a cache for the subproblem solution values.
And then if you want to do the backtracing to actually compute the solution, you could certainly have that as an additional record that's connected to this very same value.","One thing I don't get about all those recursions is, why are you guys don't mention that stack is limited and able to hold limited recursive calls to functions, and for some big enough input number for problem to be solved for, you might get stack overflow. Or is this all just theory and one shouldn't be thinking about how to use it in practice?"
"And then I keep going.
And then if I ever realize I've made a bad guess, I have to undo everything by zeroing them all out, which is making them all empty.
So one thing that this code does, and you can take a look at it.
And I would encourage you to do the first exercise, which is taking these implications and making them a little more powerful by adding three or four lines of code to this code.
And exactly what you have to do in this exercise, and I'll show you what the results should be in just a minute.
But let me just spend 30 seconds explaining to you how you could do a little bit better than what this code does.
So what I've described to you really is get this set, imply, get a singleton, et cetera.
And then you can do this, obviously, for each of these sectors.
And that's what this does.
You had a for loop up there that does it for each of the sectors.
Grab a sector and go ahead and do an implication for that sector.
Now this code just runs through the sectors, you know, One, two, three, four, five, six, seven, eight, nine and then discovers the implications if they exist, adds them to the imply list, and then throws up its hands and says I'm tired, I'm done, I don't want to do any more.
What could you do that's an improvement, given what we have described and what I've told you so far.
What is an incremental improvement over going over these sectors once and doing these implications and storing them and moving on? What is an incremental improvement? Ganatra? AUDIENCE: Look, once we get all the singletons, we can set those as-- since those are determined, like, deterministic, I think that we could set those into the original grid and say that's our new base grid and run through it again.",I was just thinking... how does the program handle multiple solutions? Seems it stops with the first solution.
"So greeting is going to be executed before greeting used in any other place.
You notice the only difference between this greeting and the Dog greeting is that ""OHAI!"" has been prepended to the phrase.
And the way that we end up doing that is we refer to-- we concatenate, and then refer to the superclass.
And once again, we have to pass in the explicit argument, self, when we're talking about a class definition.
Later, when you actually instantiate an object and use your parens, you're not going to have to put self as an argument.
It'll get confused.
We'll go over that in a second.
So let's say I create a golden retriever, Sidney.
I'm going to pass in one argument, which is the name.
We're going to consider all the definitions here first, which means that goldens are going to have a method for greeting that is specified here.
It's going to use the method for greeting from Retriever.
And we could put in anything here, right? We could put Dog.greeting.
We could put in some other function that is in the same environment as class Golden.
But here, we can explicitly access the superclass that we defined here.","So when she goes over that any child class will be applied, will the main parent class _init_ be applied if there was __init__in the child class? Or will that be just completely negated?"
"Which is why it's called a congruence.
A congruence is an equality-like relation that respects the operations that are relevant to the discussion.
In this case, we're going to be talking about plus and times.
And the first fact about congruent says that if a and b are congruent, then a plus c and b plus c are congruent.
The proof of that follows trivially from the definition.
Because the a congruent to b mod, n says that n divides a minus b.
And if n divides a minus b, obviously n divides a plus c minus b plus c.
Because a plus c minus b plus c is equal to a minus b.
That one is deceptively trivial.
It's also the case that if a is congruent to b, then a times c is congruent to b times c.
This one takes a one line proof.
We're given that n divides a minus b.
That certainly implies that n divides any multiple of a minus b.
So multiply it by c and then apply distributivity, and you discover that n divides ac minus bc, which means ac is congruent to bc modulo n.","I don't quite understand the final steps of the last example. It seem's he arrived at the answer on the very first line: r(287,4)=3 so the answer is 287^9=3(mod4). Why did he bother with the last 3 steps?
Why does he say 'Equivalent' instead of Congruent?
""if r(287,4)=3 then 287^9=3(mod4)"", we aren't allowed to make that leap. ""if r(287,4)=3 then 287^9=(3^9) (mod4)"", still need to deal with the (^9). 3^9 = 3^2 * 3^2 * 3^2 * 3^2 * 3 3^2 mod 4 == 1, so the right hand side of the above expression reduces to 1 * 1 * 1 * 1 * 3 = 3"
"Right? That the input, uh, size X, and output a number as a- as a linear function, um, of the size X, okay? And then, the mathematicians in the room, you'll say technically this is an affine function.
It was a linear function, there's no theta 0, technically, you know, but- but the machine learning sometimes just calls this a linear function, but technically it's an affine function.
Doesn't- doesn't matter.
Um, so more generally in- in this example we have just one input feature X.
More generally, if you have multiple input features, so if you have more data, more information about these houses, such as number of bedrooms [NOISE] Excuse me, my handwriting is not big.","I didn't understand the linear regression algorithm is there any way to understand it better ??
The value of x0 is always one 1. So theta0 can rely on x0 for the update. If we have single feature then h(X) =x0*Theta0 + x1* theta1 (which is ultimately equal to theta0 + x1*theta1 as x0=1, theta0 can also be referred as intercept and theta1 as slope if you compare it with the equation of a straight line such that price of house is linear function of # of bedrooms)
Wonder: Is m equals to n+1 n stands for number of inputs, while the m stands for the number of the rows which includes X0 in addition.
No not necessarly m is the number of rows, and n is the number of column or features. In his example n is equal to two (Size, and bedrooms), m can be any number. But i think that in the example m is 50"
"Okay.
And that chance node is, is a state action node.
It's going to be S and the action- I've decided the action is pi of S.
Okay.
And of this- define this new function, this Q function, Q pi of S, a, which is just the expected utility from the chance node.
Okay.
So, so we've talked about value, value is expected utility from my actual states.
I'm going to talk about Q values as expected utilities from the chance nodes.
So after you've committed that you, you have taken action a, and, and you're following policy pi.
Then, what is the expected utility from that point on, okay.","I think the given definition for value-action function (Q(s, action)) is not correct. In fact value function is the summation of value-action functions over all actions."
"Then we're going to look at v dot summary search for this the successor of high of x.
Right, finding the next 1 bit, that is successor.
And then, I want to find the first 1 bit in that cluster.
Is that a successor also? Yeah.
That's just the successor of negative infinity.
Finding the minimum element in a cluster is the successor of -1, or 0, or not zero.
But -1 would work, or negative infinity, maybe more intuitively.
That'll find the smallest thing here.
So each of these is a recursive call.
I can think of it as recursively calling successor.
So let's do that.
I want to find the successor of x in v.
First thing I'm going to do is do the ij breakdown.
I'll let i be high of x and j be-- I could do low of x.
But what I'm going to try for is to search within this cluster, high of x.
So I'm going to look for the successor of cluster i, which is cluster high of x, of low of x.
OK, so that's this first step of looking in x's cluster.
This is x's cluster.
This is x's name in the cluster.
I'm going to try to find the successor.
But it might say infinity.
I didn't find anything.
And then I'll be unhappy if j equals infinity.
So that's line one.
Well, then we're in the wrong cluster.
High of x is not the right cluster.
Let's find the correct cluster, which is going to be the next non-empty cluster.
So I'm going to change i to be the successor in the summary structure of i.
So i was the name of a cluster.
It may have items in it.
But we want to find the next non-empty thing.
Because we know the successor we're looking for is not here.",Isnt the successor function wrong? It doesnt have a base case. It should never terminate.
"So obviously, this is a cat.
We know that.
If the model guessed correctly, there would be no update to the model.
As far as the model is concerned, I'm 100% accurate.
Why would I change? Well, let's say that the model gets it wrong.
Let's say the model guessed dog.
Then in that case, we would go ahead and update the model.
So again, we have, model now knows it's a cat.
It's going to update itself.
So what should the model now say about cats? What does the model-- based on this example, what does the model now know about cats? SPEAKER 2: Answers from participants: SPEAKER 1: Are you looking for things like pointy ears, pink nose, round eyes? SPEAKER 2: I feel like there's not even enough data to say that yet.
With such a small training set, all the model knows is that a cat looks like that.
BRANDON LESHCHINSKIY: Ah, OK.
So let me ask-- so good intuition.
So we heard pointy ears, multicolor, laying on a bed, maybe, paws.
In our model-- let me update it-- are we going to say always have those things-- cats always have those things, or cats sometimes have those things? Do cats always have pointed ears and are on a bed and have a pink nose, or is it sometimes? What do we think? SPEAKER 1: Answer from participant: SPEAKER 2: Sometimes, definitely.",How do you know the data you are using is already cleaned or have to be cleaned
"One situation is that I find the successor somewhere in this interval.
In that case, I'm happy.
Because I just need this one recursive call.
OK, the other case is that I don't find what I'm looking for here.
Then I have to do a successor up here.
And then I'm done.
Then I can teleport into whatever cluster it is.
And I've stored the min by now.
So that's constant time to jump into the right spot in the cluster.
So either I find what I'm looking for here, or I find what I'm looking for here.
What would be really nice is if I could tell ahead of time which one is going to succeed.
Because then, if I know this is not going to find anything, I might as well just go immediately up here, and look at the successor in the summary structure.
If I know I'm going to find something here, I'll just do the successor here.
And I'm done.
If I could just get away with one or the other of these calls, not both, I'd be very happy.",Isnt the successor function wrong? It doesnt have a base case. It should never terminate.
"Um, and, and we're only interested in the diagonal elements here.
[NOISE] All right so that wraps up our coverage of factor analysis oh, and, uh, [NOISE] I think there was also one question which a- a student asked by doing all of this, how did we actually go about solving the- the singularity problem where, you know [NOISE] so if you remember, um.
We briefly wrote this notation, [NOISE] that for factor analysis, you know, log p of x, of the marginal likelihood actually had this closed-form expression where it turns out to be, um, our x happens to be a normal with mean Mu and covariance LL transpose plus Psi, right? So the covariance matrix estimated for the marginal of x is therefore this LL transpose, which is a low-rank matrix plus a diagonal matrix.
And because we are adding a diagonal matrix to this low-rank matrix, we are guaranteed that this is no longer singular, [NOISE] right? All right, so let's move on uh, so this- this basically wraps up factor analysis, and now we will start [NOISE] Principal Components Analysis.
PCA.
[NOISE] PCA.
[NOISE] So in factor analysis, our goal was to- to model X's as something that approximately lies in a low-dimensional space, right? So for every X we had a corresponding Z, and Z lived in a low-dimensional space, and the relation between X and Z was- was, uh, what you call as affine.
Affine is another word for linear plus some offset, right? So, um, L defined, um, L gives us from Z to the centered X, and then you add some kind of an offset, so X and Z are related by what's also called an affine relationship, right? So, um, and we had to go through this EM algorithm because we were, um, um, we were modeling it with this- with the assumption that d was much larger than n, right? PCA is another approach to, uh, where you are given X_i [NOISE] i equals 1-n, and X_i [NOISE] is, uh, in d.","Hello Professor, is there an intuitive reasoning for us to consider eigen vectors of the sample covariance matrix to maximize variance, whilst reducing dimensions?"
"And then I'm going to return the centroid.
Variability is exactly what we saw in the formula.
And then just for fun, so you could see this, I used an iterator here.
I don't know that any of you have used the yield statement in Python.
I recommend it.
It's very convenient.
One of the nice things about Python is almost anything that's built in, you can make your own version of it.
And so once I've done this, if c is a cluster, I can now write something like for c in big C, and this will make it work just like iterating over a list.
Right, so this makes it possible to iterate over it.
If you haven't read about yield, you probably should read the probably about two paragraphs in the textbook explaining how it works, but it's very convenient.
Dissimilarity we've already seen.
All right, now we get to patients.
This is in the file lec 12, lecture 12 dot py.
In addition to importing the usual suspects of pylab and numpy, and probably it should import random too, it imports cluster, the one we just looked at.
And so patient is a sub-type of cluster.Example.
Then I'm going to define this interesting thing called scale attributes.
So you might remember, in the last lecture when Professor Grimson was looking at these reptiles, he ran into this problem about alligators looking like chickens because they each have a large number of legs.","Thanks for an amazing lecture! it tries to cluster data into two groups and see if it correctly differentiated people who dies of heart attack and those that didn't. To me this is using clustering for classification task, if yes, when would someone use clustering rather than classification?"
"It's just- when the patient walks into your office, before you've even examined them, before you've even seen them, what are the odds that their tumor is malignant versus benign, right? Before you see any features, okay? And so using Bayes' Rule, [NOISE] if you can build a model for P of x given y and for P of y, um, if- you know, if you can calculate numbers for both of these quantities then using Bayes' rule, when you have a new test example [NOISE] with features x, you can then calculate the chance of y being equal to 1 as this, [NOISE] right? Where P of x by the - [NOISE] okay? [NOISE] Um, and so if you learn this term, P of x given y, then you can plug that in here, right? And if you've also learned this term P of y, you can plug that in here.
Right.
Um, and so P of x in the denominators, goes into denominator, okay? So if you've learned both- both of those terms in the red square and in the orange square, you could plug it into all of those terms and therefore use Bayes' rule to calculate P of y equals 1, given x.","Phi is the probability that he also denotes as p(y). This is by definition the probability that a new person will be positive, given no further information, so given no data. Basically, it is the best guess you have for whether a random person is positive, before you know anything about the person. It makes sense that no further information on the individual your best guess is just the ratio in the general population. He is trying to clarify this in his sentence: ""What is the chance that the NEXT patient that walks into your office has a malignant tumor?"". The key word is next, so this is happening in the future and you can't have any data yet on that person, but I agree it's misunderstandable. The probability that given the features the person is positive is denoted as p(y|x). He gives the way how to calculate this earlier in the lecture. Unsuprisingly, this does depend on the data x. So if you have features, you do consider them to make your prediction, which is p(y|x)."
"It's just going to be there for all of the cases, all of the guesses.
So that's it.
That's our recurrence relationship corresponding to the DP for this particular problem.
You can go figure out what the complexity is.
I have other things to do, so we'll move on.
But you can do the same things, and these are fairly mechanical at this point to go write code for it, trace the solution to get the optimum binary search tree, yadda, yadda, yadda.
Any questions about this equation or anything else? Do people get that? Yeah, go ahead.
AUDIENCE: What is the depth input? PROFESSOR: So the depth is getting added by the weights.","One thing I don't get about all those recursions is, why are you guys don't mention that stack is limited and able to hold limited recursive calls to functions, and for some big enough input number for problem to be solved for, you might get stack overflow. Or is this all just theory and one shouldn't be thinking about how to use it in practice?
It is very subtle, indeed. As the instructor said, for e(i,j) you add wi to wj once. For the next depth, when you compute e(i, r -1) you again add wi to w(r-1) so in total you add wi to w(r-1) twice, which corresponds to the increasing depth. The same thing goes for e(r+1, j). you add w(r+1) to wj once again for this calculation and in total w(r+1) to wj is added twice. But wr is added only once since it remained in the upper level (depth) and not included both e(i, r-1) and e(r+1, j)"
"So this is one way how we can look into this.
Another way how we can look into this is that, so far in this class we've been talking about the deep graph encoders, where basically the idea was that, we have a complex network, complex graph, complex relational structure on the input and you want to pass it through several layers of this representation and a deep learning network that at the end produces-- let's say node embeddings, edge embeddings, entire graph embeddings, right? So this is what we call a deep graph encoder because it takes the graph as the input and encodes it into some kind of representation.
The task of graph generation actually goes in the other direction.
It wants to start on the right-hand side and then through a series of complex nonlinear transforms wants to output the entire graph, right? So our input perhaps will be a little small noise parameter or something like that and we'll want to kind of expand that until we have the entire graph on the output.
So we will be kind of decoding rather than encoding, right? We'll take a small piece of information and expand it into the entire graph rather than taking a complex structure and compress it into or into its representation.
So we are talking about deep graph decoders, because on the output we want to generate an entire network.","Thanks a lot for the lecture! Is it possible to generate a very regular topologically weighted graph? I mean the mesh, for example polygonal geometry."
"So I haven't told you how to solve this problem.
But I'm convinced that it's at least this amount of time.
So remember that omega means lower bound.
So when I put it all together, in some sense-- OK, this isn't satisfying in the sense that I didn't give you precisely the runtime of this algorithm.
But hopefully, I've convinced you that it's super useless.
Yeah, OK.
Any other questions about that? But great.
So if we go back to our table for the set interface, well, in some sense, if we implemented it using this goofy algorithm, then the lower left entry in our table would be n factorial times n, which wouldn't be so hot.
But notice that actually all the rest of our operations are now quite efficient.
I can use binary search.",You clearly don't understand what the Omega notation is in Algorithms.
"So, for example, node B here takes information from two- two other nodes, uh, A and C because they are the neighbors of it, uh, in the network.
And then, of course, the goal will be to learn, uh, the transformations in- um, in- in this, uh, in this neural network that- that would be parameterized and this way, uh, our- our approach is going to work.
So the intuition is that network neighborhood defines a computation graph, and that every node defines a computation graph based on its, uh, network neighborhood.
So every node in the graph essentially can get its own neural network architecture, because these are now different kind of neural networks, they have, uh, different shapes.","How come a common W and B works for all the node embeddings, like in example some has 2 no of input others have 3 additionally, since the order of the weights are also different wouldn't that also impact the contribution of each input. How are we sure that it would be okay using the same W and B for all nodes? Kindly educate if i understood it wrong"
"F equals c S T for some cut, S,T.
This says that some cut is saturated.
Right, so that's just the statement.
The second statement, which is equivalent to the first one, says that f is a maximum flow.
So f being a maximum flow means there's some cut that's saturated.
If there's some cut that's saturated for a given flow, you have a max flow.
And then the last one, which is important for our Ford-Fulkerson algorithm, is that f admits no augmenting paths.
And so if I want to show that the Ford-Fulkerson algorithm terminates with a max flow, tell me, based on implications-- like i implies j, for numbers i and j, what it is that I need to prove.","max flow |f| = some c(S,T) cut..... not any I think. some (S,T) cut may have a larger capacity which is okay since all we need id f < any (S,T) cut. here , he just showed that there is always some (S,T) cut out there such that |f| = capacity of that (S,T) cut. did I make sense?"
"And we have something of the form true ""and"" something or other.
That means that the net outcome of this expression, it depends entirely on the something or other.
That is, it depends entirely on whether y is greater than 0.
Because the x is less than or equal to 0.
And so this expression can be simplified.
It's going to behave exactly according to whether or not y is greater than 100.
So look what I've just done.
I've argued that in this case, both of these tests cards act like y is the test y greater than 100.
Which is, they behave the same in this case as well.
So what I just figured out was that, in both cases, these two expressions yield the same result.",what happens if x = 0 you did not cover that case ((if x <=0) || y>100)
"You guys can figure that out.
I think that's 0.2.
So you'd expect that, that it should be much smaller than either of the first two probabilities.
This is the most common rule, it's something we use all the time in probabilities, the so-called multiplicative law.
We have to be careful about it, however, in that it only holds if the events are actually independent.
Two events are independent if the outcome of one has no influence on the outcome of the other.
So when we roll the die, we assume that the first roll, the outcome, was independent of the-- or the second roll was independent of the first roll, independent of the fourth roll.
When we looked at the two coins, we assume that heads and tails of each coin was independent of the other coin.
I didn't, for example, look at one coin and make sure that the other one was different.
The danger here is that people often compute probabilities assuming independence when you don't actually have independence.",What were the coins though?
"You do well in other classes, you're likely to do well in this class.
I'm going to use this one very carefully.
Prior programming experience is at least a predictor, but it is not a perfect predictor.
Those of you who haven't programmed before, in this class, you can still do really well in this class.
But it's an indication that you've seen other programming languages.
On the other hand, I don't believe in astrology.
So I don't think the month in which you're born, the astrological sign under which you were born has probably anything to do with how well you'd program.
I doubt that eye color has anything to do with how well you'd program.
You get the idea.
Some features matter, others don't.
Now I could just throw all the features in and hope that the machine learning algorithm sorts out those it wants to keep from those it doesn't.
But I remind you of that idea of overfitting.
If I do that, there is the danger that it will find some correlation between birth month, eye color, and GPA.
And that's going to lead to a conclusion that we really don't like.
By the way, in case you're worried, I can assure you that Stu Schmill in the dean of admissions department does not use machine learning to pick you.
He actually looks at a whole bunch of things because it's not easy to replace him with a machine-- yet.
All right.
So what this says is we need to think about how do we pick the features.
And mostly, what we're trying to do is to maximize something called the signal to noise ratio.",Th concept is understandable but how to make the right algorithm and set the data . Im new to this . Anyone cares enough to guide me through this ?
"This algorithm works by picking two examples, clustering all the other examples by simply saying put it in the group to which it's closest to that example.
Once I've got those clusters, I'm going to find the median element of that group.
Not mean, but median, what's the one closest to the center? And treat those as exemplars and repeat the process.
And I'll just do it either some number of times or until I don't get any change in the process.
So it's clustering based on distance.
And we'll come back to distance in a second.
So here's what would have my football players.
If I just did this based on weight, there's the natural dividing line.",Th concept is understandable but how to make the right algorithm and set the data . Im new to this . Anyone cares enough to guide me through this ?
"The number of permutations of 10 digits that have both a 60 and a 42 pattern is 8 factorial.
Now, that's the case of an intersection where these two things don't overlap.
Let's look at the case of P 60 intersection P 04.
Well, if it's got both a 60 and a 04, it actually is the same as having a 604.
So the intersection of P 60 and P 04 is the set of sequences that have the pattern 604.
And I count those in the same way.
I say, OK.
I've got an object 604 plus the remaining digits-- 1, 2, 3, 5, 7, 8, 9 for a total of eight objects, and the number of permutations of the 10 digits that half the pattern 604 corresponds to the number of permutations of these eight things.","how the heck is he getting 193 from 3*9!-3*8!???
in class have 14 student ...10 of them study English 8 of them study franch 5 of them study Spanish ..and 5 study English and franch 2 study English and spanish 4 study franch and Spanish... first question how many students study these three languages what formula should i use here ?"
"So what I've done here is invoke, essentially, this, except it's not exactly that, in the sense that it's written a little bit differently.
But if you see what's going on here, what I've done is look at this s, and I've said, think of this s as being cap V minus s.
Right? So that gives you s.
And those are clearly disjoint, right? Those are clearly disjoint sets.
There is this one and this one are disjoint sets.
That's what I mean to say.
I mean, these two aren't disjoint, but this and that are disjoint.
And that's what you need, in order to invoke the little property that you have here.
And so that all make sense? You see why I did that? OK? What can I say about either of these two quantities? Can I say something about either of these two quantities? Yeah? STUDENT: f(V,V) is 0.","What did that student mean at https://youtu.be/VYZGlgzr_As?t=3377?? We did consider c at f(d,c) which is -1 . Not sure what did he mean with his question."
"This is done because it just simplifies the math and the optimization, that seems a little bit ugly that actually makes building word vectors a lot easier, and really we can come back to that and discuss it later.
But that's what it is.
And so then once we have these word vectors, the equation that we're going to use for giving the probability of a context word appearing given the center word is that we're going to calculate it using the expression, the middle bottom of my slide.
So let's sort of pull that apart just a little bit more.
So what we have here with this expression is so for a particular center word and a particular context word O, we're going to look up the vector representation of each word.
So there U of O and V of C, and so then we're simply going to take the dot product of those two vectors.","I am wondering how the two vectors (Uw, Vw) are determined for each Word?
Objective function seeks to maximise the probable likelihood of context word given center word. However should it also not try to minimise the probability of incorrect context words given center word?"
"Okay? And we said here that this was going to be the difference from Hoeffding between Q of at - Q hat of at.
So the probability that this, remember I called this U, the probability that this was greater than U was small.
So now we're assuming that all of our confidence bounds hold, which means that we know that the difference between the real empirical mean and the true mean of this arm is bounded by U.
Yeah.
Going back, for the bottom panel, sorry, it's a little hard to see, two questions.
First of all, where does this second, um, so you have a union over i = 1, the number of arms.
I don't see where that index actually factors in.
And then also if you could just go over the third line with the delta over t squared, and summation, how we derived that.
Sure? Yeah, so, um, so what we did there is we said that if, um, are you asking about the second line to the third line? Yes.
So what we did that in this case is, um, we said for each, we wanna make sure that on each of the time steps, all of the upper confidence bounds hold.
Um, and so that's where we get an additional sum, um, over here over all the arms.
So this is conservative, um, trying to make sure we don't know- you could imagine just doing this over the arm that's selected, and but we don't know which arm is selected.
We want to be able to show, um, this is going to be a looser upper, upper bound saying this is sufficient.","Using unions with probabilities can be a little confusing. In fact, it may be related to my following misunderstanding: if Q(a*) > U_t(a_t) whenever the upper bounds hold, how can be that the probability of the former is less than the probability of the latter?
Li Yep, I was referring to that moment. I think I got It wrong when I wrote that (using unions with probabilities is still terrible, they should use sums and mention dependence and how we get rid of it). She calculates is the probability of choosing the wrong arm at some step. Effectively when all the bounds are met, Q(a*) < Ut(at) (I got that reversed the first time). Therefore, Q(a*) > Ut(at) implies that some bound does not hold and thus, Q(a*) > Ut(at) is less likely than the failure of some bound."
"Um, uh because if you do that, then this optimization objective [NOISE] becomes- [NOISE] Um, maximize 1 over norm of W subject to- [NOISE] right? Uh, so it substitutes norm of W equals 1 of gamma, and so that cancels out, and so you end up with this optimization problem instead of maximizing 1 over norm W, you can minimize one half the norm of W squared subject to this.
[NOISE] Right? Okay, and so that's a rough- I know I did this relatively quickly.
Again- as usual the full derivation is written on your lecture notes but hopefully this gives you a flavor for why.
If you solve this optimization problem and you're minimizing over W and B that you are solving for the parameters W and B that give you the optimal margin classifier.","Question for the kind hearted people who understood: in the geometric margin, y is either 1 or -1 right? But wasnt it mentioned before that y is either 1 or 0? I got a little messed up on that."
"If this is going by too fast, just walk it through yourself later on.
But I'm literally just using this expression to do the reduction until I see the pattern.
All right, what's that? Well, if your Course 18 major, you've seen it before.
If you haven't, here's a nice, little trick.
Let me let a equal that sum, 2 to the n minus 1 plus 2 to the n minus 2 all the way down to 1.
Let me multiply both the left and the right side by 2.
That gives me, 2a is equal to 2 to the n plus 2 to the n minus 1 all the way down to 2.
I'm just taking each of the terms and multiplying them by 2.
Now subtract this from that.
And then on the left side, you get a.
And on the right side, you get that term.
These all cancel out minus 1-- geometric series, cool.
So that sum is just 2 to the n minus 1.
And if I plug that back in there, ah, I've got my order of growth, exponential, 2 to the n.
OK, I was a math/physics undergrad.
I like these kinds of things.
But I wanted you to see how we can reason through it, because this is letting us see the growth.
What I want you to pull away from this is, notice the characteristic.
In Towers of Hanoi-- we're going to do another example in a second-- the characteristic was, at the recursive step, I had not one, but two recursive calls.
And that is characteristic of something with exponential growth, which I just saw here, 2 to the n.",I dont get this part either
"But in general, it's a good thing to be close.
And we talked last time about hill climbing and beam search, being close was the objective of those kinds of searches.
And at one point, in a beam search illustration, we had C, B, A, and D.
We had paths terminating at all four of those nodes as candidates for the next round of search.
And we decided on the basis of these airline distances to keep D and B, and reject A and C because they're further away as the crow flies.
Now, I repeat this even though many of you have had this fixed already in your tutorials because we're going to need this concept of heuristic distance today.","What is airline distance? Is it just synonym for Euclidean distance?
So when we use admissible heuristic algorithm, how do we know, that the path is actually the shortest? As was said in the previous lecture, heuristics usually work, but not necessarily always. We can think of a simple case when stepping away from the goal would be a better solution, even though the heuristic tells us otherwise (in that case path weights won't correspond to geometrical rules, but that's usually the case in real life problems). But if we stop when potential distances are higher when the one we found, it wouldn't be the optimal path. So this basically means that there are limitations on the heuristic we use, as it should guarantee that the ""airplane distance"" that we get is definitely lower than an actual distance
Wouldn't BFS give us shortest path as well?
i m not that confident in algorithms but dijkstra i think will calculate the shortest distance to any node while this algorithm wont go too far off the central nodes in the graph. when it finds one possible solution ""path"" it wont go over that distance in other directions right?"
"And I get the bound that I wanted, yeah? And so this is a slightly more formal little proof of exactly the same thing that's in the homework notes.
OK, so the one thing that's remaining is to actually show that our algorithm runs in a reasonable amount of time.
So I think they give us a budget of order E time.
But notice, that argument is precisely the argument that we just made right here, just minus the v factor.
And the v factor just came from looping over all the vertices in part a.
So now I think we're done with problem 1.
As usual, I've wasted too much time on the easy problem.
All right, any questions about this one? Excellent.","In problem 1, the condition for |V| and |E| in a connected graph seems to be incorrect: |V| <= |E|/2. Consider a graph with 3 vertices and 2 edges; it is connected but |V| <= |E|/2 does not hold. I think the condition should have been |V| <= |E|+1 (by induction; one edge connects two vertices and each next edge adds at most one vertex to the connected component). But as the next statement, |V|=O(|E|) still holds, it does not invalidate the rest of the proof."
"And in particular, we're going to use this notation when we talk about the value of a flow.
So the value of a flow, f, is denoted-- you can think of it as a cardinality of f.
And f is v, belonging to capital V, f(s,v).
And that is f(s,V), so what I have written here.
Well, given a flow network, I want one particular quantity that I want to maximize.
And that particular quantity is going to be defined, based on how much I can push from s, how much can I push outward from s.
That's the crucial quantity that I want to maximize.
That quantity is-- you think about everything that is going out of s, and you add it all up together.",How would you claim that using bottleneck value as the flow will always be correct ?
"And we don't count that.
We don't count the input layer.
Okay.
Yeah.
Right.
So this is our loss.
And the- in order to- to, to, uh, train our network, the- our goal is to do something like this.
For l in 1, 2, L, w_l equals- equals w_L minus.
So, um, because we are treating this as a loss, this should be the negative of the likelihood- of the log-likelihood, because we're thinking of this as a loss.
And because it's the loss, we're doing gradient descent.
Minus Alpha times partial derivative of L portion.
And similarly, b of L minus Alpha times partial of L over partial of.
Right.
So very similar to- to gradient ascent that we solve for logistic regression.
In logistic regression, we had just one set of theta vectors, and we would perform gradient ascent on that theta vector.
Here we have a collection of w and b matrices and vectors.
And we have L number of these matrices and vectors.
And what we wanna do is for each weight matrix and- and vec- uh, bias vector, take the gradient of the final loss, right, final loss all the way at the end, with respect to the corresponding, um, weight layer, uh, or weight matrix of that layer, and perform a gradient update like this.
And similarly to the, uh, uh, bias term as well.
Right.
So this is our goal.
And the way we go about calculating these gradients is using an algorithm called backpropagation.
Right.
So backpropagation helps us calculate these gradients, but once we calculate the gradients, we perform gradient descent on the last function.
Yes, question? Is it stochastic? So, uh, the question is, is this stochastic? In this case, it is stochastic because, uh, uh, we are- we have considered just one example.
All right.
So, yeah, good question.
So think of this as y_i, so y hat i, and, you know, y_i.
Yeah, so this is just the ith example.","Hello, In this lecture, you mentioned that a Neural network for classification with a sigmoid activation can be considered as some combination of logistic regressions (one for each neuron starting from the 1st hidden layer). Knowing that logistic regression's loss function is convex and NN's are not convex, can we minimize each logistic regression individually and use the optimal parameters obtained from each model to reach a global minimum for the Neural Network?"
"So what's the point of all this? It's not to learn about different kinds of drunks.
It's to show how, by visualization, we can get insight into our data that if I just printed spreadsheets showing you all of these endpoints, it would be hard to make sense of what was there.
So get accustomed to using plotting to help you understand data.
Now let's play a little bit more with the simulation.
We looked at different kinds of drunks.
Let's look at different kinds of fields.
So I want to look at a field with what we'll call a wormhole in it.
For those of you of a certain generation, you will recognize the Tardis.
So the idea here is that the field is such that as you wander around it, everything is normal.",What was the point of this whole lecture? is it that random walk is not so random?
"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
PROFESSOR: Greg was not here yesterday.
He was away.
And so this is work he's largely done on his own again.
He's done a molecular dynamics simulator.
So he's going to tell us a little bit about that, and then after that we'll do course evaluations, hand out some awards, have some cake, and then after that some pizza, and then play same PlayStation 3s.
All yours.
GREG PINTILIE: OK, so I'll talk about the make general molecular dynamics algorithm and then parallelization approaches.
So molecular dynamics is based on a potential energy function, which includes two terms, bonded terms and non-bonded terms.",Hi professor. What is the computational requirement to simulate a cell or even an organ down to the atomic level? How long will it take to render a few seconds animation on a modern super computer? And on a quantum computer? Thank you.
"All right.
And that's essentially the intuition behind node2vec is that you can- you can explore the network in different ways and you will get, um, better resolution, uh, you know, at more microscopic view versus more, uh, macroscopic view, uh, of the network.
So how are we going to now do this in practice? How are we going to define this random walk? We are going to do biased fixed-length random walks are that, that- so that a given node u generates its neighborhood, uh, N, uh, of u.
And we are going to have two hyperparameters.
We'll have the return parameter p, that will say how likely is the random walk maker step back, backtrack to this- to the previous node.","I didn't understand the part ""drawback of node2vec"", why we need to learn each node embedding individually ?
 I don't think that's entirely correct, as neighbouring nodes are positive samples, similarly to word2vec where context words are ""positive samples"". Negative sampling samples from the negative examples, in this case nodes that are NOT neighbors to the central node."
"So here are two functions that I wrote.
One is is_even_with_return.
That's its name, so pretty descriptive.
It's pretty much the same code we saw in the slides.
It just has this extra little print thing.
It gets the remainder when i is divided by 2.
And it returns whether the remainder is equal to 0.
So it'll either return a true or a false-- a Boolean.
OK so my function call is this: I'm saying is_even_with_return with a value 3.
When I make this function call, this 3 gets mapped into here-- this variable here-- so i is equal to 3.
I'm going to print with return, and then I'm going to say remainder is equal to 3 percent 2, which comes out to value 1, because there's a remainder 1.","when func_a( ) is mapped to z ... then can we write return z instead of return z( ) ?
why did it print with return and without return twice?
when you print(function_name) it just prints out what the function returns?"
"Um, so yeah.
You can apply these algorithms to anything.
The question is whether or not they're guaranteed to converge in the limit to the right value.
And they're not, if the world is not Markovian and they don't.
Like [LAUGHTER] we've seen in some of our work on intelligent tutoring systems, earlier on we were using some data, um, from a fractions tutor and we're applying Markovian techniques and they don't converge.
I mean, they converge to something that's just totally wrong and it doesn't matter how much data you have because you're- you're using methods that rely on assumption that is incorrect.
So, you need to be able to evaluate whether they're not Markovian or try to bound the bias or do something.
Um, otherwise your estimators of what the value is of a policy can just be wrong even in the limit of infinite data.
Um, what about converging to the true value in the limit? Let's assume we're in the Markovian case again.
So, for Markovian domains, does, um, DP converge to the true value in the limit? Yes.
What about Monte Carlo? Yes.
Yes.
What about TD? Yes.
Yes.
They certainly do.
The world is really Markovian, um, everything converges.
Asymptotically no under minor assumptions, all of these require minor assumptions.
Um, uh, so under minor assumptions it will converge to the true value of the limit, depends on, like, the alpha value.
Um, uh, what about being an unbiased estimate of the value, is Monte Carlo an unbiased estimator? Yes.
Yes.
Okay.
TD is not.
DP is a little bit weird.
It's a little bit not quite fair question there.","Hi, Can anyone please explain how the Monte Carlo method should be implemented in real world where we have no model of The professor explains we repeat an experiment over and over again and average over all the values. But in some cases it's not possible to gain in insight of the environment, suppose we are sending a rover to europa moon of the Jupiter. We would have no time to carry out experiments in such cases... Also let's assume we can carry out experiment. Suppose the experiment is living in this world and history repeats itself. However the conditions is changing all the time. How can we calculate the values in such cases."
"And actually, as n gets much, much larger this almost looks like a straight line.
It almost looks like a constant.
So log is almost just as good as constant.
What does exponential look like? It's the exact inverse of this thing.
It's almost an exact straight line going up.
So this is crap.
This is really good.
Almost anything in this region over here is better right.
At least I'm gaining something.
I'm able to not go up too high relative to my input size.
So quadratic-- I don't know-- is something like this, and n log n is something like this.
n log n, after a long time, really starts just looking linear with a constant multiplied in front of it.
OK, so these things good, that thing bad-- OK? That's what that's trying to convey.
All right, so how do we measure these things if I don't know what my fundamental operations are that my computer can use? So we need to define some kind of model of computation for what our computer is allowed to do in constant time, in a fixed amount of time.","How to understand this proof? What do I need to know (Im bad at math)
The definition of the problem is a mathematical relation, but it is not necessarily a function since the problem may have many correct outputs for a given input"
"So board 0 being i implies that you're taking the first column and you're iterating the positions of the queen in the rows of the first column, and so on and so forth.
And if you go ahead and run this, which we did last time you end up getting 92 different solutions to the eight queens problem, and as I mentioned, there's only 12 distinct ones.
So if you take rotation and reflection into account and each of these is a legitimate solution, that needs to be translated into the picture that you see here by essentially taking this data structure that you see.
7 becomes the queen in the left corner, et cetera.
So we absolutely wouldn't want to publish this code.","Back in last lecture, the 4 x 4 example, we were talking about we'd possibly have to place one of our previous queen in a different row to solve the problem. Here we are assuming that once a queen is placed in a column, we don't have to worry about it again. I understand that for each new queen we are placing in the new column, we only have to worry about it conflicting with the existing queen(s), but what if we weren't able to place a new queen in the n-th column without conflicting with some of the existing queen(s), and the possible solution could be achieved by placing one of the previous queen differently?"
"And the reason it's going to break apart is, when you multiply it apart like this, you're going to get products of mu i and mu j.
They will all go away.
So all you'll be left with is a bunch of the alpha i's and alpha j's that are multiplied in exactly the same way as the 1dks.
And you should check that.
And if you're rusty on this, this is from your linear algebra class.
And I'm super happy to write it out in more detail, if that confuses you.
But for now, hopefully, we can just move through this.
OK.
This thing here is the residual.
That says, how close am I? What's my distance from x to the subspace? And that's going to feature prominently in what we do next.
Please.
So this subspace is [INAUDIBLE] approximates this whole [INAUDIBLE]? So what we're going to try and do is we're going to try and find a low-dimensional subspace.
And I'll describe how we're going to do it next.
And PCA, when we're talking about, what's the principal component of variation? We're going to search across all the directions when we're looking for the principal component.
And we're going to say which one of them has this-- for example, there are two ways we can find PCA-- but has the smallest residual or will be equivalent, maximizes the amount of alpha, the amount we projected onto the subset.
OK? So there are two ways you can do this.
So we can find PCA by one, maximizing the projected amount.","if u_i and u_j are orthogonal, u_i.u_j = delta_ij only if delta_ij = 0"
"Similarly, the- the- the z's are basically the linear combinations of x's with w's plus b.
The same thing over here.
And let's call this b1 and b2- b2, and this is z1, z2, and a1 and a2.
So from z to a, we apply the g function, the nonlinearity, to take the linear combination.
We get a scalar, run it through the nonlinearity, we get another scalar, all right? And we- we can continue this.
So, similarly, we can have, uh, a z3 and an a3, in that has its own sets of weights, and its own bias, b3.
So you get z3 as the sum of over w1, x1, w2, x2, wd, xd plus b3.
And then apply the nonlinearity and you get a3, all right? And not only can we add multiple models like these.
We can then start nesting them.
In the sense, these three, the outputs of these three, that is uh, a1, a2, a3, can now become the input for another logistic regression.
It's gonna be a z and an a.
Similarly, a second logistic regression using the inputs as the outputs from the first layer, z and a, and so on.
We can nest these even further.
Yes, question? [inaudible] We'll come to that, we'll come to that.","Hello, In this lecture, you mentioned that a Neural network for classification with a sigmoid activation can be considered as some combination of logistic regressions (one for each neuron starting from the 1st hidden layer). Knowing that logistic regression's loss function is convex and NN's are not convex, can we minimize each logistic regression individually and use the optimal parameters obtained from each model to reach a global minimum for the Neural Network?"
"And we give you a problem on the path toward that to help prove a little bit about this.
This is a very typical kind of setup.
So again, I've got a bunch of trucking routes, each one of which has a max weight.
And I'm going to want to know, first of all, what's the maximum amount of weight I can ship? And then what is the minimum cost I can ship that weight? So in problem A, which is marked as useful digression, we're first going to prove kind of a handy inequality.",is he really solving the problems? or just writing some inequalities?
"The next time they come to the plate, the idiot announcer says, well he struck out six times in a row.
He's due for a hit this time, because he's usually a pretty good hitter.
Well that's nonsense.
It says, people somehow believe that if deviations from expected occur, they'll be evened out in the future.
And we'll see something similar to this that is true, but this is not true.
And there is a great story about it.
This is told in a book by Huff and Geis.
And this truly happened in Monte Carlo, with Roulette.
And you could either bet on black or red.
Black came up 26 times in a row.
Highly unlikely, right? 2 to the 26th is a giant number.
And what happened is, word got out on the casino floor that black had kept coming up way too often.
And people more or less panicked to rush to the table to bet on red, saying, well it can't keep coming up black.
Surely the next one will be red.
And as it happened when the casino totaled up its winnings, it was a record night for the casino.
Millions of francs got bet, because people were sure it would have to even out.
Well if we think about it, probability of 26 consecutive reds is that.","the wording of the last sentence was confusing and made it sound like the opposite of reality!  How it is written makes it sound that it's a 50% chance to get 26 consecutive reds, if the previous were 25 black...The correct statement is just to say, if you had 25 reds in a row, the 26th spin is still 50% to be red regardless of what happened previously as all spins are independent of one another (Gamblers Fallacy to think otherwise). Also how do you get 1/67,108,86*5* for a power of 2?
In the slide ""Gambler's Fallacy"" it reads at the bottom: ""Probability of 26 consecutive reds when the previous 25 rolls were red is:"" The wording is poor in my opinion. Does it mean: ""What is the probability of the next roll being red?"" Or Does it mean: ""What is the probability of the next 26 rolls being red?"" Or maybe : ""What is the probability of 26 consecutive reds occurring in the next roll if the previous 25 rolls were red?"" Based on his answer I think that the question should have read: ""What is the Probability of the next outcome being red when the last 25 outcomes where red?"" And then he goes on to talk about it being independent after this question. He didn't establish at the beginning that the outcomes were independent.
I think that explaining the gambler fallacy should take into account how the gambler thinks. The gambler thinks that one rare event has to compensate by another very rare event, counter to the one the gambler just experienced. In fact, the counter-event is as rare as the currently observed one, and is not likely to happen, well, because it is rare as well! What is likely to happen is a less rare event, rather than another extreme one. Would that be one of the ways to explain the gambler fallacy?
if the +10 will not even out, what would bring about they quasi 100% confidence in a fair game yielding 0 as it approaches infinity? How can the outcome approach expected outcomes(0,4%etc) over 1m tries? Remember, they each started with a huge variance, sometimes 44%. A rare event given enough trials ought to reoucurr with a fixed frequency, that's the premise of reversion to the means. Note, a rare event not repeating is itself rare, and getting rarer still with more tries. The set of events is distributionally connected, not individually, so, because heads/tails are =, you'd expect to see that over longer sequences approaching =. So, I don't subscribe to the ""black now has higher chances"", that's almost sure fire way to lose. Rather, keep betting what you were betting, i.e. random, and the irregularity ought to even itself out.
 You're viewing 1000 flips as being meaningless in respect to greater scheme, but what i'm saying about ""gambler's fallacy"" is exactly regarding what you said i quote: ""most basic tenets of probability that independent events have no effect on each other"" and this is my point exactly! If during 1000 spins you randomly get a lot more BLACKS then yes - you don't know when ""regression to mean"" will occur, BUT!! Probability of having a random sequence ""anomaly"" within 1000 spins is different than having 26 consecutive spins ""anomaly"" within those 1000, no?? I mean even if within those 1000 came more REDS, what is the probability that within those 1000 we'll get 26 consecutive BLACKS ?
Can someone explain about the regression to the mean? If the first 10 trials are all red, why the second 10 trials will be less extreme, i.e. fewer than 10? If according to the gambler's fallacy, the independent property, the second 10 trials should have a MEMORYLESS property, why the second 10 trials must have fewer red?
I still don't get how regression to means is consistent with the independence of events. Isn't the fact that the first 10 spins resulted in red (extreme) affect the next 10 spins (make it less likely to be as extreme)? Can someone pls explain that?"
"So look at all the data, that's your test cases.
Figure out a hypothesis.
Maybe say, oh, maybe I'm indexing from 1 instead of 0 in lists, for example.
Come up with an experiment that you can repeat.
And then pick a simple test case then you can test your hypothesis with.
So as you're debugging, you will encounter error messages.
And these error messages are actually pretty easy to figure out.
And they're really easy to fix in your code.
So for example, accessing things beyond the limits of the lists give you index errors.
Trying to convert, in this case, a list to an integer gives you type errors.
Accessing variables that you haven't created before gives you name errors.","even though the user provides a string , n = int(input(""How old are you? "")) this line converts n to an integer, even though n = twenty , which doesnt sount like an integer but when you ask type() it will return the answer ""integer"" so the error isnt a type error, sorry for the messy explaination even if you already googled and learned the answer , it might help the people who hasnt :D"
"I'm remembering that.
Now, I don't care about what's stored in x.next, because I've stored it locally.
That makes sense.
All right, so now I am free to relink that next pointer to my previous guy.
And now I can essentially shift my perspective over, so the thing that I'm going to relink now is the next one.
So x previous and x now equals x, x_next.
Does that make sense? Just relinked things over-- so that's the end of step 2.
Now, as I got down this at the end of this for loop, where is x? What is x_p, x, and x_next-- or x_n? Really, I'm only keeping track of x and x_p here.
So what are x_p and x at the end of this loop? I've done this n times.
I started with b at x.
So what is x? Yeah? So we have a vote that x is c.
STUDENT: [INAUDIBLE] JASON KU: So this is a little interesting.
All right.
I will tell you that c is either x_p, x, or x_n.
So we have one vote for x.
Who says something else? Eric doesn't like x.
There are only two other choices.","Great lecture, but i don't understand problem 3.. can anyone explain it to me"
"So this is, on the one hand, operation-- and when I do an insertion, I'm going to physically take some coins out of myself.
That will cost something in the sense that the amortized cost of insertion goes up in order to put those coins in the bank, but then I'll be able to use them for deletion.
So this is what insertion is going to do.
I can store credit in the bank, and then separately we allow an operation to take coins out of the bank, and you can pay for time using the credit that's been stored in the bank.
As long as the bank balance remains non-negative at all times, this will be good.
The bank balance is a sort of unused time.
We're paying for it to store things in there.
If we don't use it, well, we just have an upper bound on time.
As long as we go non-negative, then the summation will always be in the right direction.
This inequality will hold.","I got the idea of amortization in general, but these coins are totally weird. Why the heck do we charge back in time only once per insertion?
as long as the coin value is constant, O(1), it is fine, not necessary ""once"", could be 2,3,4... The point is that the cost of copying n elements charges 2 * n/2. we can save those n/2 coins with value 2 during each of the n/2 element's insertion. In contrast, we cannot save coins with value O(n), because that will make the insertion not O(1) anymore."
"Today we would think of a few microseconds, but those machines were slow.
Hence was born Monte Carlo simulation, and then they actually used it in the design of the hydrogen bomb.
So it turned out to be not just useful for cards.
So what is Monte Carlo simulation? It's a method of estimating the values of an unknown quantity using what is called inferential statistics.
And we've been using inferential statistics for the last several lectures.
The key concepts-- and I want to be careful about these things will be coming back to them-- are the population.
So think of the population as the universe of possible examples.
So in the case of solitaire, it's a universe of all possible games of solitaire that you could possibly play.","What's the main difference between the law of large numbers and the monte Carlo experiment? Monte Carlo is using the law of large numbers to run the experiment? Why do we need the monte Carlo experiment if we know the characteristic of the law of large numbers? Can anyone tell?
Thanks Professor. I have a doubt. With a mathematical model, I have estimated aircraft parameters using the extended Kalman filter. Now I have to verify whether the proposed system is robust against uncertainties. I have estimated data and true sensor data. How can I do Monte Carlo analysis with this information? I am not using any physical model of the aircraft but I am using Kinematic relationships of the aircraft i.e Kinematic model. How to perform the Monte Carlo analysis of my FDD algorithm using MS-excel or matlab? Could you please reply me out on this?
Hello sir, How can I handle this...describe the variance reduction techniques which are used as performance measures in stochastic models
Only one question how does the computer randonize numbers and is that consistantly random!!! Just asking and it may not matter that much with a large number of trials it would probable even out...
i have one question, for simulation we used different softwares, which method (discrete, continuous or Monte Carlo) is used in CST ( computer simulation technology)
Can anyone make me understand the programming language and concept used at 14.00
The computer program is no really random as the random samples are generated using psuedo-random generators.
but you didnt define combinatorics for us lol
What is the purpose of the MC simulation? Where is the advantage if a user implements it? Whatever advantage it serves for the user (gambler), isn't there any equal advantage to the house?
Any one figured out it had a little to do with Monte Carlo, but with fundamentals of probability?
Hello, thank you for your lecture on Monte Carlo. I have a question, I dont understand why it is better to run the simulation rather than just use the mean? For the roulette example, were providing the code the 1/36 probability and using a simulation to show that well win 1/36 times. But of course we know that already, and so why run the simulation? My question related to why Im researching Monte Carlo. I have a real world problem where I am trying to forecast unplanned shutdowns for machinery. I have a detailed history of unplanned shutdowns for the past 10 years. Say I have 1005 machines and I have on average 7 breakdowns per month. Can I use Monte Carlo to forecast how many machines will breakdown per month going forward? Or should I just assume Ill have 7 breakdowns per month? What would be the benefit of each? Im happy to self learn, Im hoping someone can point me in the right direction. Thank yoy
How is this related to monte carlo tree search?
does anyone know about a video about methods to calculate integral with method monte carlo when we have unusual distribitution and one side of our function has infinite vallue over y axis?"
"So that's it for algorithms today, but let me quickly tell you about some applications.
You've probably taken other classes that use applications of fast Fourier transform, so I will just summarize.
If you've ever edited audio, you probably did it in-- unless you're just pasting audio clips together-- you probably did it in what's called frequency space.
So you know that-- as I talked about in the beginning-- when we're measuring where the membrane on this microphone goes over time, that is in the time domain for every time we sample where, physically, this thing is.
If you apply-- I think the way I defined it here is the inverse fast Fourier transform, usually it's called Fourier transform-- to that time domain vector, you get a new vector.
Now it's a complex vector.
You may have started with real numbers.
You get complex numbers.
So for every position in the vector-- what it corresponds to-- the x-axis is no longer time.","How does one perform FFT on a larger domain consisting of multiple cosets of a multiplicative subgroup of the field? I've heard it can be done but couldn't find any sources that explained how.
So what is the math doing in practical terms? If I understand correctly, it's using the behavior of a signal over time to determine specific properties of that signal at specific moments. Is that correct?
Does anyone have intuition as to why Fourier transforms pop up here?"
"And notice it's only looking at this value of n because that's the frame in which I'm in.
It never sees that value of n.
OK, aren't you glad I didn't do fact of 400? We've only got two more to go, but you get the idea.
Same thing, I need to get fact of 2 is going to call fact again with n bound to 2.
Relative that evaluates the body and is not yet equal to 1.
That says I'm going to the else clause and return 2 times fact of 1.
I call fact again, now with n bound to 1, and, fortunately, now that clause is true, and it says return 1.","During the mathematical induction step he explained, he said he wants to show that ""this is equal to that"". I believe the ""that"" it the k+1 * k+2 ,,,"" but what is the ""this""?
Ken MacDonald Yeah I know but as n goes up, there will be hundreds of steps. So do they recurse on the other because the same rules still applied?
Shouldn't there be 2 cases for the factorial function? if n==1 or n==0: return 1"
"And that's it.
So this is every DP algorithm is going to have that structure.
And it's just using recursion and memoization together.
OK, so now let's apply that technique to think about the DAG shortest paths problem.
The problem was, I give you a DAG.
I give you a source vertex, S-- single source shortest paths.
Compute the shortest path weight from S to every vertex.
That's the goal of the problem.
And we saw a way to solve that, which is DAG relaxation.
I'm going to show you a different way, which turns out to be basically the same, but upside down, or flipped left right, depending which way you direct your edges.
So what are our sub-problems? Well, here, actually, they're kind of spelled out for us.
We want to compute delta and SV for all these.","Both Fibonacci and single-source shortest path in a DAG can be solved following the topological order, without recursion nor memoization. It makes me wonder if every problem solved with DP could be solved without recursion nor memoization, exploiting the topological order of its sub-problems."
"So this would mean, for example, if this is a protein, protein interaction network uh, proteins have different um, chemical structure, have different chemical properties and we can think of these as attributes attached to the nodes uh, of the network.
At the same time, what we also wanna do is we wanna be able to create additional features that will describe how um, this particular node is uh, positioned in the rest of the network, and what is its local network structure.
And these additional features that describe the topology of the network of the graph uh, will allow us to make more accurate predictions.
So this means that we will always be thinking about two types of uh, features, structural features, as well as features describing the attributes and properties uh, of the nodes.","In the previous video, while describing Graph Neural Net, mentioned there is no feature enginering, here you are telling about featues. It is confusing."
"All right.
Make sense? Any questions about this story? All right.
So I guess in the rest of the lecture, I want to go through a bunch of different examples of how security goes wrong.
So, so far, we've seen how you can think of it.
But inevitably, it's useful to see examples of what not to do so that you can have a better mindset when you're approaching security problems.
And in this sort of breakdown of a security system, pretty much every one of these three things goes wrong.
In practice, people get the policy wrong, people get the threat model wrong, and people get the mechanism wrong.","Can you provide for reference sources citing the three folded security approach (policy, tm and mechanisms)? Regarding the threat model, is there a generally accepted methodology you could mention (preferably free from product bias) specifically advised for system protection endevors ?"
"AUDIENCE: Oh, that's the parent frames.
PROFESSOR: Yeah, this is the parent frame.
That's right.
This is the child frame.
So visually speaking, the user just sees this.
But using the miracle of my da Vinci style drawing techniques, this is actually overlaid atop this transparently.
So that's the child frame.
That's the parent frame.
OK so, there's a couple different solutions-- you can imagine-- for solving this.
The first solution is to use a frame busting code.
So you can actually use JavaScript expressions to figure out if you have been put into a frame by someone else.",can someone explain to me where the script is being put?
"That is to say if we increase the number of layers to so-called depth, then we're going to increase the amount of computation necessary in a linear way, because the computation we need in any column is going to be fixed.
What about how it goes with respect to the width? Well, with respect to the width, any neuron here can be connected to any neuron in the next row.
So the amount of work we're going to have to do will be proportional to the number of connections.
So with respect to width, it's going to be w-squared.
But the fact is that in the end, this stuff is readily computed.
And this, phenomenally enough, was overlooked for 25 years.
So what is it in the end? In the end, it's an extremely simple idea.
All great ideas are simple.
How come there aren't more of them? Well, because frequently, that simplicity involves finding a couple of tricks and making a couple of observations.","Is Conway's Game of Life hard to do with neural nets?
I don't quite get the last point: the computation with respect to width is w^2 (width squared).Can someone explain?"
"And so I'll have to explain exactly what this means.
And then we'll take a look at how we could prove something like this.
So what's cool about this is that you're saying that it's the value of any flow is bounded by the capacity of any cut, OK? And so that's an upper-bound on the maximum flow value, right? So I'm saying there's all these cuts that are possible in the network.
And I'm making a statement about what the maximum flow can be, based on the values corresponding to the capacities of any cut, right? So why is that the case? Well, we're not going to be able to prove that fully today.
That's the max-flow min-cut theorem.
But you can certainly get a sense of it, by looking at a different characterization of the flow value.",How would you claim that using bottleneck value as the flow will always be correct ?
"Who wants to make a guess? By the way, this is the question.
So please go answer the questions.
We'll see how people do.
But we can think about it as well together.
Well, let's see where the time goes.
The first thing is at the sort.
So I'm going to sort all the items.
And we heard from Professor Grimson how long the sort takes.
See who remembers.
Python uses something called timsort, which is a variant of something called quicksort, which has the same worst-case complexity as merge sort.
And so we know that is n log n where n in this case would be the len of items.
So we know we have that.
Then we have a loop.
How many times do we go through this loop? Well, we go through the loop n times, once for each item because we do end up looking at every item.","Timsort is a variant of Quick Sort? AND QS has worst case complexity similar to merge sort?? I guess I don't understand Computational Complexity that well :(
Where did the I[i] come from? Shouldn't it be L[i]?"
"So Python associates names and entities in a very simple, straightforward fashion.
And if you know the ground rules, it makes it very easy to deal with.
And if you don't know the ground rules, it makes it very hard to deal with.
So what's the ground rules? Here's the gory details.
So Python associates names with values in what Python calls a binding environment.
An environment is just a list that associates a name and an entity.
So if you were to type b equals 3 what Python is actually doing is it's building this environment.
When you type b equals 3, it adds to the environment a name, b, and associates that name with the integer, 3.","I might be late but the last example... y[-1][1], and it gives 8. Shouldnt it be 7? Because if -1 represents the last list within that list, and youre asking for the 1st element (or number) within THAT list, you would see 7. So what? Also, why is -1 used to represent that element. Shouldnt it be 3 since its the 3rd list within that list
can someone explain why another environment can be created, why not just keep one?
Why did he make another environment E2 x=5, when (a+3) =5 (a=2) . Wouldn't it just be simple to evaluate that expression in it's current environment?"
"You're so- maybe overfitting.
Um, if Lambda is way too big, then you're forcing all the parameters to be too close to 0.
Um, in fact actually, if you think about it, if Lambda was equal to 10 to the 100 or some ridiculously large number, then you're really forcing all the Thetas to be 0, right? If all the Thetas is 0, then you know then you're kinda fitting the straight line, right? So that's if Lambda equals, uh, 10 to the 100.
And so- and this is a very simple function which is the function 0, right? And, and this function h of Theta, x equals 0, right, approximately 0.
It is a very simple function which you get if you set Lambda very large.
And by doubling Lambda between, you know, a far too large value like 10 to the 100 compared to a far too small value like Lambda 0, you, you, you smooth the interpolate between this much too simple function of h equals 0 and a much too complex function, okay? Um, so there is, um- so that's pretty, uh, it, it, it, um, it- so that's pretty much it for regularization in terms of what you need to implement but you feel like your learning algorithm may be overfitting, um, add this to your model and solve this optimization problem, um, and it will help relieve overfitting.
Um, more generally, if you are, um, let's see.
More generally if you have a, uh, say logistic regression problem where this is your cost function.","In the part where he discusses regularization, isn't that a linear function theta^T x^(i) used there? In which case, wouldn't the curve be a line?"
"Another set of models for generating graphs, came mostly from the statistics and the social networking literature where basically, the idea was that there is maybe some, that there might be some latent social groups and based on those latent social groups, edges of the social network get created.
And then the question is, how can you take that model, fit it to the data and perhaps discover the groups? However, today and in this lecture, we are going to use deep learning and representation learning to learn how to generate the graphs.
So in contrast to prior work in some sense that either assumed some mechanistic generative process or assumed some statistical model that was motivated by the-- let's say social science, here we want to be kind of agnostic in this respect.
And the goal will be that, can we basically given a graph or given a couple of graphs, can we learn how to gene-- what the properties of those graphs are, and how can we generate more instances of those types of graphs? So we'll be kind of completely general, we'll just learn from the data in this kind of representation learning framework.","Thanks a lot for the lecture! Is it possible to generate a very regular topologically weighted graph? I mean the mesh, for example polygonal geometry."
"And so that is the right part of this table.
So of course, these array data structures are not efficient.
They take linear time for some of the operations.
So the sorting algorithms are not efficient.
But they're ones we've seen before, so it's neat to see how they fit in here.
They had the-- selection sort and insertion sort had the advantage that they were in place.
You just needed a constant number of pointers or indices beyond the array itself.
So they're very space efficient.
So that was a plus for them.
But they take n squared time, so you should never use them, except for n, at most, 100 or something.","Is my understanding correct of why we prefer to use an array over the pointers approach? Since our implementation of insert, delete (and by extension, build) all comes down to insert_last or delete_last (with a bunch of value-swapping), each of which an array can support in constant time, it makes the pointer implementation less desirable because at each node we would need to store both a value and a pointer, which would effectively make our memory allocation double that of an array. (are there any other benefits that I missed?)"
"And it allows you to go through Python, paste a code, go through it step-by-step.
Like with each iteration, it'll show you exactly what values each variable has, what scope you're in, when scopes get created, when scopes get destroyed, variables within each scope.
So pretty much every single detail you need to sort of understand functions.
As we're starting to-- you can see we've had couple questions, and these were great questions.
So if you're still trying to understand what's going on, I would highly suggest you take a piece of code and just run it in the Python Tutor and you should be able to see exactly what happens, in sort of a similar way that I've drawn my diagrams.
In all of the codes for this particular lecture, I've put links to the Python Tutor for each one of those exercises.","im 12 and i dont understand what is func, help..."
"Yeah, cheetahs are short, zebras are medium height, and giraffes are tall.
But our output is just pretty much 0.5 for all of them, for all of those shadow heights, all right, [? with ?] no training so far.
So let's run this thing.
We're just using simple [? backdrop, ?] just like on our world's simplest neural net.
And it's interesting to see what happens.
You see all those values changing? Now, I need to mention that when you see a green connection, that means it's a positive weight, and the density of the green indicates how positive it is.
And the red ones are negative weights, and the intensity of the red indicates how red it is.
So here you can see that we still have from our random inputs a variety of red and green values.
We haven't really done much training, so everything correctly looks pretty much random.
So let's run this thing.
And after only 1,000 iterations going through these examples and trying to make the output the same as the input, we reached a point where the error rate has dropped.
In fact, it's dropped so much it's interesting to relook at the test cases.
So here's a test case where we have a cheetah.
And now the output value is, in fact, very close to the desired value in all the output neurons.
So if we look at another one, once again, there's a correspondence in the right two columns.
And if we look at the final one, yeah, there's a correspondence in the right two columns.
Now, you back up from this and say, well, what's going on here? It turns out that you're not training this thing to classify animals.
You're training it to understand the nature of the things that it sees in the environment because all it sees is the height of a shadow.
It doesn't know anything about the classifications you're going to try to get out of that.
All it sees is that there's a kind of consistency in the kind of data that it sees on the input values.","How did the professor trained the Neural Net...any idea??
When it guesses the wrong thing (school bus on black and yellow stripes) isn't the ""real problem"" there that it doesn't have enough data or good enough data?"
"But today uh, linear algebra, you know, packages have gotten good enough that when you invert the matrix you just invert the matrix.
You don't have to worry too much- when you're solving you don't have to worry too much about it.
So in the early days of SVM solving this problem was really hard.
You had to worry if your optimization packages were optimizing it.
But I think today there are very good numerical optimization packages.
They just solve this problem for you and you can just code without worrying about the- the details that much.
All right.
So this L1 norm soft margin SVM and, uh, oh and so, um, and so this parameter C is something you need to choose.
We'll talk on Wednesday about how to choose this parameter.
But it trades off um- how much you want to insist on getting the training examples right versus you know, saying it's okay if you label a few terms out of this one.
[NOISE] We'll- we'll discuss on Wednesday when we discuss bias and variance.
How they choose a parameter like c.
All right.
So the last thing I want to- last thing I'd like you to see today is uh just a few examples of um, SVM kernels.
Uh, let me just give um- all right.
So, uh, it turns out the SVM with the polynomial kernel, uh, works quite well.","What is that that weird symbol he wrote during L1 soft margin svm? He called it c-i , i guess"
"To what the program expected.
So all of these errors that I've talked about in the previous slides are actually examples of exceptions.
And there are actually many other types of exceptions, which you'll see as you go on in this course and also in 60002.
So how do we deal with these exceptions? In Python, you can actually have handlers for exceptions.
So if you know that a piece of code might give you an error.
For example, here I'm dealing with inputs from users.
And users are really unpredictable.
You tell them to give you a number, they might give you their name.
Nothing you can do about that.
Or is there? Yes there is.
So in your program you can actually put any lines of code that you think might be problematic, that might give you an error an exception, in this try block.","I am surprised that no one in the class asked why does it raise ValueError instead of TypeError, as the user provides the wrong Type of input i.e string Type whereas the expected input was integer Type. I had to google to find the answer. May be they already know the answer ?
even though the user provides a string , n = int(input(""How old are you? "")) this line converts n to an integer, even though n = twenty , which doesnt sount like an integer but when you ask type() it will return the answer ""integer"" so the error isnt a type error, sorry for the messy explaination even if you already googled and learned the answer , it might help the people who hasnt :D"
"And now I notice that P of B and P of E don't depend on A which means that I can pull this out and push the summation in.
That's just, uh, algebraic manipulation.
And then what is this value? This value is just 1 because of the previous slide.
So I can just drop it.
And now I have p of b times p of e.
And lo and behold what is- what is this? This is if you had just gone and defined a sub- miniature of Bayesian network over B and E.
This was exactly what you've written down.
Okay, so that's kind of cool.
So the general idea here is that when you're marginalizing out, uh, a leaf node that yields a Bayesian network just without that node.
So marginalization produces this, um, this Bayesian network where you've just erased, um, the very- the leaf node along with its incoming edges.
All right, so in other words, I've turned basically what was, would have been a algebraic operation into a graphical one.
And generally those are good moves because it's much easier to kinda think graphically and, uh, make large operations then go through tons of algebra.
Yeah.
Definition equals, it seems like it's from like the probability [inaudible].
Yeah, so the question is what about this first definition equals? What I mean here is by the laws of probability.
Um, so it's not technically a definition, it follows from the axioms of probability.
Yeah, thanks.","In 25.54 we are told that B and E are independent. But shouldn't we have specified that B and E are independent given A, or am I missing some major point? Because since we aggregate probabilities against hidden variables so this shrinks the expressivenes of our knowledge."
"The closer you are to the top of this list, the better off you are.
If you have a solution that's down here, bring a sleeping bag and some coffee.
You're going to be there for a while.
Right? You really want to try and avoid that if you can.
So now what we want to do, both for the rest of today in the last 15 minutes and then next week, is start identifying common algorithms and what is their complexity.
As I said to you way back at the beginning of this lecture, which I'm sure you remember, it's not just to be able to identify the complexity.
I want you to see how choices algorithm design are going to lead to particular kinds of consequences in terms of what this is going to cost you.
That's your goal here.
All right.
We've already seen some examples.
I'm going to do one more here.
But simple iterative loop algorithms are typically linear.
Here's another version of searching.
Imagine I'll have an unsorted list.
Arbitrary order.
Here's another way of doing the linear search.
Looks a little bit faster.
I'm going to set a flag initially to false.","That unsorted list, I didn't understand how is the complexity (1+4n+1) ?
This is cool. But i am after how can i improve my complexity? I can determine the complexity but how can I overcome this?"
"So do you think that Python implements permutation sort? I certainly hope not.
Yes? AUDIENCE: [INAUDIBLE] JUSTIN: Right.
So the question was, why is it omega and not big O? Which is a fabulous question in this course.
So here's the basic issue.
I haven't given you an algorithm for how to compute the set of permutations for a list of numbers.
I just called some magic function that I made up.
But I know that that algorithm takes at least n factorial time in some sense.
Or if nothing else, the list of permutations is n factorial big because that's all the stuff has to compute.",You clearly don't understand what the Omega notation is in Algorithms.
"So y of minus 1 is x of minus 1 minus x of minus 2.
Since both of those are 0, it says that the output at time minus 1 is 0.
Trivial, right? Trivial.
And similarly, we can just iterate through the solution to the whole signal.
So y of 0 is x of 0 minus x of minus 1.
x of 0 is that special one, that is 1.
So now we get 1 minus 0, which is 1.
y of 1 is x of 1 minus x of 0.
Now the special one is on the other side of the minus sign, so the answer is minus 1.
y of 2 is x of 2 minus x of 1 -- they're both 0.
And in fact, all the answers from now on are going to be 0.
So what I just did is a trivial example of -- I use a difference equation to represent a system, and I figured out the output signal from the input signal.
That's the method that we call-- that's the representation for discrete time systems that we refer to as difference equations.","The labels on the x axis are not the value of the x's but the time steps. The definition of the delta signal is that x[t]=0 for every t different from 0 so x[2]=0. The term x axis is confusing here, it would better be called time axis or just horizontal axis."
"So you could still set that constant large enough so that this part, which is multiplied by c, would annihilate this part, which would have a big O.
I guess I'll write it in just for kicks so you've seen both versions.
This would be minus c see times t plus 1 times c.
So that would still work out.
If you set c to the right value, you will still get 2.
So binary counters, constant amortize operation.
So I think this is very clean, much easier than analyzing the fractal of the costs.
Now, binary counter with increment and decrements, that doesn't work.","I was wondering about that. In his analysis table doubling would have increased the runtime complexity, this makes more sense."
"Something that grows linearly is not bad.
Something that grows, as we've seen down here, exponentially tends to say, this is going to be painful.
And in fact, you can see that graphically.
I'll just remind you here.
Something that's constant says, if I draw out the amount of time it takes as a function of the size of the input, it doesn't change.
Logarithmic glows-- gah, sorry-- grows very slowly.
Linear will grow, obviously, in a linear way.
And I actually misspoke last time.
I know it's rare for a professor to ever admit they misspeak, but I did.
Because I said linear is, if you double the size of the input, it's going to double the amount of time it takes.
Actually, that's an incorrect statement.
Really, what I should have said was, the increment-- if I go from, say, 10 to 100, the increase in time-- is going to be the same as the increment if I go from 100 to 1,000.
Might be more than double depending on what the constant is.
But that growth is linear.
If you want to think of it, take the derivative of time with respect to input size.
It's just going to be a constant.
It's not going to change within.
And of course, when we get down to here, things like exponential, it grows really fast.
And just as a last recap, again, I want to be towards the top of that.
There was my little chart just showing you things that grow constant, log, linear, log-linear, quadratic, and exponential.
If I go from n of 10, to 100, to 1,000, to a million, you see why I want to be at the top of that chart.
Something up here that grows logarithmically, the amount of time grows very slowly as I increase the input.
Down here, well, like it says, good luck.
It's going to grow up really quickly as I move up in that scale.","Could you not argue that the intToStr() example is linear with respect to the number of digits in the input number? How do we decide what the input space is when it's ambiguous?
Let's say the time required by a program for input of size n is n+100. If you now go from 10 to 20, for 10, it takes 110 secs. But for 20, it takes 120 secs. For 30, it takes 130 secs. But difference between times for 10 and 20 and between 20 and 30 is same (10). Observe that from 10 to 20, time taken doesn't double. But the difference remains same
Sure. But if you read the original comment, the question was what the professor meant by his statements. That was what I clarified. As you said, as the inputs get bigger, the difference becomes ""negligible"", but not necessarily 0. Hence, it is not going to be exactly double but approximately. That was the whole point of the quotes in the original comment."
"The bank account would accumulate transactions.
So if the input to the accumulator is 7, then the state gets incremented by 7, and the output is the value of the new state.
So that's what this says, the starting state is always 0, the getNextValues always returns for this new state-- state plus input, and for the current output, the same thing as the current state.
Everybody's clear? So the question is, what would happen if I did this sequence of operations? Let's say that I have the state machine class, I have the accumulator subclass.
I make an instance A, I do some stuff to A, I make an instance B.
I do some stuff to B, I do some stuff to A, and I type this.
What gets printed? Take 30 seconds, talk to your neighbor, figure out what the answer is.
[AUDIENCE DISCUSSION] PROFESSOR: OK.
What will get printed by the print statement? Answer (1), (2), (3), (4), or (5).
Everybody raise your hand with an answer.
Excellent, excellent, almost 100%.
I don't think I see a single wrong answer.
So everybody seems to be saying (2).","Why did the accumulator return two values? Did it start with 8 and step 13 then start with 13 and step with 8?
the scope of the variable ie global local static etc"
"Okay? It's- it's a very different way of- of- of looking at- at, um, of- of- of plotting things, right? The- the value of J Theta is not visible here, and it is- it's kind of implicit in the shape of the contours that we draw, right? Now, uh, a few things that you can observe is that, uh, what can we say about the shape of J theta here? Okay.
Is it like a boat shape? Yeah, it's like a boat shape where it is minimized at this value.
And as you- as you kind of move farther away in the parameter space from this value, you are- the cost function evaluates to larger values, right? Is this clear? Yes, question.","Hello Professor Avati, Hope you are doing well! I have a small question While equating the determinant of J(teta) to zero, we assume that the value of 'teta' at which the equation is zero is the minima. But we haven't deciphered the shape of the function. This value could easily be maxima if the function had an inverse bowl shape."
"We're going to use a search tree to keep track of where we've been and how we got there.
Search tree is going to be comprised of nodes.
It's otherwise going to look like a directed, acyclic graph.
And it's going to have a lot of similarity to any particular state transition diagram that we end up searching.
But it's going to have nodes instead of states.
Nodes are different.
Nodes represent both the state that you've visited as a consequence of expanding its parent node, the parent node, or the place that you came from as a consequence of getting to that node, and the transition that you made in order to get there, or the action that happened that got you from the parent node to the child.
Keeping track of a list of nodes is known as a path, or it specifies where you've been and how you got there.
And if you're at a given node, you can actually use the reference to the parent node and the action to trace back from whatever node you're at currently to its parent node, to its parent node, to its parent node, and then finally get back to the start state.","For the DFS, before finding the path ACDE, does it should visit ACDA first because you don't have a visited node constraint here?"
"Without memoization, this is not true, Fibonacci to exponential time.
But if we add memoization, we know that we only solve each sub-problem once.
And so we just need to see, for each one, how much did it cost me to compute it, assuming all the recursion work is free, because that's already taken into account by the summation.
So in particular, this summation is at most the number of sub-problems times the time per sub-problem, which in this case was order n.
We could try to apply that analysis to merge sort, because after all, this is also a recursive algorithm.
It happens to not benefit from memoization.
But we could throw in memoization.
It wouldn't hurt us.
But if you think about the call graph here, which is like s of 0 m, which calls s of m-- 0 n over 2 and o of n over 2n and so on.
It has the same picture, but there's actually no common substructure here.
You'll never see a repeated sub-problem, because this range is completely disjoined from this range.","Thanks. Do parallelism of recursion hurts the idea of memorization, imagine in your recursion tree there is F(n-1) in two branches ,since they are running in parallel no one can use the value of the other since when the programme arrive to F(n-1) in the two branches no one of them is done ! Is there any explanation to that please
Both Fibonacci and single-source shortest path in a DAG can be solved following the topological order, without recursion nor memoization. It makes me wonder if every problem solved with DP could be solved without recursion nor memoization, exploiting the topological order of its sub-problems.
Why does O(ceil(n/w)*n) = O(n + n^2/w)? Where does the addition with n come into play, shouldn't it just be O(n^2/w)? I guess it doesn't matter since n^2/w dominates anyways (assuming w is a const or grows slower than n)"
"So we have to talk about two things and put up our recurrence together.
The two things we have to talk about are what you do when you move, and that's actually fairly easy, and the second thing is what the model of the opponent looks like when you're waiting for him or her to move.
So let's take a look at your move.
And I've got V1.
Let's look at Vi.
Vj here, dot dot dot, Vn.
So that's what you see.
And you're looking at i and j.
And at this point, you're seeing all those other coins, the outer coins have disappeared into people's pockets.
And you're looking at this interval.
So I'm going to write out what Vij should be.
And keep in mind that we want to maximize the amount of money.","can anyone help me with the complexity of coin game ques how is no of sub problems n square?
I am not sure what you mean by sum(i,j) here. It looks to me as if the point that your and Praveen's point can be made more clearly by omitting the references to sum, like so (using C ""?"" syntax): V(i,j) = (i == j) ? v[i] : max(v[i]-V(i+1,j), v[j]-V(i,j-1))."
"So maybe keyfunction will just return the value or maybe it will return the weight or maybe it will return some function of the density.
But the idea here is I want to use one greedy algorithm independently of my definition of best.
So I use keyfunction to define what I mean by best.
So I'm going to come in.
I'm going to sort it from best to worst.
And then for i in range len of items sub copy-- I'm being good.
I've copied it.
That's why you sorted rather than sort.
I don't want to have a side effect in the parameter.
In general, it's not good hygiene to do that.
And so for-- I'll go through it in order from best to worst.","Timsort is a variant of Quick Sort? AND QS has worst case complexity similar to merge sort?? I guess I don't understand Computational Complexity that well :(
One more question: Why is density function returns self.getValue() / self.getCost()
Are the numbers inside the 'values' array randomly picked by the instructor or the does it act as a grading scale for each menu item?
Thank you! I was confused that he was describing a local optimum with those examples because the metrics he is using are qualitatively different, ie. it might be more desirable to me to have slightly less overall calories but me maximising on ""value"" (how much I like the food) rather than cost. What seems significant for determining the optimum is the order of the elements, and the metric (or the key function) determines the order. So then the global optimum is the solution with biggest total across all orderings.
Where did the I[i] come from? Shouldn't it be L[i]?"
"And yeah, it turns out it doesn't really matter, which, um, which way you put it.
It just- basically, you're trying to either minimize the loss of the children or maximize the gain in information, basically.
[inaudible].
Yeah.
Let's see.
Yeah, you're right.
That should actually be a max.
Let me fix that really quick.
Because you start with your parent loss, and then you're subtracting out your children's loss, and so the amount left, let's see, the higher this loss is- yeah.
So you really want to maximize this guy.
Makes sense, everyone? Thanks for that.
Okay, so I've sort of given this like, hand-wavy- Oh, sure, what's up? [inaudible].
So that would be log-based.
The question is, for the cross-entropy loss, is it log base 2 or log base c? It's log base 2.
Okay, here, I can write that out.
Yep.
[inaudible].
Oh, sorry, I didn't quite hear that.
[inaudible].
Okay.
Um, so the question is can- uh, what is the proportion that are correct versus incorrect for these two examples we've worked through here? Um, and so, yeah- basically, what we're starting with is, we're starting with we have 900/100, 900 positives and 100 negatives.
All right, so you can imagine that if you just stopped at this point, right, you would just cla- classify everything as positive, right, and so you get 100 negatives incorrect.",In cross entropy what if phat goes to 0. It also decreases the loss?
"I suppose there should be one last line here where everything is green and we're happy.
But in some sense, we're pretty sure that an array of one item is in sorted order.
And so essentially, from a high level, what does selection sort do? Well, it just kept choosing the element which was the biggest and swapping it into the back and then iterating.
Now, in 6.006, we're going to write selection sort in a way that you might not be familiar with.
In some sense, this is not so hard to implement with two for loops.
I think you guys could all do this at home.
In fact, you may have already.
But in this class, because we're concerned with proving correctness, proving efficiency, all that good stuff, we're going to write it in kind of a funny way, which is recursive.",why are we doing comparison/merges in reverse order for selection and merge sort? what's the point?
"You got an example of a Las Vegas algorithm.
And tomorrow in section, you'll see a slightly more involved analysis for something that looks a lot closer to the randomized quicksort.
So see you next time.
","Hi Guys, How different is this course comparing to the Introduction to Algorithms class taught by him?"
"The trouble is, gee, there are only three variables there.
And when there are lots of variables it gets pretty hard to make up those numbers or to even collect them.
So we're driven to an alternative.
And we got to that alternative just at the end of the show a week ago.
And we got to the point where we were defining these inference nets, sometimes called ""Bayes nets."" And the one we worked with looked like this.
There's a burglar, a raccoon, the possibility of a dog barking, the police being called, and a trash can being overturned.
So more variables than that.
That only has three.
This has got five.
But we're able to do some magic with this because we, as humans, when we define-- when we draw this graph we're making an assertion about how things depend or don't depend on one another.
In particular, there's something to break down and memorize to the point where it rolls off your tongue.
And that is that any variable on this graph is said by me to be independent of any other non-descendant given its parents.
Independent of any non-descendant given its parents.
So that means that the probability of the dog barking, given its parents, doesn't depend on T, the trash can being overturned.","Wait, arent R depended on B based on Markov blanket ?? 9 https://en.wikipedia.org/wiki/Markov_blanket
First, it is not clear about ""you omitted the R variable from P(B|T,R)"". There is no omission of R variable in that expression. Only small ""mistake"" was in the expression, he forgot adding ""|"", i.e., P(T,R) should be P(T|R). Re. your next question.. not sure how I can help.. but let me try.. You seem to think: ""if burglar is true and this caused the dog barking"", then this excludes the case ""if racoon is true, then this causes the dog barking"" Please note that all possible combinations could cause dog barking, i.e., (burglar, racoon): (T T), (T F), (F T), even (F F) could cause the dog barking. So, the fundamental starting point you may want to revisit is the beginning part of Lecture 21, i.e., joint prob. table can answer all things we wanted to know. In another words, there is no ""explaining away"" principle from the beginning.. it is all about observations and measurements for all combinations and simply ""tally"" and ""count"" in the table. Then, based on this table we can know how likely or unlikely the event (which we are interested in) could happen... So, question about ""how is this relationship between B and R modeled?"" ... they are already incorporated into the joint. prob. table. The downside of directly building joint prob. table is cumbersome and taking too much energy/time.. so.. using belief net (dependence knowledge) w/ much smaller number of effort of ""tallying"" or ""measurement"" will/can produce the exact same joint prob. table which again can answer whatever we wanted to know.. like miracle/magic. So, understanding why joint. prob. table can provide all info we want to know... this ""basic"" and ""fundamental"" concept will help you understand what is going on the rest of his lecture and may lead to answer your question yourself."
"We extended paths that went through A more than once, like so.
Would it ever make sense to extend this path? No because we've already extended a path that got there with less distance.
Will it ever make sense to extend this path? No because we've already extended another path that gets to be by a shorter distance.
So if we keep an extended list, we can add that to branch inbound to our advantage.
So let's see how that would work on the classroom example.
And then we'll do Cambridge.
So this is bridge inbound, plus an extended list.
And I do mean extended.
Not in the N queued list.
N queued list won't work here.
So let's see, I start off the same way as I did before.
S goes to either A or B.
That's a length of 3.
That's a length of 5.
So I extend A.
That goes to either B or D.
But B is as if it wasn't there at all.
Oh, sorry.
Hang on.
B goes there.
And those path lengths are 7 and 6.
And now I look around on the board, and I say what is the shortest path so far? And it's B.
So I extend that to get to A and C with path lengths of 9 and 9.","What is the difference between enqueued list and extended list ?
A node is enqueued does not mean that it is extended. In fact, we can enqueue many times the same node, but with different distence value each time, they are just candidiates for extension. And we just choose the shortest for extension.
I think this consistency matter is there whenever we don't extend the same node twice -Sure it is true what he said, but I think he should have mentioned/clarified that whenever it is not a map (the distance eq AB+BC>=AC is not necessarily true) We should check not just whether we've extended this node before or not, but with what cost value???? I mean even if we are not using heuristics this could miss a shorter path (i. e. for example it could be yes we have extended C before but with a longer path, if SC=11 while SA->AC=5+2=7) . Or am I missing something here????
At the end he shows the example where A* doesnt work. Wouldn't you just make it so that if you find a shorter path to a node that already been covered you swap it in?
When he explains consistency, couldn't you use a check for your extended list where instead of checking if a node has already been reached, instead take whichever repeat is the shortest? Therefore although he reaches C through S-B-C and therefore he was hosed when he tried S-A-C, it'd instead take S-A-C because if there's a way between S & C that is less than the others you'd use that path
Wouldn't BFS give us shortest path as well?
+qidas123 we only extend a node one time, algorithm actually never says extend if the path is shorter. What we keep in the ""Extended List"" is only the names of nodes we already extended, nothing about the length of path to those nodes.
The Branch & Bound + Extended List Algorithm is basically Dijkstra. Or is there something I miss?"
"And likewise for the other words.
So you can see this sort of matrix here, right? So we just want to make sure that our attention weights are 0 everywhere here.
And so in the affinity's calculation, we add negative infinity to all of these, in this big matrix.
And that guarantees that we can't look to the future.
OK, so now we can do big matrix multiplications to compute our attention as we will see, and we sort of don't worry about looking at the future because we've added these negative infinities.
And that's the last problem with self attention, sort of that comes up fundamentally as like, what do we need for this building block? You didn't have an inherent notion of order, now you have a good notion of order or at least something of a notion of order.
You didn't have nonlinearities, add feed forward networks, and then you didn't want to look at the future, you add the masks for the decoders.
So self attention is the basis of any self attention based building block, hello, position representations are useful, nonlinearities are good.
You don't have to use a feed forward network, right, like you could have just done other stuff I guess, but in practice actually it's really easy to parallelize these feed forward networks as well.
So we end up doing that.
And then the masking, Yeah, you don't want information to leak from the future to the past in your decoder.
So let me be clear, we haven't talked about the transformer yet.
But this is all you would need if you were thinking like gosh, what do I need in order to build my self attention building block? We'll see that there are a lot more details in the transformer that we're going to spend the rest of the lecture going through.","Good lecture, but I think there is some confusing usage of the K, Q, and V matrices. You introduce them by saying: (1) K*x_i = k_i, Q*x_i = q_i, V*x_i = v_i In this way you're saying that X, Q, and V operate on x_i column vectors from the left to yield another column vector. But then later, when you stack the transposes of the x_i to produce the X matrix for the tensor attention calculation, you multiple from the right with K: (2) softmax(XK(XQ)^T) = Attention weights I think both K and Q should be transposed in (2) as you're using them in order to be consistent with the way you define the matrix in (1).
Great lecture, very clear! Thanks for making it freely available! In multihead self-attention, each head is going to produce values vectors of dimension d/h whereas the single head self-attention produces value vectors of dimensions d. That allows the multihead self-attention to look at multiple places at the same time however reducing the dimension is going to discard some information. Could it be possible that single head self-attention works better in some cases than multihead self-attention because it doesn't reduce the dimensionality?"
"Both of those new examples are clearly below the dividing line.
They are clearly examples that I would categorize as being more like receivers than they are like linemen.
And I know it's a football example.
If you don't like football, pick another example.
But you get the sense of why I can use the data in a labeled case and the unlabeled case to come up with different ways of building the clusters.
So what we're going to do over the next 2 and 1/2 lectures is look at how can we write code to learn that way of separating things out? We're going to learn models based on unlabeled data.
That's the case where I don't know what the labels are, by simply trying to find ways to cluster things together nearby, and then use the clusters to assign labels to new data.
And we're going to learn models by looking at labeled data and seeing how do we best come up with a way of separating with a line or a plane or a collection of lines, examples from one group, from examples of the other group.",Th concept is understandable but how to make the right algorithm and set the data . Im new to this . Anyone cares enough to guide me through this ?
"No ties in baseball, so Boston wins, New York loses and vice versa.
So I want those numbers as well and these are things that you typically don't see but they're going to be important, because they do determine whether your team is alive or eliminated.
So 5 The reason I have dashes here is because the team doesn't play itself in the diagonal.
AUDIENCE: Is that the number of games? PROFESSOR: Yes that is absolutely the number of games that they're going to play against each other.
All right so I've got small number.
I mean 5 plus 7, 12.
Plus 4.
What is that? 16.
16 plus 3 is 19.
There's 28 games left to play, there's obviously games that are being played against other teams outside the division.
And so this is just rij, OK? So that's the story here.
And I should say that when I talk about this, what are these corresponding to? Well this corresponds to NY.
This column corresponds to Baltimore this corresponds to Boston, same order, this corresponds to Toronto.
This column corresponds to Detroit.
So otherwise it's under specified.
Any questions about the table here? If you want to know, the Bo Sox did not make the playoffs that year.
So all that most sportswriters do, most people do, is say Team i is eliminated.
If wi plus ri is strictly less than wj for sum j.
So you're just going to say, OK you know what? If I've won all my games, I'm still not going to make it to this team that's already won 75 games, right? And so clearly I'm done.
That team has already won more games than I could possibly win.
So that's an easy one.
And so this is strictly less, because we're just talking about elimination, total elimination.
You get your summer vacation, or your fall vacation.
So let's just say that you had Detroit, and you're talking about w 5 years.","The last example ,from what I understood allows team NY to win only one game w.r.t the games it plays with the other 5 teams ,but can there be a case where the team wins a game against a team we have not considered and hence not allowed team 5 to still make it ? Please correct me if I am wrong"
"So this was the ELBO in case of EM, where each Q was separately calculated for each example in parallel.
However, when we move over to amortized inference, we don't have Q and theta anymore.
We had Q because we would calculate Q separately for each distribution.
Instead of having separate Q's for each distribution, we instead have shared phi and psi that are shared across examples, right? And the rest of the expression stays the same, except Qi is now not something that was independently calculated for each example.
But instead, we use this amortized inference technique, where we feed each example to these networks, and the output of the network will become the parameters of the corresponding Q distribution, right? So now we are pretty much ready.
In case of EM, we would optimize this ELBO with coordinate ascent.
Now we're going to optimize this objective with gradient descent, right? And the parameters that we want optimize are the phi, psi, and theta.
So this means you want to now maximize this ELBO with respect to these parameters, which means we want our update rules to look like this.
So we want theta to be theta plus some learning rate-- let's call it eta.
That's just the learning rate-- times the gradient with respect to theta of ELBO of psi, phi, theta.
And phi is equal to phi plus eta gradient with respect to phi of basically the same thing.
Similarly, psi equal to psi plus eta times the gradient with respect to psi of the ELBO.
You want to do this until convergence, right? In EM, we would do E step and M step until convergence.
In case of the variational autoencoder, we want to perform these gradient updates until convergence, which means, in case of EM, we had Q's and thetas.","Professor, can I use gradient decent to solve GMM or factor analysis? Thanks!"
"And we are going to show Bellman-Ford in the context of the DAG algorithm we're going to solve today.
So that's the very general case in terms of restrictions on our graph.
But in reality, most problems that come up in applications occur with graphs that have positive edge weights.
You can think of a road network.
You've got-- or non-negative ones anyway.
You're traveling along, and it's not ever useful to go back to where you came from, because you want to make progress to where you're going.
So in the context where you don't have negative weights, you don't have this problem where you have negative weight cycles.
We can actually do a lot better by exploiting that property.
And we get a bound that's a little bit-- that looks a little bit more like n log n.
It's pretty close to linear.
You're losing a log factor on the number of vertices.
But it's pretty good.
This is called Dijkstra, and we'll get to that in two lectures.
OK, so that's the roadmap of what we're going to do for at least the next three lectures.","great lecture, thanks mit! I have a doubt, in the dag relaxation algo we init the distance estimate from the source to the source equal to 0, but the professor said that this value could negative or -infinity, however that can occur when the graph has a negative cycle, given that the graph is a DAG then the shortest path distance from the source to the source has to be 0 because DAG has no cycles"
"We only care whether a certain ID's the largest.
So for example, in this case, when A send its ID 5 to B, B knows this 5 won't be the leader.
All right.
Because 5 is too small.
It's even smaller than his ID.
So we can choose to drop this message.
There's no point in passing that message further.
Same thing for this message.
C knows that 10 is too small.
And it doesn't have to pass it along.
AUDIENCE: [INAUDIBLE] LING REN: Oh, the ID? It's just the integer each node chooses at random.
See.
Yeah, the only purpose of the ID is to break symmetry.
So, yeah.
Like we have seen the lecture, if they don't do this, if they don't have any unique identifier, they won't be able to select the leader.","Not much intuitive , if half of node goes inactive in next round how could they ever know who is their supreme leader ? If I have A, B, C, D if say B send message to A, C I am the leader and they agree , then B sends message to D and say I am the leader , by D says that , I have large number dude , you are not leader then D is the leader now B know that D is the leader it need to pass that information down to B and C and now all know that D is there leader ."
"SQUEAKING] [RUSTLING] [CLICKING] ERIK DEMAINE: All right welcome back to 006, Dynamic Programming.
We're now in step two out of four-- going to see a bunch more examples-- three more examples of dynamic programming-- longest common subsequence, longest increasing subsequence, and kind of a made up problem from 006, alternating coin game.
And through those examples, we're going to explore a few new ideas, things we haven't seen in action before yet-- dealing with multiple sequences, instead of just one; dealing with substrings of sequences, instead of prefixes and suffixes; parent pointers so we can recover solutions, like in shortest paths.
And a big one, which will mostly be fleshed out in the next lecture, is subproblem constraint and expansion.
This is all part of the SRTBOT paradigm-- remember, subproblems, relations, topological order, base case, original problem, and time.
Here is subproblems and relations.
I've written down both what these things are and the key lessons we got from the first dynamic programming lecture.","What is the difference between the DP solution to LIS in this video and BruteForce?
Hm. How does memoization and tabulation play in with something like the alternating coin game. Your done SRTBOT yes, but thats just a recursive algorithim at the end of the day, not DP. Unless I'm missing something."
"So is it, like you remember this example, is it true that I can derive rain or traffic here, OK? So how do I check that? Well, to check soundness I need to actually get to the models and meanings of each one of these formulas and I need to check entailment, so let's check that on this example.
So if I have rain or snow, what is models of rain or snow? So I have here my truth table, it's going to be a little bit larger because I have both snow, rain and traffic, so I need to look at 0 1 values for all of them.
So I have rain or snow, rain or snow corresponds to these shaded areas, so that's models of rain or snow, and then I have models of not snow or traffic, that corresponds to these shaded areas.
And remember as I add more formulas to my knowledge base I'm shrinking its models, I'm adding more constraints so I'm shrinking the models, that is why models of these two formulas is going to be the intersection of their models.",how did you go from A -> (B V C) to ~A V B V C? wouldn't it be ~A V (B V C) ?
"So say that 0 is the reference point.
And then you look at the gradient with vector theta evaluate at theta 0.
This is the first order gradient times theta minus 0.
So this is the first order Taylor expansion.
And then you say you have some higher order terms, which we are going to ignore.
And once you do this, you can define maybe this one.
Let's call this g theta x.
Of course, it also depends on theta 0.
But let's say theta is the variable, so that 0 is fixed.
So this is a function of theta.
So this is a linear function.
So if you define this, then g theta x is a linear function in theta.
Because where theta shows up-- see, it only shows up here.
And it shows up linearly.
And basically, you linearize your model.
And you can also, I guess, define the other theta, which is the difference between theta and theta 0.
I guess, technically, you should call this-- maybe this is a affine function because there is a constant term, affine function.
In theta or in the other theta, they are not too different.
I guess just want to introduce this notation, delta theta.
And so f theta 0, this reference point, this is a constant from this perspective, right? It's a constant that doesn't depend-- constant for fixed x.
It doesn't change as you change theta.
And in some sense, this is just not that important, so not very important.
Because it's a constant.
And sometimes for convenience, you choose-- so choose theta 0 such that f theta 0 x is equal to 0 for every x.
How do you do it? So you do it-- so if you really want to do this, you need to-- for example, what you can do is you can design network that you split your networking into two parts.",Quick clarification if possible: what does he mean by the notation y^(i) (or x^(i) )? looks like the i-th derivative to me
"And I have by more or less automatically plugging into the definitions and a structural induction, I've proved that this inequality holds for the recursively defined depth function, and we're done.
Let's look at one more familiar example.
I want to give [? the ?] recursive definition of the positive powers of 2.
So the base case is the 2 is a positive power of 2, and the constructor is just one constructor I'm going to use-- that if x and y are positive powers of 2, then their product is a positive power of 2.","I dont understand the length vs depth proof. Suppose I have this string. [][][], the length is 6, and the depth is 1. 6 > 2^(1 + 1) = 4 I'm misunderstanding something here, could anyone enlighten me?
The way I understand it, by the definition of M, the added brackets for d(r) can only enclose s, so the depth being max(d(s)+1, d(t)) should be correct see this video: https://www.youtube.com/watch?v=TXNXT3oBROw
In general, you can have a very long sequence of depth 1: [][][][][][][][][][][][]..., so how is the length upper-bounded by any function of the depth?"
"Each pocket has a number and a color.
You bet in advance on what number you think is going to come up, or what color you think is going to come up.
Then somebody drops a ball in that wheel, gives it a spin.
And through centrifugal force, the ball stays on the outside for a while.
But as the wheel slows down and heads towards the middle, and eventually settles in one of those pockets.
And you win or you lose.
Now you can bet on it, and so let's look at an example of that.
So here is a roulette game.","the wording of the last sentence was confusing and made it sound like the opposite of reality!  How it is written makes it sound that it's a 50% chance to get 26 consecutive reds, if the previous were 25 black...The correct statement is just to say, if you had 25 reds in a row, the 26th spin is still 50% to be red regardless of what happened previously as all spins are independent of one another (Gamblers Fallacy to think otherwise). Also how do you get 1/67,108,86*5* for a power of 2?
Sir, is their way to account actually experimented pattern for this random event simulation ? (like some numbers are more likely to result in this gambling event because of the biasness of the roulette). Thank you
In the slide ""Gambler's Fallacy"" it reads at the bottom: ""Probability of 26 consecutive reds when the previous 25 rolls were red is:"" The wording is poor in my opinion. Does it mean: ""What is the probability of the next roll being red?"" Or Does it mean: ""What is the probability of the next 26 rolls being red?"" Or maybe : ""What is the probability of 26 consecutive reds occurring in the next roll if the previous 25 rolls were red?"" Based on his answer I think that the question should have read: ""What is the Probability of the next outcome being red when the last 25 outcomes where red?"" And then he goes on to talk about it being independent after this question. He didn't establish at the beginning that the outcomes were independent.
One question about the roulette game - if your odds of winning are 1/36 but the payout is 35x your bet. So infinite spins should converge to a -100% return, no? If I am wrong, please explain how so I can understand.
the next toss is independent of the previous toss ;but there is a different question that can be asked :what is the probability of of x tail(heads) in a row=1/2^x .Two completely different betting strategies"
"And we already did O, and then time-- is a little different from the past examples.
So number of subproblems, just like usual, is linear length of A subproblems.
It's only one sequence we're thinking about now, unlike the previous example.
But the work we're doing in this relation now is non-trivial work.
Before we were just guessing among two different choices.
Now we're guessing among up to n different choices.
This n here is length of A.
And so we have theta length of A, non-recursive work that we're doing in each subproblem.
Or you might think of this as choices that we're considering.
And for each choice, we're just spending-- I mean, we're just taking a max of those items, adding 1.
So that's a constant overhead.
And so we just get this product, which is A squared-- cool.
So that's longest increasing subsequence.
Make sure I didn't miss anything else-- so we're using this idea of asking a question, and guessing or brute forcing the answer to that question in two places.
One place is we're promising-- we're requesting that the longest increasing subsequence starts at i, so then the question is, well, what is the very second item that's in the longest increasing subsequence that starts with i? We're calling that j, and we're brute forcing all the possible choices that j could be, which conveniently lets us check, confirm that it's actually an increasing subsequence locally from i to j.",Hard time understanding relation between graphs and answer of problems
"You don't care about diversity anymore, And we could just turn that off.
Is that thing still running? Go back to fitness rank-- bingo.
So there you are-- you're stuck for 600 million years.
So let's see if this will handle the moat problem.
See, our step size is still small.
We'll just let this run.
So the diversity of P sub is keeping it spread out, pretty soon, bingo, it's right in there.
It's across that big moat, because it's got the crossover mechanism that combines the best of the x's and the best of the y's.
So that seems to work pretty well.
OK, so see, these are some of the things that you can think about when you're thinking-- oh, and of course, we're a shark, we're going to forget about diversity.
We'll change the selection method from fitness and diversity rank to just diversity.
It collapses down on to the highest hill.
Yeah, [? Feedball ?], what? STUDENT: How does step size translate into mutations? PATRICK WINSTON: Oh, just the-- question is, how does step size translate into mutation? Instead of allowing myself to take steps as big as 1/10, I might allow myself to take steps as big as 3/10, according to some distribution.","because the next generation values are based on the original values, so they will be similar to the original values and might get stuck into a local maximum. You mutate them to try o get out of local maximum. Like imagine if all the random original values were 1, you would only get 1s in the next generation without the mutation.
Hey guys if we remove the diversity factor at some instance i.e. generation then that generation could be perfect?????
This is very helpful for me. But I have a question. What is Pc ? And how much is it. I watch the screen ,find the rank probability is 0.05. (1-Pc) equals 0.95,so 0.95^39 always more than 0.05,if Pc equals 0.05. I think I need some help.
Kinda disappointed by this lecture: 1. The lecturer said mutation is essentially hill-climbing which I agree. But he didn't explain what cross-over is and why it is important. At least he should have stressed that it was still a mystery. 2. Crediting the artificial creature program for its ""rich solution space"" rather than genetic algorithm without even justifying it is kinda irresponsible. Because that's a bold and non-trivial claim. 3. Yes, GA requires fine-tuning of parameters, in machine learning we have feature engineering which is doing the same thing. Isn't it naive to thinking an algorithm as general as GA would work well on all problem instances without feature engineering? There is no universal problem solving algorithm that works well for all problem instances (no free lunch theorem) Overall, I have the impression that the lecturer has prejudice against GA.
I watched your lecture with great interest. I'm teaching myself Python by coding a GA. Often, when selection and reproduction are discussed, the biological model of two parents are combined into one offspring. I have a different idea. Say you have a starting population of 200. You apply your fitness function to score each member and then the grim reaper function to kill the bottom half in terms of fitness. You have a population of 100 members. Why not combine each member with every other member? (think nested loops). 100 * 100 (crossover) produces 10,000 new members. apply a mutation function randomly against the population and against each cell in the DNA string. Then reduce the population by 99% by fitness back to the original level of 100. In effect producing the next generation from the top 1 percent of the current generation. Have you considered such an approach? Can you give me your opinion? Thank you!
John David Deatherage How are you sure you won't take out the other top ones when you reduce the population?Newbie here"
"If you don't fix your paths if you want to generate a formula that corresponds to going through all paths, then it's also relatively easy.
The Only thing is you have to deal with loops in more of a special way.
Yes? AUDIENCE: [INAUDIBLE].
ARMANDO SOLAR-LEZAMA: I don't know.
So dictionaries and maps are actually very easy to model using uninterpreted functions.
And, in fact, the theory of arrays itself, it's just a special case of uninterpreted functions.
So more complicated things can be done with uninterpreted functions.
In modern SMT solvers there is native support for reasoning about sets and set operations, which can be very, very useful if you're reasoning about a program that involves lots of set computations, for example.
When designing one of these tools the modeling step is really important.
And it's not just how you model complicated program features down to your theories.
So, for example, things like heaps down to arrays.
And also the choice of what theories and the solver you use.",What is the basic difference between SMT solvers and Answer Set Programming solvers? Is there a good resource that clarifies differences between ASP and SMT?
"You just propagated the emptiness upwards.
Now you take this empty node, and you look for its siblings.
Again, its sibling is-- well, it's barely full.
So what do you do now? You bring the 30 down, and you merge this.
So let's do that.
30 comes down, and there we go.
Looks fine? Does that tree look good? Questions about the operation? I'm sure it was not clear, but-- anything? Make sense? OK, let's do a deletion where we can actually do a rotation.
So let's go ahead and delete 20.
So you do your searching, go down the tree.
You find the 20 under here.
So now, OK.
So you're left with just-- actually never mind.
We'll do another one.
So this doesn't do anything.","can someone explain why in the last bit we chose to rotate 17 on the left over 30 on the right? thxxxxx in advance!
excuse me sir , your deletion of 41 is wrong , why should you bring 30 down ?
Complexity of delete operation?"
"I then can compute what for- for every coordinate, what is the mean value of that coordinate and what is the variance along that coordinate, uh, of the vector, uh, acro- across this n, uh, input points, X, that are part of a- of a mini-batch.
And then, um, I can also have, um, in this case, I can have, um, uh, um, uh, then, uh, you know, there is the input, there is the output that are two trainable parameters, gamma, uh, and beta, and then I can, uh, come up with the output that is simply, um, I take these inputs x, I standardize them in a sense that I subtract the mean and divide by the variance along that dimension so now these X's have, uh, 0 mean and unit variance and then I can further learn how to transform them by basically linearly transforming them by multiplying with gamma and, uh, adding a bias factor, uh- uh, bias term, beta.
And I do this independently for every coordinate, for every dimension of, uh, every data point of every embedding, i, that is in the mini batch.
So to summarize, batch normalization stabilizes training, it first standardizes the data.
So stand- to standardize means subtract the mean, divide by the, uh, by the standard deviation.
So this means now X has 0 mean and variance of 1, so unit variance, and then I can also learn, uh, these parameters, beta and gamma, that now linearly transform X along each dimension and this is now the output of the, uh, batch normalization.
Um, the second technique I wanna discuss is called, uh, dropout.
And the idea- and what this allows us to do in neural networks is prevent over-fitting.
Um, and the idea is that during training, with some small probability P, a random set of neurons will be set to 0.",From the lecture it is not clear how Batch Norm works for the mini-batch.
"Yeah, you could.
[BACKGROUND].
Yeah, you could, you could, right? So that's, uh, you know, this is- this is the product rule.
Um, another, uh, very useful, um, identity that you're gonna be using is gonna be, um, gradient of A of the log of the determinant of A.
And this looks pretty- pretty nasty.
So what's happening here? A is a matrix.
Take the determinant of the matrix, and then you take the log of the determinant of the matrix and you wanna differentiate it with respect to A.
Why would you want to do that? I mean, [LAUGHTER] uh, but it- it- it turns out that this is gonna be a recurring, um, uh, um, uh, pattern.",One question. Why is the determinant equal to the division of output volume over input volume?
"And actually the fact that stochastic gradient descent is kind of noisy and bounces around as it does its thing.
It actually means that in complex networks it learns better solutions than if you were to run plain gradient descent very slowly.
So you can both compute much more quickly and do a better job.
OK, one final note on running stochastic gradients with word vectors, this is an aside.
But something to note is that if we're doing a stochastic gradient update based on one window, then actually in that window we'll have seen almost none of our parameters.
Because if we have a window of something like five words to either side of the center word, we've seen at most 11 distinct word types.","While using Stochastic Gradient descent, if we choose a corpus of 32 center words, how do we make updates to the outside (context) words that surround it. Because these words will show up when we compute the likelihood and if out corpus doesn't include them then how does their probability of occurring get updated? Thanks!"
"We started with log u mass.
We have two copies of log u over 2.
That's the same total mass.
So how many recursions do we do? Well we do do log log u recursions.
The total number of leaves in that recursion tree is log u.
Each of them, we pay constant.
So this is log u, not log log u.
To get log log u, we need to change this 2 into a 1.
We can only afford one recursive call.
If we have two recursive calls, we get logarithmic performance.
If we have three recursive calls, it's even worse.
Here, I would definitely use the Master method.
It's less obvious.
In this case, we get log u to the log base 2 of 3 power, which is log u to the 1.6 or so, so both worse than log n.
This is strictly worse than log n.
This is maybe just a little bit worse than log n, depending on how u relates to n.
OK, so we're not there yet.
But we're on the right track.
We have the right kind of structure.
We have a problem of size u.
We split it up into square root of u sub problems of size u.
From a data structures perspective, this the first time we're using divide and conquer for data structures.
It's a little different from algorithms.
So that's how the data structure is being laid out.
But now we're worried about the algorithms on those data structures.
Those, we can only afford t of u equals 1 times [? t of ?] squared of u plus order 1.
Then we get log log u.","space requirement was said to be O(n) at the end, but what about the fact that keys use O(logu) bits? I guess we consider one key to have O(1) space requirement, but bit count for representing a key is in dependence with u, so if we count the bits then it's O(nlogu)
Why is T(u) = 2*T(u) + O(1) <=> T'(lg u) = T'(lg (u^1/2) + O(1)? Like why can we replace u with lg(u) and obtain the time complexity for T(u)? Thanks!
i have question.. so we are basically having 3 layer ( 1. space O(u) the leaf part/// 2. the OR operation part(middle thing O(rootu))/// 3. summary) in every tree level? or do i have just ""one"" tree level made of 3 layer i think former is right because otherwise we don't need to consider recursion time complexity..?"
"What I'm going to do is convince you that it's not valid by giving you a counter-model where I choose an interpretation.
I choose a domain of discourse and predicates that P and Q are going to mean over that domain and that make the left-hand side of this implication true.
And then I'm going to show you that the right-hand side is not true.
And that means that in that domain with those interpretations of P and Q, this implication fails so it's not valid.
So I need to make the left-hand side true and the right-hand side false.
Well, I'm going to choose the domain of discourse to be the simplest one that will make this false, namely let's let the domain of discourse just be the numbers 1 and 2.
And let Q of z be the predicate that says z is 1 and P of z be the predicate that says z is 2.
Well, is the left-hand side true? Yeah, because the only things there are in the domain are 1 and 2, and so clearly everything in the domain is either 1 or 2.
So the antecedent is true.
On the other hand, is everything in the domain, does it satisfy P? Is everything in the domain equal to 2? No, 1's not equal to 2.
What about, is everything in the domain equal to 1? Is it true that for all y Q of y holds? No, 2 is in the domain, and it's not equal to 1.
And so we have found exactly what we wanted, a counter-model which makes the left-hand side of the implies true and the right-hand side of the implies false.
Let me close with just one more example of a valid formula that we can talk through.
This is the version of De Morgan's law that works for quantifiers.","A thoughts on this. Taking the last example, why are we replacing x with y? This is done though out the examples but it is never explained why. I must be missing the reason but it seems just as understandable to me to say not(all x.p(x)) IFF exists x.not(P(x))"
"RINI DEVADAS: Good morning, everyone.
Thanks for coming in on a Friday morning, especially with the long weekend coming up.
We're going to pick up where we left off yesterday.
And so if you recall, we were looking at an iterative approach to solving the eight queens problem.
And in case you've forgotten in the, I guess, intervening 20 hours, the eight queens problem, which is a specific instance of n queens is you have a chess board-- and I won't draw all the squares-- which is, obviously, an 8 by 8 board.
And you want to place eight queens on this board such that no two queens attack each other.
And we looked at different data structures to solve this.
The naive data structure would be to represent the board as a two-dimensional list.
And so you end up having an 8 by 8, if you have a chessboard for the eight queens problem.
We looked at a 4 by 4 for starters.
And then after that, we looked at compressing that representation down to a one-dimensional list where let's say you had a 5 by 5 board because we've got five entries here.","Back in last lecture, the 4 x 4 example, we were talking about we'd possibly have to place one of our previous queen in a different row to solve the problem. Here we are assuming that once a queen is placed in a column, we don't have to worry about it again. I understand that for each new queen we are placing in the new column, we only have to worry about it conflicting with the existing queen(s), but what if we weren't able to place a new queen in the n-th column without conflicting with some of the existing queen(s), and the possible solution could be achieved by placing one of the previous queen differently?"
"So we'll talk about frames a bit more in a little bit, but think of a frame as almost like a separate JavaScript universe.
It's a little bit equivalent to a process and UNIX.
So maybe this frame here, maybe this guy belongs to https://facebook .com/likethis.html.
So maybe here we have some inline JavaScript from Facebook.
And then, maybe, we also have some image.
So you know, f.jpeg.
That comes from https://facebook.com.
OK, so this is what a single tab might have in its contents.
But as I just mentioned, all this can, potentially, come from all these different principles.
So there's a bunch of interesting questions that we can ask about a application that looks like this So for example, can this analytics code from google.com actually access JavaScript state that resides in the jQuery code.
So to first approximation, maybe that seems like a bad idea because these two pieces of code came from different places.
But then again, maybe it's actually OK because, presumably, foo.com brought both of these libraries in so that they can work with each other.
So who knows.
Another question you might have is can the analytics code here actually interact with the text inputs here.
So for example, can the analytics code define event handlers? So a little bit of background in JavaScript.
JavaScript is single threaded vent driven model.
So basically, in each frame, there's just an event loop that's just constantly pulling events.",can someone explain to me where the script is being put?
"What might you see? So one thing that you might see is an advertisement.
So you might see an advertisement in the form of a gift.
And maybe that was downloaded from ads.com.
Then you also might see, let's say, an analytics library.
And maybe this comes from google.com.
So these libraries are very popular for doing things like tracking how many people have loaded your page, looking to see where people click on things to see which parts of their site are the most interesting for people to interact with, so on and so forth.
And you might also have another JavaScript library.
Let's say it's jQuery.
And maybe that comes from cdn.foo.com.
So some content distribution network that foo.com runs.
jQuery is very popular library for doing things like GUI manipulation.
Things like that.
So a lot of popular websites have jQuery.
Although, they serve it from different places.
And then, on this page you might see some HTML.
And here's where you might see stuff like buttons for the user to click on, text input, and so on and so forth.
So that's just raw HTML on the page.
And then, you might see what they call inline JavaScript from foo.com.
In my inline, you have a script tag.
And then, you have a closed script tag.
And then you just have some JavaScript code included in their directly.
That's as opposed to where you say something like script.
And then, the source equals something that lives on some server remotely.
So this is what's called inline JavaScript.
This is what's referred to as an externally defined JavaScript file.
So you might have some inline JavaScript there from foo.com.
And the other thing that you might have in here is actually a frame.",can someone explain to me where the script is being put?
"That's a little bit of setup.
And so let's dive right in into convex hull, which is my favorite problem when it comes to using divide and conquer.
So convex hull, I got a little prop here which will save me from writing on the board and hopefully be more understandable.
But the idea here is that in this case, we have a two dimensional problem with a bunch of points in a two dimensional plane.
You can certainly do convex hull for three dimensions, many dimensions.
And convexity is something that is a fundamental notion in optimization.
And maybe we'll get to that in 6046 in advanced topics, maybe not.
But in the context of today's lecture, what we're interested in doing is essentially finding an envelope or a hull associated with a collection of points on a two dimensional plane.","Can't i propose a situation where all points are on the convex hull? If that case were true, then it's complexity would basically be O(n) right?"
"And he said, well, what can we do to get around this? Well, we can represent the feature as a binary number.
Has legs, doesn't have legs.
0 or 1.
And the problem he was dealing with is that when you have a feature vector and the dynamic range of some features is much greater than the others, they tend to dominate because the distances just look bigger when you get Euclidean distance.
So for example, if we wanted to cluster the people in this room, and I had one feature that was, say, 1 for male and 0 for female, and another feature that was 1 for wears glasses, 0 for doesn't wear glasses, and then a third feature which was weight, and I clustered them, well, weight would always completely dominate the Euclidean distance, right? Because the dynamic range of the weights in this room is much higher than the dynamic range of 0 to 1.","What are some some methods to evaluate the quality of the clusters, if we do not have an outcome variable? In the example they were evaluated based in part based on whether the subjects in the cluster died at a higher rate. What do I do if I don't have an outcome to look at, only characteristics? For context, I'm creating cognitive style groups based on user data for an insurance company, and these styles will be later used for morphing, churn etc. but do not have an outcome variable per se."
"Imagine I give you a string of characters that are all soon to be composed of decimal digits.
I just want to add them all up.
This is also linear, because there's the loop.
I'm going to loop over the characters in the string.
I'm going to cast them into integers, add them in, and return the value.
This is linear in the length of the input s.
Notice the pattern.
That loop-- that in-iterative loop-- it's got that linear behavior, because inside of the loop constant number of things that I'm executing.
We already looked at fact iter.
Same idea.
There's the loop I'm going to do that n times inside the loop a constant amount of things.","Where does he get 1+6n+1 when nothing else changed in the program to make it 1+5n+1?
for i in range(x+1): x from 0 to x , doesn't this have (x+1) times ops?"
"So that's a good thing.
That's sort of like a convenient piece of information.
When we're solving these kinds of algorithms problems, notice that I've done sort of a similar reasoning in both of the last two problems, which you can do.
And actually, it's a pretty practical way of thinking about algorithms, where, like, this problem tells me, at the end of the day, my algorithm has to run in n log n time, right? Or like, in the previous problem, it had run in n cubed time-- I guess v cubed time.",is he really solving the problems? or just writing some inequalities?
"So to start to make that a bit more concrete, this is what we're doing.
So we have a piece of text, we choose our center word which is here in two and then we say, well, if a model of predicting the probability of context words given the center word and this model, we'll come to in a minute, but it's defined in terms of our word vectors.
So let's see what probability it gives to the words that actually occurred in the context of this word.
It gives them some probability, but maybe it'd be nice if the probability it assigned was higher.
So then how can we change our word vectors to raise those probabilities? And so we'll do some calculations with into being the center word, and then we'll just go on to the next word and then we'll do the same kind of calculations, and keep on chunking.
So the big question then is, well what are we doing for working out the probability of a word occurring in the context of the center word? And so that's the central part of what we develop as the word2vec object.
So this is the overall model that we want to use.
So for each position in our corpus, our body of text, we want to predict context words within a window of fixed size M, given the center word WJ.
And we want to become good at doing that, so we want to give high probability to words that occur in the context.
And so what we're going to do is we're going to work out what's formerly the data likelihood as to how good a job we do at predicting words in the context of other words.
And so formally that likelihood is going to be defined in terms of our word vectors.
So they're the parameters of our model, and it's going to be calculated as taking the product of using each word as the center word, and then the product of each word and a window around that of the probability of predicting that context word in the center word.","I dont get what theta(parameters) here?
Objective function seeks to maximise the probable likelihood of context word given center word. However should it also not try to minimise the probability of incorrect context words given center word?
Do areas of sparsity in the high dimensional word2vec space mean anything: For example, can you say - some word should exist here, but doesn't?
I had a question about ""observed - expected"" around Maybe I misunderstand but isn't the summation of p(x|c)*ux our prediction therefore making it our observed?"
"Right? Uh, and the covariance, um, of- of, ah, random variables are defined like this.
So, um, so the variance of- [NOISE] so the variance of x was defined as expectation of x square minus expectation of x square.
We saw this on a few slides ago.
Similarly, uh, the covariance of x comma y is defined as expectation of x, uh, times y minus expectation of x into expectation of y, which basically tells you that, um, the covariance of x with x is just the variance of x.
[NOISE] Okay? [NOISE] All right.
The multivariate Gaussian.
So this is, uh, one of the most commonly used distributions defined over a collection of random variables, right? And this is something that's gonna to show up a whole lot in this course.","In the case of the probability density of the multivariate gaussian distribution in the denominator, the normalising constant sqrt(2*pi) will be raised to the power of 'n' instead of 'd' because we are having n many random samples?"
"And one of those measurements might be the total area, including the area of the holes of these electrical covers.
Just so you can follow what I'm doing without craning your neck, let me see if I can find the electrical covers.
Yes, there they are.
So we've got one big blank one, and several others.
So we might also measure the hole area.
And this one here, this guy here, this big white one has no hole area, and its got the maximum amount of total area.
So it will find itself at that point in this space of features.
Then we've got the guy here, with room for four sockets in it.",Isn't there an ambiguous way to divide the graph into 4 areas? That little triangle in the middle looks like it could be included in any of the four boundaries.
"And actually, the number of comparisons that I do in an execution of the algorithm is just along a path from here to the-- to a leaf.
So what do the leaves actually represent? Those represent outputs.
I'm going to output something here.
Yep? AUDIENCE: [INAUDIBLE] JASON KU: The number of-- OK.
So what is the output to my search algorithm? Maybe it's the-- an index of an item that contains this key.
Or maybe I return the item is the output-- the item of the thing I'm storing.
And I'm storing n things, so I need at least n outputs, because I need to be able to return any of the items that I'm storing based on a different search parameter, if it's going to be correct.","""what is n?"" gg"
"So I now have G with the new f.
So I can now think about what is G of f1 correspond to.
The G is separate from f, so in some sense the G stayed the same, but the f changed, as you saw here.
And now G f1 looks like s, a, t.
What do I do between s and a? Well, I only have something from a to s.
I don't have anything from s to a because the residual capacity in this direction, thanks to the fact that the flow has saturated the edge capacity, is actually zero.
But I do have an edge this way.
And what is the value for that edge? It's 2.
And over here, what are the numbers for the top and the bottom? 2 and 2.
And now I look at G f1, and I've gone through one iteration of this pseudo code, and I now try to find an augmenting path in G f1.
And is there a path from s to t? No, which means I'm done.
This is the max flow.
You believe me? For this example, you believe me.",I don't understand why you can't do s-a-t and skip the a-b or b-a bottleneck. Thanks in advance!
"So those are pretty straightforward cases.
So we have that 0 equals x of 0, comma j.
Remember, the first index is the number of coins you have.
So this is saying, I can't make anything.
I don't have any coins.
And similarly, equals x i, comma 0 for all i, j.
So this is the coins and the objects.
OK.
So let's see.
I keep writing these problem sessions too big and then spending half of it erasing.
So let's try and fit this on one board here.
So we're going to do SRTBOT.
Then the second-- no, the first O, because there's no O in SRT-- is the original problem that you want to solve.
So of course, you start out with n objects and n coins.
So the original problem we want to solve is equivalent to computing x income n.",can someone write the relation of the last problem please? and how can we choose the f?
"So now I'm going to do a for-loop over the edges, increasing order by weight.
Now I want to know-- I have an edge, it's basically the minimum weight edge among the edges that remain, and so I want to know whether I should add it.
I'm going to add it provided the endpoints of the edge are not in the same connected component.
How can I find out whether two vertices are in the same connected component, given this setup? Yeah? AUDIENCE: Call find-set twice and then-- ERIK DEMAINE: Call find-set twice and see whether they're equal, exactly.
Good answer.
So if you find-set of u is from find-set of v, find-set just returns some identifier.",what if u and u' are connected by a path that includes e' ?
"Cool.
And then one other thing that we had mentioned earlier is instead of using augmentations, we could use, basically, our positives could be frames in a video that are nearby in time.
And the negatives could be things that are further away in time, or from other videos.
And there have been a number of papers that have done something like this and they've been able to get good results on tasks like robotics tasks.
And so in this paper can take, basically, a data set with a ton of diverse videos of humans.
Do this sort of time contrastive learning, where you pull together frames that have similar representations and push apart frames that have different representations.
And there was also an additional loss that sort of did a form of contrastive learning between videos and language.",What papers is she referring to when describing two different loss functions?
"Now, we are left with a subproblem that starts after request to j finishes.
So I'll write that as Rj, Where I define Rj to be the set of requests where their stop time is later than the finish time of the j-th request.
OK, so just to repeat, we choose a request, has the potential to be the first request, and then, we look at all the requests that start after it, and solve a subproblem of that case.
Then, we take a max of all the candidate we have, and that's going to give us the optimal solution.
Any question about this algorithm? So this algorithm runs in n square time.
Now, we're going to try to optimize that, and come up with a better algorithm.
So in order to improve anything, we first want to identify the inefficiency in this algorithm.
So which part in the algorithm do you think is inefficient, or silly, unnecessary? Go ahead.
AUDIENCE: So it's inefficient to look through every previous subproblem, when we're trying to find maximum [INAUDIBLE].
LING REN: I was saying that we don't need to go through every of this case.
AUDIENCE: Yeah, we shouldn't Well, we should be able to efficiently query for the right one.
LING REN: OK, I think you are definitely correct.
So let me just go through what this algorithm does, and it will be more clear.","the weighted interval scheduling. Why was the first alg n squared and second one just n? it takes constant time to computer R1
Is finding the intervals in R the only reason the first algorithm is n^2? I feel like in both cases we're assuming R is constant time, but there's a different reason the unsorted algorithm is n^2, but I can't find an explanation anywhere on the internet or in textbooks"
"Uh, that's times the partial derivative of Theta J of X Theta X minus Y, right? So if you take the derivative of a square, the two comes down and then you take the derivative of what's inside and multiply that, right? [NOISE] Um, and so the two and one-half cancel out.
So this leaves you with H minus Y times partial derivative respect to Theta J of Theta 0X0 plus Theta 1X1 plus th- th- that plus Theta NXN minus Y, right? Where I just took the definition of H of X and expanded it out to that, um, sum, right? Because, uh, H of X is just equal to that.","why in cost function he did 1/2 and not 1/2*m ?
1.14.54 my answer is (X^T X )+(X^T ^T X)-(X^T Y)-(Y X^T) its same or my ans is wrong ?
why is it that the cost function has the constant 1/2 before the summation and not 1/2m?
The value of x0 is always one 1. So theta0 can rely on x0 for the update. If we have single feature then h(X) =x0*Theta0 + x1* theta1 (which is ultimately equal to theta0 + x1*theta1 as x0=1, theta0 can also be referred as intercept and theta1 as slope if you compare it with the equation of a straight line such that price of house is linear function of # of bedrooms)
I don't really understand what you mean by 1/2m. However, from my understanding, the 1/2 is just for simplicity when taking the derivative of the cost ftn the power 2 will be multiplied to the equation and cancellyby the half."
"In this case, color is going to be important because, uh, probably you want your camera to work in, uh, different settings, day and night as well.
So, the luminosity is different, the brightness and also we all have different colors and we need to all be detected, compared to each other.
Yeah.
I might go somewhere in an island and come back, uh, you know, full of color but, uh, but I still want to be able to access the gym.
Uh, output.
What should be the output? The question on the resolution, is that a minimum resolution or is that like a- I think if you have mo- in unlimited computational power, you would take more resolution but that's a trade-off between computation and resolution.
So, output is going to be one, if it's you and zero if it's not you in which case they will not let you in.
Okay.
Now, uh, the question is what architecture should we use to solve this problem now that we collected the data set of mapping between student IDs and images.","I have a question which is about verification of student faces, in that problem we also have to check the data set for the corresponding image to compare it with input, then why Sir is telling that in face verification we campare it with one image and in face recognition we campare the input with the whole dataset?"
"And we cancel this.
You cancel this.
And you get mod of V dash is greater than or equal to k, which means that you have a clique of size greater than equal to k.
Questions? Does that make sense? Really? All of it? OK.
That's good.
Any case.
So let's go back and see what we are doing here.
So the way you are doing NP-hard reductions is you take a problem that you already know is hard, you take any arbitrary instance of that problem, and you transform the input into an input to the problem that you're trying to show is hard.
So you take problem A, which you know is hard.
You transform the input into an input for problem B.",Given the completeness theorem what problem in proof theory about syntax does the SAT problem translate to?
"We'll assume a successful search.
That is going to cost me order log n with high probability.
And the cool idea here in terms of analyzing the search in order to figure out how we're going to add up all of these moves is we're going to analyze the search backwards.
So that's a cool idea.
So what does that mean exactly? Well, what that means is that we're going to think about this b search, which think of it as the backward search, starts-- it actually ends, so that's what I'm writing in brackets here, at the node in the bottom list.
So we're assuming a successful search, as I mentioned before.
Otherwise, the point would just be in between two members.
You know that it's not in there because you're looking for 67 and you see 66 to your left and 72 to your right.
So either way it works, but keep in mind that it's a successful search because it just makes things a little bit easier.
Now, at each node that we visit, what we're going to do is we're going to say that if the node was not promoted higher, then what actually happened here was that when you inserted that particular element, you got a tails.
Because otherwise you would have gotten a heads, that element would have been promoted higher.
Then you go-- and that really means that you came from the left-hand side, so you make a left move.",what is a quad node?
"Cap-Y means the signal Y.
And I'm going to reinterpret all the boxes.
This box means, take this signal, the whole thing, all n values on it, multiply it sample by sample, by minus 1.
Flip the whole signal upside down.
So I'm going to reinterpret all of the operations on the block diagram in terms of signals, rather than samples.
To do that, I need a representation-- a mathematical representation, for the delay box.
And I'm going to call that R, the right shift operator.
If you apply the right shift operator to a single X, it takes the whole signal X and shifts it to the right one.","The labels on the x axis are not the value of the x's but the time steps. The definition of the delta signal is that x[t]=0 for every t different from 0 so x[2]=0. The term x axis is confusing here, it would better be called time axis or just horizontal axis."
"So now we can look at an interesting region, which is this region, the area between the curve and a random classifier.
And that sort of tells me how much better I am than random.
I can look at the whole area, the area under the curve.
And that's this, the area under the Receiver Operating Curve.
In the best of all worlds, the curve would be 1.
That would be a perfect classifier.
In the worst of all worlds, it would be 0.
But it's never 0 because we don't do worse than 0.5.
We hope not to do worse than random.
If so, we just reverse our predictions.
And then we're better than random.
So random is as bad as you can do, really.
And so this is a very important concept.
And it lets us evaluate how good a classifier is independently of what we choose to be the cutoff.
So when you read the literature and people say, I have this wonderful method of making predictions, you'll almost always see them cite the AUROC.
Any questions about this or about machine learning in general? If so, this would be a good time to ask them, since I'm about to totally change the topic.","Doesn't doing sensitivity instead of 1 - sensitivity just flip the curve left-right, and keep the area the same? Or am I missing something?"
"And then when you add in the elements in B here-- A plus B-- you're counting all the elements in the intersection a second time.
The ones that are in A minus B get counted once.
The ones that are in B minus A get counted once, but the ones that are in A intersection B get counted twice.
So to get the right count, I have to subtract the size of A minus B so it's only counted once in the total formula, and that's an intuitive explanation of why inclusion-exclusion formula holds for two sets.","when we are talking about ""proof the inclusion/exclusion principle"", what does it mean? thanks :)
in class have 14 student ...10 of them study English 8 of them study franch 5 of them study Spanish ..and 5 study English and franch 2 study English and spanish 4 study franch and Spanish... first question how many students study these three languages what formula should i use here ?"
"Methods are like functions except that there's a couple of differences which you'll see in a moment.
And when you're calling methods, you're using the dot operator, like L dot append, for example, for lists.
So let's go back to defining our coordinate class and let's define a method for it.
So so far we've defined that part there, class coordinate and an init.
So we have that.
So in this slide we're going to add this method here.
So this method here is going to say I'm going to define a method called distance and I'm going to pass in two parameters.
Remember self, the first parameter, is always going to be the instance of an object that you're going to perform the operation on.","Why should coordinate be an object. It has no behaviour in itself, it's just a tuple of two ints. Just data. And a function to compute euclidian distance is just a function that takes two coordinates, four integers or two arrays, depending on how you decide to represent coordinates as data.
Nice ! extremely grateful learn from anna ma'am. Well I have questions( time-----24.11) what is guarantee that use will provide X,Y in distance(self,other) for OTHER parameter.how he/she even know this ? Definitely One way by documentation but any other way .?"
"And presumably the server knows the password, so the server can [INAUDIBLE] this quantity and see it actually matches what the user sent.
So what's nice about this protocol is that if we ignore man in the middle attacks for a second, the server is now confident that the user's actually Alice, because only Alice would know this password here.
And what's nice about this is that if the server is actually the attacker-- so in other words, if Alice sent this thing to someone who's not the person who she's trying to authenticate to, then the attacker still doesn't know the password.
Because the attacker got to choose C, but the attacker doesn't know what this is.
And so basically for the attacker to figure out what the password is, the attacker has to be able to, once again, invert these hash functions.
Do you have a question? AUDIENCE: I'm just curious, how can you not make a client do the hashing? [INAUDIBLE] PROFESSOR: So let's see, so your proposed scheme is that the client side is going to call this thing? AUDIENCE: Yeah, so instead of setting the password, and having the server hash the password and check it, the client would just send the hash password.",What is the paper the teacher is referring ?
"It's just a way to estimate sums of returns- sums up rewards.
So, that's both nice in the sense that, um, if you're using this in a process that you had estimated from some data or you're making the assumption that things are, er, um, you know this is the dynamics model but that's also estimated from data and it might be wrong, um, then this can give you sort of, um, if you can really roll out in the world then you can get these sort of nice estimates of really how the process is working.
But it doesn't leverage anything about the fact that if the world really is Markov, um, there's additional structure we could do in order to get better estimates.
So, what do I mean by better estimates here? I mean if we want to, um, get sort of better meaning sort of computationally cheaper, um, ways of estimating what the value is a process.
So, what the Markov structure allows us to do, with the fact that the present that the future is independent of the past given the present, is it allows us to decompose the value function.
So, the value function of a mark forward process is simply the immediate reward the agent gets from the current state it's in plus the discounted sum of future rewards weighed by the discount factor times the- and where we express that discounted sum of future words is we can just express it with V, V(s').",How return function is different from value function ? How come return will be different from value function when process is not stochastic .( both having sum of reward )
"Every instance of Staff6.01 is going to have an attribute room.
And that attribute room is going to be set to the string describing 34-501, the 6.01 lab.
If I want every 6.01 staff member to be able to do a particular thing, or have a particular method, or call a particular function, then I specify it like this.
This is the beginning of a method in the class Staff6.01.
It's called sayHi.
I'll talk about self in a second.
Don't worry about it.
Act as though-- if this is your introduction to Python programming, then pretty much pretend it's not there.",what is the difference between 6.01 and 6.001 ?
"So that's a plus over there.
Vampire.
Now, if we look at the ones who do cast a shadow, all those are not vampires.
They're all OK.
And now there're 8.
Three are vampires.
So that means that two of these must be vampires.
And I've got three, four, five, six so far.
So there must be two left.
So that's the way the shadow test divides up the data.
Now let's do garlic.
Vampires traditionally don't eat garlic.
I don't know why.
So we look at the garlic test, and we see that all of the Nos-- well, there're three Yeses, and they all produce a No answer.
So if somebody eats garlic, they're not vampires.
That means the three vampires must be over here.
Then there are two left.
So that's what the garlic test does.
See what we're trying to do? We're trying to look at all these tests to see which one we like best on the basis of how it divides up the data.
So now we've got complexion.
And there are three choices for this.
You can have an average complexion.","Sorry, I still don't understand how you arrived at 4 in the shadow group. Is it because there were 4 question marks? I fully understand the other homogenous sets. Thanks."
"The choice of this proper network representation of a given domain or a given problem deter- will determine our ability to use networks, uh, successfully.
In some cases, there will be a unique, unambiguous way to represent this, um, problem, this domain as a graph, while in other cases, this representation may, by no means, be unique.
Um, and the way we will assign links between the objects will determine, uh, the nature of the questions we will be able to study and the nature of the, um, predictions we will be able to make.
So to show you some examples of design choices we are faced with when co-creating graphs, I will now go through some concepts and different types of graphs, uh, that we can- that we can create from data.
First, I wi- I will distinguish between directed and undirected graphs, right? Undirected graphs have links, um, that- that are undirected, meaning, that they are useful for modeling symmetric or reciprocal relationships, like collaboration, friendship, um, and interaction between proteins, and so on, while directed, um, relationships are captured by directed links, where every link has a direction, has a source, and has a destination denoted by a- by an arrow.",what will be the effect of losing use-case-specific information?
"So in Kerberos, if I want to connect to a file server, maybe I just know the file server's public key from somewhere.
Like me as a freshman I get a printout saying the file server's public key is this.
And then you can go ahead and connect it.
And the way you might actually do this is you could just encrypt a message for the public key of the file server that you want to connect to.
But it turns out that in practice, these public key operations are pretty slow.
They are several orders of magnitude slower than symmetric key cryptography.
So almost always you want to get out of the use of public crypto as soon as practical.
So a typical protocol might look like this where you have a and b, and they want to communicate.","Can we see public key on certificate? Anyone can explain for me, please I'm still vague."
"So for example, if you are a human and you think about human social network, uh, the maximum degree that you could have, the maximum number of friends you could have is every other human in the world.
However, nobody has seven billion friends, right? Our number of friendships is much, much smaller.
So this means that, let's say the human social network is extremely sparse, and it turns out that a lot of other, uh, different types of networks, you know, power-grids, uh, Internet connection, science collaborations, email graphs, uh, and so on and so forth are extremely sparse.
They have average degree that these, you know, around 10 maybe up to, up to 100.
So, uh, what is the consequence? The consequence is that the underlying adjacency matrices, um, are extremely sparse.",what will be the effect of losing use-case-specific information?
"What's the variance of that going to be? x over sigma sub x.
Anybody see, instantaneously, what the variance of that's going be? Or do we have to work it out? It's going to be 1, Work out the algebra for me.
It's obvious, it's simple.
Just substitute x prime into this formula for variance, and do the algebraic high school manipulation.
And you'll see that the variance turns out not to be of this new variable, this transformed variable you want.
So that problem, the non uniformity problem, the spread problem, is easy to handle.
What about that other problem? No cake without flour? What if it turns out that the data-- you have two dimensions and the answer, actually, doesn't depend on y at all.
What will happen? Then you're often going to get screwy results, because it'll be measuring a distance that is merely confusing the answer.
So problem number two is the what matters problem.
Write it down, what matters.
Problem number three is, what if the answer doesn't depend on the data at all? Then you've got the trying to build a cake without flour.
Once somebody asked me-- a classmate of mine, who went on to become an important executive in an important credit card company-- asked me if we could use artificial intelligence to determine when somebody was going to go bankrupt? And the answer was, no.
Because the data available was data that was independent of that question.
So he was trying to make a cake without flour, and you can't do that.
So that concludes what I want to say about nearest neighbors.
No I want to talk a little bit about sleep.","so.. what is nearest neighbor??
If you are talking about ""x prime"", that's not the derivative of x. It's a new random variable. More precisely, it's a random variable transformed from the original x. With the definition of ""x prime"", you can calculate its variance by plugging it in the formula. You will get 1."
"So in order to actually write out my DP, I'm going to have to now look at the worst case situation in terms of the board I get back, because the only times I'm adding values is when I see a board in front of me and I pick a coin.
And I'm going to have to say now the opponent is going to see an i plus 1 j-- or an i j minus 1, excuse me, and might do something.
And I'm going to get something back.
I'm going to assume that the opponent is just as smart as I am, knows DP, taken 6046, et cetera, et cetera.
And I still want to get the maximum value that I can definitely win.
And so we need to look inside of that a little bit.",Why do we need to minimize opponents move. If we just consider all possible move opponent can play and just maximize shouldn't we still get the correct answer? Unless we specifically demand the opponent to optimize as well.
"This is our new estimating vector r.
And if the coordinates, the entries in the vector don't change too much, they change less than epsilon, then we are done.
And the equation we are going to iterate is written here.
Basically, node j will say, ""My new estimate of my importance is simply, take the estimate of the nodes i that point to me, uh, from the previous step, you know, divide each one by the out-degree of node i, sum them up, and that is my new importance.
And we are just going to iterate this, um, you know, a couple of times and it's ah, it's guaranteed to converge to the, uh, to this- to the solution, which is essentially saying this is guaranteed to find me the leading eigenvector of- of the underlying, uh, matrix, uh, M.
So, um, the method that do- does this, what I explained, is called power iteration, where, um, basically, the way we are going to do this, to just write it again in the very simple form, we initialize vector r, you know, let's call in- call it initialization to be times zero, to be simply, uh, you know, every node has, let's say the same importance or you can assign the random importances.
And then we will simply iterate, you know, new estimate of r equals M times previous estimate of r.","If M has more than 1 linearly independent eigenvectors with eigenvalue 1, the power iteration may not converge; there is more than one solution to the flow equation. One can assume it's not the case; it's interesting why.
According to the slides, we solve the rank vector r via Power Iteration, we don't use sparse properties of the matrix."
"So you obey that constraint and you just look at the edges that go from S to T, sum up those capacities, and your maximum flow or any flow is bounded, [INAUDIBLE], by that quantity.
And that's our c S T.
So you now have a relationship between the minimum capacity cut and the max flow.
We didn't actually exploit that in the sense of the algorithm itself, the operation of the algorithm.
But we're going to actually exploit the notion of a cut when we prove this algorithm that I got to late on Tuesday's lecture that corresponds to the Ford-Fulkerson algorithm.
I showed you the execution of it.
In order to prove that it's correct we're going to need this notion of that bound on the flow value given any cut in the network.","max flow |f| = some c(S,T) cut..... not any I think. some (S,T) cut may have a larger capacity which is okay since all we need id f < any (S,T) cut. here , he just showed that there is always some (S,T) cut out there such that |f| = capacity of that (S,T) cut. did I make sense?
I don't understand why you can't do s-a-t and skip the a-b or b-a bottleneck. Thanks in advance!"
"And the probability of that is about 40%, 0.4 as you can check yourself easily.
Then it turns out that if you roll 60 times, the probability of being-- the expected number in 60 rolls is going to be 10.
So the probability of there being within 10% of 10, or nine to 11 sixes is 0.26.
And likewise, the probability of there being within 10% of 100, which is the expected number of sixes when you roll 600 times, is 0.72.
And so on until finally the probability of being within 10% of 1,000, which is the expected number when you roll 6,000 times, that is between 900 and 1,100 sixes in 6,000 rolls, is 0.999-- triple nines.
In fact, it's a little bit bigger.
So it's really only about one chance in 1,000 that your number of sixes won't fall in that interval, within 10% of the expected number.
Well, suppose I ask for a tighter tolerance and I'd like to know what's the probability of being within 5%.
Well first of all, notice of course, that as the number of rolls get larger, the probability of being in this given interval is getting higher and higher, which is what Bernoulli said and what we intuitively believe.
The more rolls, the more likely you are to be close to what you expect.
If you tighten the tolerance, of course, then the probabilities wind up getting smaller that you'll do so well.
So if you want to be within 5% of the average in six rolls, it means you still have to roll exactly one sixth, which means the probability is still 0.4.
But if you're trying to be within 5% of the expected number 10 and 60 rolls, meaning between five and 15, that probability is only 0.14 compared to the probability of 0.26 of being within 10%.",Hi Dr. Meyer. Could you comment on how you get the probability of the average = 1/6 +/- 10% ? Sorry for the silly question. Thanks you very much
"And you can also see, the size of that set's doubling each time.
Because you get to 4, I'm going to add everything in to all of those pieces-- really nice recursive description.
Let's write some code.
So I'll also hand it out to you, but here's the code.
And I'm going to walk through it carefully.
And then we're going to analyze it.
But it's actually, for me, a beautiful piece of code.
I did not write it, by the way, John did.
But it's a beautiful piece of code.
I want to generate all the subsets with a power set of some list of elements.
Here's how I'm going to do it.
I'm going to set up some internal variable called res, OK? And then, what am I going to do? Actually, I don't know why I put res in there.
I don't need it.
But we'll come back to that.
If the list is empty, length of the list is 0, I'm going to just return that solution.
And this is not a typo.
What is that funky thing there? It is a list of one element, which is the empty list, which I need.
Because the solution in this case is a set with nothing in it.
So there is the thing I return in the base case.","For the power set, wouldn't it be better to put the last element (""extra"") in a list of itself before adding it to ""small""? This way the function will work for all variables that can be indexed (tuple, list, string) instead of working only with lists. def genSubsets(L): if len(L) == 0: return [[]] else: smaller = genSubsets(L[:-1]) extra = [L[len(L)-1]] # put the last elem in its own list new = [] for small in smaller: new.append(small + extra) return smaller + new
Why is res = [] needed in the power set problem code? I don't see any reason why it needed to be there.
I don't get the exponential complexity code
I dont get this part either"
"How many times do I have to flip a coin before I get a heads? Definitely at most log n.
Now we have to do this for each secondary table.
There are m equal theta and secondary tables.
There's a slight question of how big are the secondary tables.
If one of these tables is like linear size, then I have to spend linear time for a trial.
And then I multiply that by the number of trials and also the number of different things that would be like n squared log n n.
But you know a secondary table better not have linear sides.
I mean a linear number of li equal n.
That would be bad because then li squared is n squared and we guaranteed that we had linear space.
So in fact you can prove with another Chernoff bound.
Let me put this over here.
That all the li's are pretty small.
Not constant but logarithmic.
So li is order log n with high probability for each i and therefore for all i.
So I can just change the alpha my minus 1 n to the alpha and get that for all i this happens.
In fact, the right answer is log over log log, if you want to do some really messy analysis.","Why log n trail whp?
Also didn't get that. If Pr[total space of 2nd level linear] > 1/2 (for some constant we get to choose which does not depend on the absolute value of n), then I think that we'd have Pr[#trials until we get linear space > t] < 1/2^t, that is Pr[#trials > lg s] < 1/s (with s=2^t) which is whp on s which has nothing to do with n imho. I don't see how it would depend on n. What am I missing?"
"So now, um, you know what does it mean optimizing random walk embeddings, it means finding this coordinates z, uh, used here such that this likelihood function is, uh, minimized.
Now, uh, the question is, how do we do this in practice? And the problem is that this is very expensive because if you look at this, we actually have to sum- two nested summations, uh, over all nodes of the network.
We sum over all nodes in the- of- of the network here for starting nodes of the random walks.
And here when we are normalizing softmax, we are normalizing it over all of the nodes of the network again.
So this is a double summation, which means that it will have complexity order, um, V squared.
So it will be, uh, quadratic in the number of nodes in the network, and that's prohibitively expensive.
So let me tell you what, uh, we do to make this, uh, practical.
And the- the issue here is that there is this problem with the softmax that, um, we need to sum over all the nodes to basically normalize it back to- to a distribution over the nodes.
So, um, can we approximate this theorem? And the answer is yes.
And the solution to this is called negative sampling.","I had a quick question. Do you generate the negative samples every epoch, and then use them for each node, or do you generate new negative samples for every single node you sample in every generation?
What do you mean by the negative examples in negative sampling?
 I don't think that's entirely correct, as neighbouring nodes are positive samples, similarly to word2vec where context words are ""positive samples"". Negative sampling samples from the negative examples, in this case nodes that are NOT neighbors to the central node.
To my understanding: The loss has two terms: 1, (z_u, z_v) corresponding to the positive example (neighbor and central node's similarity) 2, (z_u, z_ni where z_ni are elements of the negative samples i = 1,...,k) corresponding to the selected k negative samples. The loss can be optimized by either increasing term 1 or decreasing term 2. Now, if k is increased, the sum in term2 will have more elements and that part will have more weight, resulting in stronger emphasis on the negative samples, that is, the mapping will be adjusted so that the similarity to the negative examples are smaller. This way there will be a higher bias for negative events, as the loss will punish greater values for similar non-neigbor nodes, that it will reward similar neighbor nodes."
"Let's try this guy.
Oh.
What do you think? What happened here? Well, we're screwed, right? Because it's linearly inseparable-- bad news.
So in situations where it's linearly inseparable, the mechanism struggles, and eventually, it will just slow down and you truncate it, because it's not making any progress.
And you see the red dots there are ones that it got wrong.
So you say, well, too bad for our side-- doesn't look like it's all that good anyway.
But then, a powerful idea comes to the rescue, when stuck switch to another perspective.
So if we don't like the space that we're in, because it gives examples that are not linearly separable, then we can say, oh, shoot.
Here's our space.
Here are two points.
Here are two other points.
We can't separate them.
But if we could somehow get them into another space, maybe we can separate them, because they look like this in the other space, and they're easy to separate.
So what we need, then, is a transformation that will take us from the space we're in into a space where things are more convenient, so we're going to call that transformation phi with a vector, x.
That's the transformation.
And now, here's the reason for all the magic.
I said, that the maximization only depends on dot products.","is syscall an intrerrupt or does it invoke the shell
This is also my question, could somebody explain?
""now what are we trying to do here? i forgot""
Great lecture but why does the kernel trick work? Is there a guarantee that it would work?
Maybe I am missing some thing but it seems to me that the utility of this technique requires that you have a transformation that pushes non-linear data onto another dimension. How are these transformations found, is this by trial and error. If so then isn't this still a hill climbing exercise
Did you figure out why? I have the same question
i also do not understand the same thing. please somebody answer
First of all you don't seek transformations, you directly seek a function that gives you the dot product of 2 transformed points. So you can easily have an insane amount of dimensions which increases the likelyhood that the data is linearily seperable. as far as I understand you can also have infinitely many dimensions. If your transformation has infinitely many dimensions (fore example f(x) = [x, x, x...] the dot product of f(x) and f(y) could be a converging infinite sum! So this kernel allows you to have a lot of dimensions which seem to practically guarantee linear seperability
Could you explain at what later steps that would be convenient?"
"So basically, the idea here is that we count how many neighbors, um, the two nodes have in common, but the importance of uneven neighbor is, uh- is low, uh- decreases, uh, with these degrees.
So if you have a lot of, um, neighbors in common that have low degree, that is better than if you have a lot of high- highly connected celebrities as a set of common neighbors.
So this is a- a net- a feature that works really well in a social network.
Of course, the problem with, uh, local, um, network neighborhood overlap is the limitation is that this, uh, metric always returns zero if two- two nodes are not- do not have any, uh, neighbors in common.","It depends on the interpretation of your graph (can you befriend yourself?), but generally speaking, we set the diagonals of A to be 0 in graphs meaning there are no self loops (simple graph)."
"If I call that n, it's going to take that n times over the outer loop.
But what about n here? All of the earlier examples, we had a constant number of operations inside of the loop.
Here, we don't.
We've got another loop that's looping over in principle all the elements of the second list.
So in each iteration is going to execute the inner loop up to length of L2 times, where inside of this inner loop there is a constant number of operations.
Ah, nice.
That's the multiplicative law of orders of growth.
It says if this is order length L1.
And we're going to do that then order length of L2 times, the order of growth is a product.
And the most common or the worst case behavior is going to be when the lists are of the same length and none of the elements of L1 are in L2.
And in that case, we're going to get something that's order n squared quadratic, where n is the length of the list in terms of number of operations.
I don't really care about subsets.
I've got one more example.
We could similarly do intersection.
If I wanted to say what is the intersection of two lists? What elements are on both list 1 and list 2? Same basic idea.
Here, I've got a pair of nested loops.
I'm looping over everything in L1.
For that, I'm looping over everything in L2.
And if they are the same, I'm going to put that into a temporary variable.
Once I've done that, I need to clean things up.","shouldn't it be n plus n? N for the nested loop and n for the second loop, since it's at worst iterating over the length of the list, so n. And due to the addition principle it'd be O(n+n) = O(n)
Oh he's also counting the check again for the if condition? I don't get the second nested loop
 My understand is, the worst case scenario only cares about the term with the highest degree of polynomial. For example, if it takes 3n + n + 1000000, then the worst case scenario is still O(n) because you don't care about the constant (1000000), the terms with lower degree of polynomial (n) and the coefficient of the highest degree of polynomial (3). Let's say we have 2 lists, L1 and L2 with the same numbers in them. [1, 2, 3, 4, 5, 6,...,n], and we try to compare them. Let's say in the first loop we do one operation, and in the nested loop we do 3 operations. The first time you compare 1 in L1 to 1 in L2, it will do 3 operation since you do 3 in the nested loop. Since the comparation is True, you continue on with the first loop. The second time you compare 2 in L1 and 1 in L2, you also do 3 in the nested loop. But since the comparation is False, you continue on with the nested loop. Now you compare the number 2 in L1 and 2 in L2, and you also do 3. In total, you do 3 + 3 = 6 operations in the second loops. The 3rd loop you will do 3 + 3 + 3, and so on. And so in total, you do 3 + 6 + 9 + ... + 3n = 3n(n+1), which is still O(n) because we don't care about the number 3 and the term n behind.
How come 3**n is the dominant term ? Excuse my ignorance, it would be helpful if someone can clarify.
There are two loops in second loop. First for each 'e' in tmp[] and then for that each 'e' it is checked if 'e' is already in res[]. So for two loops the second one is also quadratic.
top coder, I know that there is two loops in the second half of the code, in which ""e in res"" is an implicit loop. But when the teacher says ""I'm only looping over tmp, which is at most going to be len(L1) long"", that is false. To prove my point, imagine L1 and L2 to be filled with all 1's. (L1 = [1,1,1,1,1], L2 = [1,1,1,1,1]). In that case, the line if e1 == e2 will always be true and execute with every iteration of its inner loop, thus making tmp the size of len(L1)*len(L2) (and if len(L1)==len(L2) then tmp will be len(L1)^2). I agree though that in such case O(n^2) is still applicable to the lower code half aswell as all of the code."
"OK, that's a nice clean answer.
But we got it in this way that doesn't really give much intuition.
So let's look at another naive way to derive the expected time to the first head, without having to worry about generating functions, and all that sophisticated stuff about series-- which is easy to forget.
So let's look at the outcome tree that describes this experiment of flipping until you get the first head.
So starting at the root, with probability, p, you flip a head immediately and you stop.
Or, with probability, q, you flip a tail, and then with probability, p, you finally flip the head and stop.",How did you get 1/p from p + (1+E(f))*q ? I keep getting 1/q
"So I've listed up there six L's, five forks, four T's, and three arrows.
Let's see if I can figure out why there are those 18 and nothing else.
Well, if we have three vertexes coming together, that means there are eight octants, right? And the stuff of the object may fill 1, 2, 3, 4, 5, 6, 7 or all eight octants.
Now of course, if you fill all eight octants, there's no junction.
So we don't consider that case.
If we don't fill any the octants, there's no junction.
There's no vertex.
So we don't consider that case.
But if just one of the eight octants is filled with stuff, then we can look at it from any of the seven remaining octants.",why cant exist a arrow with 2 bondaries and 1 concave? at 19.:18 you can see in this perspective
"You get that sort of vector from your learned matrix, and you give it the same thing in both cases.
And so, what we're going to be doing today, is actually not conceptually all that different from training Word2vec.
Word2vec training you can think of as pretraining just a very simple model that only assigns an individual vector to each unique word type, each unique element in your vocabulary.
Today, we'll be going a lot further than that.
But the idea is very similar.
So back in 2017, we would start with pretrained word embeddings.
And again, remember, no context there.
So you give a word an embedding independent of the context that it shows up in.
And then, you learn how to incorporate the context.
It's not like our NLP models never used context, right? Instead, you would learn to incorporate the context using your LSTM, or if it's later in 2017, you know, your transformer.",It's unclear whether the pretrained models are working based on Word2Vec or One-Hot-Encoding
"Depth first-- following my way down this path.
So let's write the code for-- yes ma'am? AUDIENCE: Pardon me.
Is the choice of depth first node we go down, is that random? PROFESSOR: The question is, which node do I, or which edge do I choose? It's however I stored it in the system.
So since it's a list, I'm going to just make that choice.
I could have other ways of deciding it.
But think of it as, yeah, essentially random, which one I would pick.
OK, let's look at the code.
Don't panic.
It's not as bad as it looks.
It actually just captures that idea.
Ignore for the moment this down here.",Theres one issue with the code that is given. Nowhere in the lecture notes or in the video defines the printPath() function. Also how does he print out in that format when the only way to do it is by calling on the Edge class method to print? especially when he is appending nodes and not edges. I am guessing it is done in the printPath() function
"So one natural way, you can think of this as a recursion, which gives you a recursive algorithm.
So I wrote-- but didn't write it.
But I could have written size of node equals this-- size of node.left plus-- and that would give you a linear time algorithm to count the size.
And if you don't have any information, that is what you would do.
And that would be very painful.
So that would make this algorithm really slow.
If I'm calling size as a recursive function, it's bad.
What I'm instead doing is storing the sizes on every single node and pre-computing this.
So in fact, I'm going to define the size of node in-- so this is the definition mathematically.
But the algorithm for this function is just going to be return node.size.
So that's constant time.
So the challenge now is I have to keep these sizes up to date, no matter what I do to the tree.
And you could look back at last lecture and see, OK, what were all the changes that I did in a tree? We only did changes during insert and delete.","Question: If height can be the property that can be stored in each node , then why not depth can be stored ? It's written that Index and depth we can't store in node bcz they are global properties"
"So that's the two parts of the lecture today.
But first, let's talk about how do we set up this graph generation task as a machine learning task? So we are going to proceed in the following way.
We are going to assume that the graphs are sampled from this p data distribution.
So basically nature is sampling from p data and is giving us graphs.
Our goal will be to learn a distribution p model and then be able to learn how to sample from this p model.
So basically, given the input data, we are going to learn a probability distribution p model over the graphs, and then we are going to sample new graphs from that probability distribution.","Thanks a lot for the lecture! Is it possible to generate a very regular topologically weighted graph? I mean the mesh, for example polygonal geometry."
"Many people think of computer learning as involving leading some neural net to submission with thousands of trials.
Programs were written in the early dawn age that learned that an arch is something that has to have the flat part on top, and the two sides can't touch, and the top may or may not be a wedge.
In the late dawn age, though, the most important thing, perhaps, was what you look at with me on Wednesday next.
It's a rule-based expert systems.
And a program was written at Stanford that did diagnosis of bacterial infections of the blood.
It turned out to do it better than most doctors, most general practitioners.
It was never used, curiously enough.",what math and computer science courses are prerequisites for this?
"It's a child of this node.
So this node has two elements, so it's being divided-- dividing the interval up into three parts.
So it's in between 10 and 17 is the point here.
AUDIENCE: So then this node has five children? PROFESSOR: Sorry? No, it has three children.
So don't think of every key as a node.
Think of the whole unit as a node.
So it's not necessarily-- in a binary search tree, you have one element, but here every node has multiple elements.
That's the point of it.
Anyone else? OK, let's start with searching.
So let's leave this here.
Well, you have the formulas up there, so that's good.","What is the meaning of the R in the playlist? Is R1 available?
I could not follow quite well when he said why use B Trees over BSTs. Can someone please help me understand it?.
In the Cormen textbook, section 18.1, it is written that if a node has 2*B-1 keys then it is called ""full node"" and once the # keys become 2*B then split occurs. But in this lecture, split occurs at 2*B-1 itself as here max # keys should be strictly less than 2*B-1. So practically which one to prefer is the confusion here 
it is a good lecture for clearing the concept of B tree. But I have a doubt that value of maximum keys in a node is less than 2*B-1 or less or equal to 2*B-1 OpenCourseWare.
can someone explain why in the last bit we chose to rotate 17 on the left over 30 on the right? thxxxxx in advance!
I think the reason to use these trees is balancing! Near the beginning, he asks ""Why use B-trees over Binary Search Trees"". He then explains something about avoiding reading memory from disk. This is not the reason I was taught in my class. The reason I was taught, is that these trees will stay balanced. With a Binary Search Tree, the algorithm of insert could result in the tree being extremely deep on one branch and very shallow on others. This means you will not actually get log(n) performance, since most of the data is in longer branches.
B is defined as the branching factor. By definition, this establishes a bound on the number of children: [B,2B) and hence a bound on the number of keys: [B-1, 2B-1]. You can what B is by taking the floor of lg(# of children in any branch)....(correct me if im wrong here...)
Any one please explain how its no of children B<= No children < 2B . When B=10 it can have macimum of 11 children . then ???
isnt his definition wrong though? in CLRS 3rd edition page 489 the definition for internal nodes of degree t is: t <= num children <= 2t
so how can it adjust 19 children in it there
Why log n? shouldn't it be log n base 3"
"And what I want the computer to do is, given that characterization of output and data, I wanted that machine learning algorithm to actually produce for me a program, a program that I can then use to infer new information about things.
And that creates, if you like, a really nice loop where I can have the machine learning algorithm learn the program which I can then use to solve some other problem.
That would be really great if we could do it.
And as I suggested, that curve-fitting algorithm is a simple version of that.
It learned a model for the data, which I could then use to label any other instances of the data or predict what I would see in terms of spring displacement as I changed the masses.","Th concept is understandable but how to make the right algorithm and set the data . Im new to this . Anyone cares enough to guide me through this ?
What if the machine isn't actually learning, but carrying out many steps by rote very quickly?"
"I don't care about what its output is.
All I know is that if I call the sine function in many different places with the input I will get the same output.
And that's enough for me to reason about my code.
And so the most common ones you will see when analyzing real systems are bit-vectors to deal with integers, and logs, and pointers.
Actually, pointers are often represented with integer because you're generally not going to be doing complicated bit whittling on pointers.
Sometimes you will and then you can't use integers anymore.
So OK.
So that's all well and good.
That's what an SMT solver can do for you.
How does it actually work? What's inside it that makes it work? And SMT solvers actually rely on our ability to solve SAT problems, on our ability to take problems involving just purely Boolean constraints and Boolean variables, and telling us whether there is an assignment to these Boolean variables that is satisfiable or not.",What is the basic difference between SMT solvers and Answer Set Programming solvers? Is there a good resource that clarifies differences between ASP and SMT?
"And in fact, I've proved the base case n equals 1.
Let's keep going.
Now, in the inductive step, I'm allowed to assume that n horses have the same color, where n is any number greater than or equal to 0.
Now right here, students complain that that's not fair, because you're already assuming something false and that's absurd.
Well, yeah, it is absurd.
But that can't be the problem.
I'm just allowed to assume an induction hypothesis.
All I have to do is prove that n implies n plus 1.
Since it's absurd, there ought to be some problem with the proof.
Well, let's watch and see if there's a problem with the proof.
So again, I can assume that any set of n horses have the same color.
I have to prove that any set of n plus 1 horses have the same color.
How will I do that? Well, there's a set of n plus 1 horses, and let's consider the first n of those horses.
Now by induction hypothesis, the first n of them have the same color.
Black, maybe.
Also by induction hypothesis, the second set of n horses-- that is, all but the first horse-- have the same color.
And what that tells me is that the first and the last horse have the same color as all of the horses in the middle.
And therefore, in fact, they all have the same color.
End of proof, QED.
So, there had better be something wrong.","It's actually very simple, but in my opinion not well explained. Assume that every pair of horses has the same color. Then, you can almost immediately say that every n horses will have the same color, because if there was a horse with a different color, then there would exist a pair of horses that had different colors. But we assumed that every pair is the same. So by negation, we proved that n horses have the same color (but starting with the assumption that every pair of horses has the same color!!). Now what the explained induction proof does is exactly the same thing, but it tricks you by stating that it starts with the assumption that every set of one horse has the same color (which is true), but then it makes you jump over the more necessary assumption, that all pair of horses have to have the same color, in order for the proof to work. Thats it.
I do not understand this, please bare with me Im too stupid to be here. So you have a group of horses (group A), you pick x amount of horses from that group (group B). Group B is all the same color. You take one horse from group B, and its the same color like stated. You take a horse from group A, and it is a different color from group B. Wheres the problem/paradox here? We never said group A had to be the same color.
I still don't understand why the induction breaks down when you get two horses. I get that there's no ""middle"" horse to compare with, but I don't understand why you can't compare the two each other.
Is the issue with the base case or is it with P(n) implies P(n+1). .
The proof for the Formula for the Sum of a Geometric Series (from the last video) suffers the same flaw as this bogus proof. That is, P(0) does not imply P(1). In order for it to be a correct proof, wouldn't the previous proof require the base case to be P(2)? If not, why not?
If the group A and B have same element (middle) then it have same color but that isnt the case if n = 1.
thats my exactly my question, but if i assume that any set of n horses have the same color, but not necessarily the same color in two seperate sets, for example: we can say that the first horse is a set of n(1) horses of black color, while the the second horse is a set of n(1) horses of white color. But that's not what the proposition says so i have no clue :-)
In the case of n+1=2, we can assume the set of the first n horses have the same color, and that the set of the second n horses have the same color. But look at the proposition P(n) again: it's only claiming that all horses in that set have the same color. So while the first and last horse have the same colors within their own respective sets, that doesn't mean that the colors of those two sets are equal. Maybe all the horses in the first set were white and all the horses of the second set were black. Hopefully that helps."
"The plus just adds the signals on these two paths.
So y of n is equal to x of n.
Subtract out, because I'm multiplying by minus 1.
Delay, so I'm getting the one from before.
So this block diagram just is a symbolic representation of that difference equation.
The value of the block diagram-- we'll see several of them, but one of them is focus on signal flow path.
If you want to visualize the transformation from input to output, the block diagram can provide visual insight into what that transformation is like.
So as before, if I gave you this representation rather than the difference equation, you could still step by step and figure out the way it worked.
It's still easy.
There's one new caveat here.
We have to start the system in a state.
The state that we will usually talk about is what we will call, at rest.
At rest just means all the outputs of all the delays are initially 0.
So that specifies the state of the system at the time the signal was turned on.","The labels on the x axis are not the value of the x's but the time steps. The definition of the delta signal is that x[t]=0 for every t different from 0 so x[2]=0. The term x axis is confusing here, it would better be called time axis or just horizontal axis."
"The beauty of order log n is that there's a constant in there that you control.
That constant is d.
So you tell me that c log n is 50.
So c log n is 50.
Then what I'm going to do is I'm going to say something like, well, if I flip a coin 1,000 times, then I'm going to have an overwhelming probability that I'm going to get 50 heads.
And that's it.
That's what the Lemma says.
It says tell me what c log n is.
Give me that value.
And I will find you a d, such that by invoking Chernoff, I'm going to show you an overwhelming probability that for that d you're going to get at least c log n heads.",Any idea how can I understand how log n is derived?
"So k of x comma z is equal to 1 plus inner product of x comma z [NOISE] plus inner product of x comma z squared plus inner product of x comma zq.
This is exactly the same as phi transpose phi of z.
So this is one example of a kernel, right? Where for this particular feature map which had, you know, if x was in rd, then p is approximately order dq, right? And in order to perform an inner product or the outer product in this high dimensional feature space would require harder dq number of operations, right? Whereas over here, in the kernel form, we see that this takes order d, this takes order d, and this takes order d, right? And you sum over three such order d operations and you're still in order d.
Right? So this is a computational trick where a high dimensional feature map can be, or the dot-product between two high-dimensional feature maps can be compactly represented by a simple, most straightforward function like this.
Yes, question? [inaudible] Why does this hold? Yeah.
[inaudible] This one? Yeah.
So, there is details of that in the notes.
It's algebra, you just work through it and you can see that these two are actually exactly the same, right? I could give you some intuitions, for example, right? So one x1, x2, til xd.
And similarly, here you have say, z1, z1, z2, til zd.
You have x1 squared, z1 squared, and so on.
So if you take the dot product between these two, you get 1 plus x1z1 plus x2z2 plus so on until xdzd plus x1 squared, z1 squared, and so on.","what is z in thata kernel function?
thanks but im talking about k(x, z) are you talking the same ? isnt it like x is the rows of matrix and z is column ?"
"Sort of.
But notice that I said you should read the labels on the axes.
There is no label here.
But you can bet that the y-intercept is not 0 on this.
Because you can see how small 101.7 looks like.
So it makes the difference look bigger than it is.
Now, that's not the only funny thing about it.
I said you should look at the labels on the x-axis.
Well, they've labeled them.
But what do these things mean? Well, I looked it up, and I'll tell you what they actually mean.
People on welfare counts the number of people in a household in which at least one person is on welfare.
So if there is say, two parents, one is working and one is collecting welfare and there are four kids, that counts as six people on welfare.","Great lecture. Just a quick question : the age coefficient is very small (-0.03), but are all the features normalized before fitting logistic regression? If they are not, then age has a much bigger impact since the difference between a 20 and a 50 year old is (30x-0.03) =-0.9 which is almost twice the impact of being in the third cabin."
"Right, that I'll print the mean, which I'm rounding.
And then I'm going to give the confidence intervals, plus or minus, and I'll just take the standard deviation times 1.6 times 100, y times 100, because I'm showing you percentages.
All right so again, very straightforward code.
Just simulation, just like the ones we've been looking at.
And well, I'm just going-- I don't think I'll bother running it for you in the interest of time.
You can run it yourself.
But here's what I got when I ran it.
So when I simulated betting a pocket for 20 trials, we see that the-- of 1,000 spins each, for 1,000 spins the expected return for fair roulette happened to be 3.68%.
A bit high.
But you'll notice the confidence interval plus or minus 27 includes the actual answer, which is 0.
And we have very large confidence intervals for the other two games.
If you go way down to the bottom where I've spun, spun the wheel many more times, what we'll see is that my expected return for fair roulette is much closer to 0 than it was here.
But more importantly, my confidence interval is much smaller, 0.8.
So now I really have constrained it pretty well.
Similarly, for the other two games you will see-- maybe it's more accurate, maybe it's less accurate, but importantly the confidence interval is smaller.","Thanks Professor. I have a doubt. With a mathematical model, I have estimated aircraft parameters using the extended Kalman filter. Now I have to verify whether the proposed system is robust against uncertainties. I have estimated data and true sensor data. How can I do Monte Carlo analysis with this information? I am not using any physical model of the aircraft but I am using Kinematic relationships of the aircraft i.e Kinematic model. How to perform the Monte Carlo analysis of my FDD algorithm using MS-excel or matlab? Could you please reply me out on this?
Hello sir, How can I handle this...describe the variance reduction techniques which are used as performance measures in stochastic models
Sir, is their way to account actually experimented pattern for this random event simulation ? (like some numbers are more likely to result in this gambling event because of the biasness of the roulette). Thank you
One question about the roulette game - if your odds of winning are 1/36 but the payout is 35x your bet. So infinite spins should converge to a -100% return, no? If I am wrong, please explain how so I can understand.
Only one question how does the computer randonize numbers and is that consistantly random!!! Just asking and it may not matter that much with a large number of trials it would probable even out...
Why is the minimum number of tries required to find the return is 35?
Can someone explain when he calculated the confidence interval on the mean, why did he use the empirical rule instead of the central limit theorem? Since he has 20 means shouldnt he have divided z,alpha/2*sigma by sqrt(20)?
Can anyone make me understand the programming language and concept used at 14.00
if the +10 will not even out, what would bring about they quasi 100% confidence in a fair game yielding 0 as it approaches infinity? How can the outcome approach expected outcomes(0,4%etc) over 1m tries? Remember, they each started with a huge variance, sometimes 44%. A rare event given enough trials ought to reoucurr with a fixed frequency, that's the premise of reversion to the means. Note, a rare event not repeating is itself rare, and getting rarer still with more tries. The set of events is distributionally connected, not individually, so, because heads/tails are =, you'd expect to see that over longer sequences approaching =. So, I don't subscribe to the ""black now has higher chances"", that's almost sure fire way to lose. Rather, keep betting what you were betting, i.e. random, and the irregularity ought to even itself out.
 You're viewing 1000 flips as being meaningless in respect to greater scheme, but what i'm saying about ""gambler's fallacy"" is exactly regarding what you said i quote: ""most basic tenets of probability that independent events have no effect on each other"" and this is my point exactly! If during 1000 spins you randomly get a lot more BLACKS then yes - you don't know when ""regression to mean"" will occur, BUT!! Probability of having a random sequence ""anomaly"" within 1000 spins is different than having 26 consecutive spins ""anomaly"" within those 1000, no?? I mean even if within those 1000 came more REDS, what is the probability that within those 1000 we'll get 26 consecutive BLACKS ?
The computer program is no really random as the random samples are generated using psuedo-random generators.
Hello, thank you for your lecture on Monte Carlo. I have a question, I dont understand why it is better to run the simulation rather than just use the mean? For the roulette example, were providing the code the 1/36 probability and using a simulation to show that well win 1/36 times. But of course we know that already, and so why run the simulation? My question related to why Im researching Monte Carlo. I have a real world problem where I am trying to forecast unplanned shutdowns for machinery. I have a detailed history of unplanned shutdowns for the past 10 years. Say I have 1005 machines and I have on average 7 breakdowns per month. Can I use Monte Carlo to forecast how many machines will breakdown per month going forward? Or should I just assume Ill have 7 breakdowns per month? What would be the benefit of each? Im happy to self learn, Im hoping someone can point me in the right direction. Thank yoy
When he is talking about roulette he has 20 trial of 1000 spins how is this any different than 1 trial of 20,000 spins? Why does he make this distinction I assume he is just averaging the mean of all 20 trials anyways what is the difference?"
"That is, that for every z, P of z holds and Q of z holds.
And then I'm going to try to prove, based on that, that the consequent holds, namely that the right-hand side for all x P of x and for all y Q of y holds.
OK.
How am I going to do that? Well, so here's the formula written just to fit on the line with the concise mathematical symbols.
The upside down V means AND, and the arrow means implies.
And we want to try to prove that this is valid a little bit more carefully.
Well, the strategy, as I said, is to assume that the left-hand side holds.
Well, what's the left-hand side say? It says that for every z, Q of z holds and P of z holds.
That means that for every possible environment that assigns a value to z, Q of z and P of z both come out to be true.
Well, suppose that the environment assigns the value c to z, where c is some element of the domain.","A thoughts on this. Taking the last example, why are we replacing x with y? This is done though out the examples but it is never explained why. I must be missing the reason but it seems just as understandable to me to say not(all x.p(x)) IFF exists x.not(P(x))"
"So back propagation with a convolutional net seems to do just about as good as anything.
And while we're on the subject of an ordinary deep net, I'd like to examine a situation here where we have a deep net-- well, it's a classroom deep net.
And we'll will put five layers in there, and its job is still to do the same thing.
It's to classify an animal as a cheetah, a zebra, or a giraffe based on the height of the shadow it casts.
And as before, if it's green, that means positive.
If it's red, that means negative.
And right at the moment, we have no training.
So if we run our test samples through, the output is always a 1/2 no matter what the animal is.
All right? So what we're going to do is just going to use ordinary back prop on this, same thing as in that sample that's underneath the blackboard.
Only now we've got a lot more parameters.
We've got five columns, and each one of them has 9 or 10 neurons in it.
So let's let this one run.
Now, look at that stuff on the right.
It's all turned red.
At first I thought this was a bug in my program.",How did the professor trained the Neural Net...any idea??
"But no matter.
And if you look at what I have here, I have 36 equals 0.
So the objective value here is 0.
It matches what you had before.
But the basic solution for II, I'm going to set the non-basic values.
So what is a solution here? The non-basic values are 0.
So the solution is going to be 9, because 9 is non-basic.
x1 is now non-basic.
x2 and x3 are still basic.
And then I have 21 and 6.
And x6 now has become basic.
So the way I get this solution is simply by plugging in 0's on this aside.
And I get 9, 21, and 6, because I just plugged in 0's on the right hand side.
So that's how I got a new solution.
And if you look at the objective value for this, the objective value, you can look at this objective value simply by looking at the original problem.
And the original problem had 3x1 plus x2 plus x3 as the objective value.
And so if you go off and you see, well, that you had 0's for the other ones, but you have 3 times 9, so we have an objective value of 27.
So this flip of our pivot basically got you from an objective value of 0, while maintaining correctness, to an objective value of 27.","I couldn't get a better solution than (x1, x2, x3) = (8, 4, 0) >> 28 How can Professor Devadas get to 30?
I didnt undestand what happened! Can you explain please? Thanks. :)"
"Well, I'll cancel the d's.
And we have that the size of S is less than or equal to the size of its image.
That's the definition of a bottleneck-- The violation of that would be a bottleneck.
This says, there's no bottlenecks.
And we have proved the degree-constrained condition is sufficient to verify Hall's condition.
And by Hall's theorem, it's sufficient to guarantee that there is a match.
Now, there's a lot of graphs with matches that are not degree-constrained.
This is not an if and only if theorem.
It's just a sufficient condition that comes up often enough that it's worth mentioning.
Degree-constrained implies Hall's condition is satisfied, which implies that there's a perfect match.
So let's turn now to the general case of Hall's theorem.
And let's talk about proving Hall's theorem.
Hall's theorem says that if there's no bottleneck, then there is a match.
So let's begin by setting up a lemma that will play a crucial role.","what is S?
What is a bottleneck?
|S| <some condition> for all sets S that are a subset of L(H). Each S is some subset of the left side of the graph (as racun correctly stated), and we are considering all such subsets. Basically we want to analyze every subset S of the left side of our graph and see if it to conforms to the condition that its size is is less than or equal to the number of nodes it is connected to on the other side; |S| <= |E(S)|.
A bottleneck is where |S| > |E(S)| for some subset S  G. This violates Hall's matching condition."
"Is this the less extreme example of [INAUDIBLE]?? Could you do learning augmentations instead of just creating the density? Oh yeah, absolutely, you can have the-- question is, can you learn instead augmentations? Yeah, you can start with actual images and learn mild augmentations to the images so that you get better performance.
I think a couple of teams are working on that as a final project.
Yeah, I'm going to move on in the interest of time.
So we're going to move on to-- in this large scale optimization setting, what are some applications that people have-- previous papers have looked at? So one application is hyper-parameter optimization.","Hi Yoon-ho, I have a question. If I understand correctly, we need these approaches to overcome memory burden as well as any kind of burden coming from the large scale. Is there any reason we restrict these techniques to large-scale meta learning? I mean, arent those approaches able to be applied to any sorts of large-scale single-task machine learning?"
"So we just replaced that delete last with a decrement, and that's going to shrink the Q by 1.
It has the exact same impact as leading the last item.
But now, it's constant time, worst case not amortized.
And the result is we never actually build a dynamic array.
We just use a portion of A to do it.
So what's going to happen is we're going to absorb all the items into the priority queue, and then start kicking them out.
As we kick them out, we kick out the largest key item first, and we put it here, then the next largest, then the next largest, and so on.
The minimum item is going to be here.
And, boom, it's sorted.
This is the whole reason I did max-heaps instead of min-heaps, is that in the end, this will be a upward sorted array with the max at the end.
Because we always kick out items at the end.
We delete the max first.
So that is what's normally called heapsort.","Is my understanding correct of why we prefer to use an array over the pointers approach? Since our implementation of insert, delete (and by extension, build) all comes down to insert_last or delete_last (with a bunch of value-swapping), each of which an array can support in constant time, it makes the pointer implementation less desirable because at each node we would need to store both a value and a pointer, which would effectively make our memory allocation double that of an array. (are there any other benefits that I missed?)"
"And so this ends up being X1 transpose Theta, X2 transpose Theta, down to X_m transpose Theta, which is of course just the vector of all of the predictions of the algorithm.
And so if, um, now let me also define a vector y to be taking all of the, uh, labels from your training example, and stacking them up into a big column vector, right.
Let me define y that way.
Um, it turns out that, um, J of Theta can then be written as one-half of X Theta minus y transpose X Theta minus y.
Okay.
Um, and let me see.
Yeah.
Let me just, uh, uh, outline the proof, but I won't do this in great detail.","1.14.54 my answer is (X^T X )+(X^T ^T X)-(X^T Y)-(Y X^T) its same or my ans is wrong ?
Why do we take the transpose of each row, wouldn't it be stacking columns on top of each other?
Wonder: Is m equals to n+1 n stands for number of inputs, while the m stands for the number of the rows which includes X0 in addition."
"We've seen that together, one time, everybody remembers? So, I'll go quickly.
Uh, you have a network, here is a dog given as an input to a CNN.
The CNN assuming the constraint is that there is one animal per image was trained with a Softmax output layer and we get a probability distribution over all animals, iguana, dog, car, uh, cat and crab.
And what we want is to take the derivative of the score of dog and backpropagate it to the input to know which parts of the inputs were discriminative for this score of dog.
Does that make sense? Everybody remembers this? And so, the interesting part is that this value is the same shape as x.","Why do we have the ||x||^2, it says x should look natural, what do they mean with that? PS. That is one hell of a smart pet shop"
"Membership is so basic that there's a lot of different ways to say it.
Besides using the membership symbol x is a member of A, you can sometimes say x is an element of A, or x is in A, as well as x is a member of A.
They're all synonyms.
So for example, 7 is a member of the integers.
Z is our symbol for the integers.
2/3 is not a member of the integers because it's a fraction that's not an integer.
And on the other hand, the set Z of integers itself is a member of this three-element set consisting of the truth value T, the set of all integers, and the element 7.
So here's an example where a set can contain sets, quite big ones even, and that's fine.
That's not any problem mathematically.","Why is null set IMPROPER subset of all set?
Aint u jus repeating what the professor saying? :) What I am not understanding why u don't care? What kinda weird messy explanation is this? ...One part is false then u don't look at other one and jus say the whole thing is true. That rule comes out from some sorta bible or what... that whatever written there is true? : I perfectly understand why a null set can be assumed a subset of every set without knowing all this mess... but this kinda explanation I don't understand. Do we devise things for easier explanation or to complicate things to nonsense level? Why don't just assume null set has null element (nothing) and so any other set can be assumed to contain that... no? Isnt that how we learned it in prep school? ... Now we doing higher maths and it all goes ZOOOMMMMM! :) Isnt x+ 0 = x? that we know since elementary times.....So can be set .... Any Set + null element = Any Set ... I mean this could be another way of seeing it. "
"So if you look at that Venn diagram, and stare at it long enough, and try some things, you can say, well, there is no case where this will give a worse answer.
Or, you might end up with the conclusion that there are cases where we can arrange those circles such that the voting scheme will give an answer that's worst than an individual test, but I'm not going to tell you the answer, because I think we'll make that a quiz question.","Great lecture. I would LOVE to see an updated version of it (without having to go to Cambridge...), as much has changed over the past 10 years. For one thing, I imagine the focus would now be on gradient boosting... Anyway, I'm curious to hear people's thoughts on the implied quiz question around 8m15s. I thought about it for a few minutes, and my feeling is that as long as all of the individual models have the same classification accuracy (i.e., the sizes of the small circles are the same), ensembling can never hurt. Yes/no?"
"So the first ball was dropped once, one drop.
And this one, if in fact, the hardness coefficient was 64, you drop the second ball 63 times.
And that would be the worst case in this instance.
Now, I've skipped over one little thing.
The fact that this is the midpoint implies that this is, in fact, the correct answer.
But we do have to analyze.
I mean remember, you know, when you do analysis, you do have to cover the entire spectrum here.
So you do have to look at the case where the drop from 64 does not break.
And in that case, you're looking at 65 through 128.","What is the worst case if you have 128 balls?
How is binary search not the answer??? Drop a ball at 64. If it breaks, drop a ball at 32. If it doesn't break drop a ball at 96. Is it a rule that you have to pick an interval k? If so, I didn't pick up on that. With binary search, it the maximum drops would be log base 2 of n + 1. That would be 8 in the case of 128."
"It'll become more obvious if I write it this way.
f(V-- I'm putting a minus in here-- V minus s minus t and cap V again, right? So all I've done here is flip these two.
Skew symmetry allows me to do that.
And now look at what I have here.
I'm talking about a flow that corresponds to some-- for any vertex, I pick-- and it's not an s vertex, it's not a t vertex, so it's an intermediate vertex.
And if I look at an intermediate vertex and look at the flow that goes out to all vertices, conservation says that has to be 0, right? So that's exactly what this says.
For any u that's neither s nor t but in V, the sum has to be 0.
So this is zero, and we're done.
All right.
Oh, you-- a Frisbee? Who is that? Ah.
Here.
So that's the power of implicit summation notation.
So we could invoke these different properties.
It was fairly straightforward.
Your first example of this.
You'll probably see a few more.
All right? Any questions so far? OK.
So as you can see, as I promised, or threatened at the beginning, but followed through on my threat, we have a lot of notation, a lot of baggage here before we get to algorithms.","How does f(u, v) work in skew symmetry when (u,v) are not connected by an edge?"
"The details may vary.
Some papers there's less to say about the model and there's more to say about the experiment.
So you can sort of move things around a bit, but roughly something like this.
Okay, so I now want to go on and say a bit about research and practical things that we need to do.
A lot of these things are relevant to everybody.
At any rate, there are things that you should know a little bit about.
So the very first one is finding research topics, which is sort of especially vital to custom final projects.",Could anyone please tell which research paper were they required to read for first part of the project component?
"So grid IJ equals E in the original code got replaced with this procedure that we'll talk about, which I don't want to spend a whole lot of time on, but it's essentially something in terms of details.
But it's essentially something that puts in different values in the different locations.
And grid IJ equal zero is replaced by undo implications, which is cleaning up all of the incorrect guesses and incorrect implications.
And the reason the implications are incorrect-- because it came from an incorrect guess.
And so that's it.
Undo implications is trivial.
It just sets all of the implications, and I'll tell you what the data structure is in a second, but think of it as making everything zero, going back to a clean slate.","I'm wondering if the code that supports the implication function is more complicated than it needs to be? Instead of keeping that n-tuple data structure of implications (which are then reversed when an invalid position is reached), couldn't you just save a ""snapshot"" of the current grid before starting any implications, then restore the grid to that snapshot if/when needed?
Why does he use range 09 for i and j (in general, and especially in the routine to find the next empty cell) ? The positions i and j can only have values 19, and what he says about using 0 to indicate an empty cell doesnt apply. Or am I missing something?"
"So, um, we're gonna show provably that like what we did before when we were doing policy improvement, we're showing that if you pick a policy, um, pi i, that was, uh, generated by being greedy with respect to your Q function, then that was guaranteed to yield monotonic improvement, and the same thing is gonna be true here too, when you do e-greedy.
Um, so if you use sort of er, an e-greedy policy, then you can gather data such that, uh, the new policy- the new value you get, if you're optimistic with respect to that- oh, sorry, if you're greedy with respect to that, that means you're gonna get any better policy.","In the monotonic e-greedy Policy Improvement theorem, why do we add the (1-e)/(1-e) factor instead of just using the (1-e) that is already there? I see this step unnecessary and confusing as the original (1-e) is canceled with the unnecessary new (1-e) denominator, and it's therefore never used."
"So not something explicitly built like that comparison of weights and displacements, but actually implicit patterns in the data, and have the algorithm figure out what those patterns are, and use those to generate a program you can use to infer new data about objects, about string displacements, whatever it is you're trying to do.
OK.
So the idea then, the basic paradigm that we're going to see, is we're going to give the system some training data, some observations.
We did that last time with just the spring displacements.
We're going to then try and have a way to figure out, how do we write code, how do we write a program, a system that will infer something about the process that generated the data? And then from that, we want to be able to use that to make predictions about things we haven't seen before.
So again, I want to drive home this point.
If you think about it, the spring example fit that model.
I gave you a set of data, spatial deviations relative to mass displacements.
For different masses, how far did the spring move? I then inferred something about the underlying process.
In the first case, I said I know it's linear, but let me figure out what the actual linear equation is.
What's the spring constant associated with it? And based on that result, I got a piece of code I could use to predict new displacements.
So it's got all of those elements, training data, an inference engine, and then the ability to use that to make new predictions.",Th concept is understandable but how to make the right algorithm and set the data . Im new to this . Anyone cares enough to guide me through this ?
"Right? So, um, if we have n examples, what we want to maximize is the- for all in examples, the norm of this- this, uh, projected point.
So the norm of the projected point is basically the length from here to the projected point.
Right.
The, uh, if we max- if we find u such that the sum of the squares of all the projected lengths is maximum, then effectively we would have performed PCA.
Okay? So what we want to do is, um, find u such that 1 over n, sum i equals 1 to n, the norm of the projection of u times x squared is maximized.","While performing PCA, will the projected points (on a lower dimension) also have mean = 0? If not, how is maximizing norm equal maximizing variance?"
"What we're going to do is we're going to have a genetic algorithm that looks for an optimal value in a space.
And there's the space.
Now, you'll notice it's a bunch of contour lines, a bunch of hills in that space.
Let me show you how that space was produced.
The fitness is a function of x and y, and it's equal to the sine of some constant times x, quantity squared, times the sine of some constant y, quantity squared, e to the plus x plus y divided by some constant.
So sigma and omega there are just in there so that it kind of makes a nice picture for demonstration.
So there's a space.
And clearly, where you want to be in this space is in the upper right-hand corner.
That's the optimal value.
But we have a genetic algorithm that doesn't know anything.
All it knows how to do is mutate and cross over.","Hey guys, I might be a bit thick here, but what does the professor mean when he says near the end of the lecture -> ""We were amazed by the SPACE of solutions ... and not by the GENETIC algorithms'? Any further explanation is welcome :)
Why would i need to change the values (mutate them) to random values if i the original values were created randomly too?
because the next generation values are based on the original values, so they will be similar to the original values and might get stuck into a local maximum. You mutate them to try o get out of local maximum. Like imagine if all the random original values were 1, you would only get 1s in the next generation without the mutation.
Kinda disappointed by this lecture: 1. The lecturer said mutation is essentially hill-climbing which I agree. But he didn't explain what cross-over is and why it is important. At least he should have stressed that it was still a mystery. 2. Crediting the artificial creature program for its ""rich solution space"" rather than genetic algorithm without even justifying it is kinda irresponsible. Because that's a bold and non-trivial claim. 3. Yes, GA requires fine-tuning of parameters, in machine learning we have feature engineering which is doing the same thing. Isn't it naive to thinking an algorithm as general as GA would work well on all problem instances without feature engineering? There is no universal problem solving algorithm that works well for all problem instances (no free lunch theorem) Overall, I have the impression that the lecturer has prejudice against GA.
I watched your lecture with great interest. I'm teaching myself Python by coding a GA. Often, when selection and reproduction are discussed, the biological model of two parents are combined into one offspring. I have a different idea. Say you have a starting population of 200. You apply your fitness function to score each member and then the grim reaper function to kill the bottom half in terms of fitness. You have a population of 100 members. Why not combine each member with every other member? (think nested loops). 100 * 100 (crossover) produces 10,000 new members. apply a mutation function randomly against the population and against each cell in the DNA string. Then reduce the population by 99% by fitness back to the original level of 100. In effect producing the next generation from the top 1 percent of the current generation. Have you considered such an approach? Can you give me your opinion? Thank you!
That's the concept of genetic drift, is it not? where you're having a bottleneck effect occur every generation, and not using a natural selection based algorithm that would include 'inclusive fitness' and regular fitness to the number of offspring produced."
"If there's no bottleneck at all, then indeed, there's no bottleneck in this other part of the complement of S and the complement of E of S.
And this gives me a hook on proving Hall's theorem, because that's basically the way I'm going to split the problem into two separate matching parts.
So let's now proceed to prove that if there are no bottlenecks in a graph, then there's a perfect match.
And it's going to be by strong induction on the number of girls.
So case one is the case that we just examined, that there's a non-empty subset of girls-- a proper subset, not all the girls-- some subset of girls who-- another way to say it is there's a subset of girls that's not empty and its complement is not empty either.","what is S?
What is a bottleneck?
A bottleneck is where |S| > |E(S)| for some subset S  G. This violates Hall's matching condition."
"But we're going to do that in a much more systematic way over here.
So you kind of weigh these two ways of approaching this problem.
One of which is I'm just going to blast through the different combinations, having a giant tree structure in my head of where, you know this might imply that particular grid location grid IJ equals eight.
This might imply that grid IJ equals seven.
And there's obviously a huge number of combinations corresponding to which of these squares that I pick and what value I assign to those squares.
And then once I do that there's another set.
And this explodes on you very quickly.
And so if you just did this in a completely brutish way, there's no way your program would ever end, even on a simple puzzle.
But thanks to these rules, it turns out a fairly straightforward program that's 20 lines of code is going to solve most problems-- at least the ones that I've looked at-- in a reasonable amount of time.
And then it's just interesting from an algorithmic standpoint and an efficiency standpoint to look and see how we can take that fairly naive approach which does have some pruning but it's exhaustive and improve that.","I've been solving a lot of sudokus lately, and one thing I've noticed that this brute force + implication method doesn't account for is a puzzle with the potential for a ""uniqueness"" problem. It's entirely possible that a sudoku with a unique solution might have a valid position along the way that presents a choice where one option would eventually lead to the correct solution and one (or more) of the other options would lead to a set of solutions where you could swap/rotate certain cells' values and still have a valid position by the algorithm's standards. With this naive approach, I wonder how you could build in a feature that detects uniqueness problems and discounts them from the set of valid positions in the solving path.
Ping Pong Cup Shots: Fair enough, I was mainly asking about whether there are any conditions on how the grid should be initially filled which guarantees your grid to be solvable, rather than having to attempt to solve it explicitly.
why? Which part are you talking about? It's an honest question because I truly want to understand whether we can solve sudoku with an algorithm purely based on deductive rules, rather than search
I was just thinking... how does the program handle multiple solutions? Seems it stops with the first solution."
"Which means that the rationals, sure enough, are countable, even though they seem to be spread out all over the line.
So, again, we saw that if N cross N is countable, and there's a surj, described above, to the non non-negative rationals, so they're countable.
Well, just looking ahead a little bit, it's going to turn out that, in contrast to the rational numbers, the real numbers are not countable.
And in fact, neither are the infinite binary sequences that we saw-- there was a bijection between the infinite binary sequences and the power set of the non-negative integers.
And both of these are going to be basic examples of uncountable sets, so sets that are not countable, which we will be examining in the next lecture.
","If all positive fractions can be enumerated, then the natural numbers of the first column of the matrix 1/1, 1/2, 1/3, 1/4, ... 2/1, 2/2, 2/3, 2/4, ... 3/1, 3/2, 3/3, 3/4, ... 4/1, 4/2, 4/3, 4/4, ... 5/1, 5/2, 5/3, 5/4, ... ... can be shuffled such that they cover the whole matrix. In short, there is a permutation such that the X of the first column XOOOO... XOOOO... XOOOO... XOOOO... XOOOO... ... cover all matrix positions. But this is obviously impossible.
So its z is countablly infinite?
Susuya Juuzou I meant a strict formal definition like the von Neumann construction of N or the Peano axioms"
"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
PROFESSOR: All right, we've got some good competition here.
So let's actually work through it.
So I have a function called always sunny and it's going to take in two variables right, t1 and t2.","How do you know that first is a string instead of a tuple? Because it doesn't have parentheses?
In your case, t1 = ('cloud'), t2 = ('cold'), they are both tuples, so the answer shouldn't be ('sunny','cloudcold')?
If you look closely, you can see that the first one is t1 = ('cloud'). In Python, this makes this value a string, because there is no comma within the braces. This is similar to (1 + 2), which doesn't create a tuple either. The second one, t2, however, has a trailing comma ('cold',) - This creates a tuple. The tuples are created inline, therefore the braces are required to disambiguate the tuples from the parameters of the function call. However, the braces aren't even required to create a tuple. You could do t2 = 'cold', which would create a tuple (because of the comma). This means that the call is actually always_sunny('cloudy', ('cold', )). Therefore t1[0] gives you a c, but t2[0] gives you the whole string."
"We'll also introduce an element called an op-amp, and use that element in order to enable us to do things like modularity and abstraction from our circuits.
First, let's talk about representation.
In the general sense, when you come across a circuit diagram, you're going to see-- at the very broad level-- a bunch of elements and a bunch of connections between the elements.
Those things will form loops and nodes.
If you don't actually specify the elements, then your circuit diagram actually looks a whole lot like a block diagram.
And in fact, block diagrams and circuit diagrams are very closely related in part because block diagrams are used to model feedback systems, which frequently are implemented using circuits.
In this course, we're going to be focusing on independent sources and resistors as the two major kinds of elements that we'll use in our circuits.
We'll also use things like potentiometers, which are resistors that you can adjust, and op-amps.
And we'll look at op-amps specifically in a later video.","if the output of voltage of RL circuit is taken to be the voltage across the inductor, the circuit is said to be what
How would i2 relate to it being correlated to R3 being over the sum and for i3 visa versa? Why can you not use i2 with R2 over the sum and so on?
What would you use, Maxwell equation within time or frequency domain?"
"So that verifier is just some algorithm.
And software and hardware are basically the same thing, right? So you can convert that algorithm into a circuit that implements the algorithm.
And if I have a circuit with like ANDs and ORs and NOTs, I can convert that into a Boolean formula with ANDs, ORs, and NOTs.
Circuits and formulas are about the same.
And if I have a formula-- fun fact, although this is a little less obvious-- you can convert it into this form, an AND of triple ORs.
And once you've done that, that formula is equivalent to the original algorithm.
And the inputs to that verification algorithm, the certificate, are represented by these variables, the xi's.",Why do we need poly size certificates here ?
"And kernel methods are widely used for traditional machine learning in, uh, graph-level prediction.
And the idea is to design a kernel rather than a feature vector.
So let me tell you what is a kernel and give you a brief introduction.
So a kernel between graphs G, and G', uh, returns a real value, and measures a similarity between these two graphs, or in general, measure similarity between different data points.
Uh, kernel matrix is then a matrix where simply measures the similarity between all pairs of data points, or all pairs of graphs.
And for a kernel to be a valid kern- kernel this ma- eh, kernel matrix, uh, has to be positive semi-definite.
Which means it has to have positive eigenvalues, for exam- and- and- as a consequence, it has to be, symet- it is a symmetric, uh, ma- matrix.
And then what is also an important property of kernels, is that there exist a feature representation, Phi, such that the kernel between two graphs is simply a feature representation, uh, wa-, uh, of the first graph dot product with the feature representation of the second graph, right? So Phi of G is a vector, and Phi of G is a- is another vector, and the value of the kernel is simply a dot product of this vector representation, uh, of the two, uh, graphs.
Um, and what is sometimes nice in kernels, is that this feature representation, Phi, doesn't even need to- to be explicitly created for us to be able to compute the value, uh, of the kernel.","positive semi-definite means the eigenvalue is nonnegative, not just positive"
"So that's it.
A lot of writing here, but this should make things clear.
So let's say that we're searching for 66.
I want to trace through what the backwards path would look like, and keep that code in mind as I do this.
So I'm searching for 66, and obviously, we know how to find it.
We've done that.
But let's go backwards as to what exactly happened when we look for 66.
When we look for 66, right at this point when you see 66, where would you have come from? AUDIENCE: [INAUDIBLE] SRINIVAS DEVADAS: You'd have come from the top.
And so if you go look at what happens here, the node when it got inserted was promoted one level.
So that means that you would go up top in the backward search first.",what is a quad node?
"So nodes address, as well as entire graphs can have attributes or properties attached to them.
So for example, an edge can have a weight.
How strong is the relationship? Perhaps it can have my ranking.
It can have a type.
It can have a sign whether this is a friend-based relationship or whether it's animosity, a full distrust, let say based relationships.
Um, and edges can have di- many different types of properties, like if it's a phone call, it's, it's duration, for example.
Nodes can have properties in- if these are people, it could be age, gender, interests, location, and so on.
If a node is a, is a chemical, perhaps it is chemical mass, chemical formula and other properties of the- of the chemical could be represented as attributes of the node.",what will be the effect of losing use-case-specific information?
"So in order for us to do that, I want to first tell you about kind of how are we going to set up the problem in terms of its set up, in terms of its kind of mathematical statistical foundation.
And then I'm going to talk about what methods allow us to achieve the goal of graph generation using representation learning.
So let's talk about graph generation.
Generally, we have two tasks we will talk about in this lecture.
First, is what we will call realistic graph generation, where we want to generate the graphs that are similar to a given set of graphs.
And I'm going to define this much more rigorously in a second.
And then the second task, I'm also going to talk about is what we call goal-directed graph generation.","Thanks a lot for the lecture! Is it possible to generate a very regular topologically weighted graph? I mean the mesh, for example polygonal geometry."
"Who could? In fact, the play goes down that way, over this way, down that way, and ultimately to the 8, which is not the biggest number.
And it's not the smallest number.
It's the compromised number that's arrived at virtue of the fact that this is an adversarial situation.
So, you say to me, how much energy, how much work do you actually saved by doing this? Well, it is the case that in the optimal situation, if everything is ordered right, if God has come down and arranged your tree in just the right way, then the approximate amount of work you need to do, the approximate number of static evaluations performed, is approximately equal to 2 times b to the d over 2.
We don't care about this 2.
We care a whole lot about that 2.
That's the amount of work that's done.
It's b to the d over 2, instead of b to d.
What's that mean? Suppose that without this idea, I can go down seven levels.
How far can I go down with this idea? 14 levels.
So, it's the difference between a jerk and a world champion.
So, that, however, is only in the optimal case when God has arranged things just right.
But in practical situations, practical game situations, it appears to be the case, experimentally, that the actual number is close to this approximation for optimal arrangements.
So, you'd never not want to use alpha-beta.
It saves an amazing amount of time.
You could look at it another way.
Suppose you go down the same number of levels, how much less work do you have to do? Well, quite a bit.
The square root [INAUDIBLE], right? That's another way of looking at how it works.
So, we could go home at this point except for one problem, and that is that we pretended that the branching factor is always the same.","I find something here about alpha & betha, what if we're changing the position between 3 and 9 on the left tree...then the first cut off wouldn't happen... so the interesting thing is the alpha betha depended on the evaluation method... For example if you're doing evaluation from the right position so the cut-off will be different :D ... anyway thank you for the explanation... it's really clear
When exactly does A-B prune the most nodes?
I got thrown off a little on the alpha beta part. So at each level we when we make comparisons do we look at the values from both the min and max perspective?
Thanks for your response. You didn't get my question. How are you going to decide if a certain move is a ""somewhat good move""? Only leaf nodes can tell you what is a good move and what is a bad move. Forget about the win move at (d-5) level, think about not choosing a lose move, at (d-5)level how can you decide that a certain move isn't going to lose you the game down the road?
I was confused about it for a little bit too. From what I can understand, in our case, each node of the tree represents a board configuration. As someone else, we must have a heuristic to determine how good a particular board configuration (node) from the perspective of min and max. Lecturer mentioned that one of the possibilities for heuristics is the piece count for each player (just as an example). But obviously value of each piece is not equal, so that wouldn't be the best heuristics. But you get the idea of what could be used as a heuristic. If we didn't have any such heuristic, then your only values would be -1, 0, 1 at leaf nodes. They would represent final configurations - loss, draw, win. In this case yes, you wouldn't be able to determine values at intermediate nodes because there's nothing to tell you whether min or max are closer to winning or losing.
I wonder how much you save by using the tree of the last move as a basis for the next one, since the min player can be a human, and he might not take the branch you predicted. So the alpha beta algorithm assumes the min player will always take the option that is most in the min player's own interest, which is not always the case in computationally ""flawed"" humans."
"Which is a nondeterministic model.
In a nondeterministic model, what you can do is say instead of computing something from something you know, you could make a guess.
So you can guess one out of polynomially many options in constant time.
So normally a constant time operation, in regular models, like you add two numbers, or you do an if, that sort of thing.
Here we can make a guess.
I give the computer polynomially many options I'm interested in.
Computer's going to give me one of them.
It's going to give me a good guess.
Guess is guaranteed to be good.
And good means here that I want to get to a yes answer if I can.
So the formal statement is, if any guess would lead to a yes answer, then we get such a guess.
OK, this is weird.
And it's asymmetric.
It's biased towards yes.
And this is why we can only think about decision problems, yes or no.
You could bias towards no.
You get something else called coNP.
But we'll focus here just on NP.
So the idea is I'd really like to find a guess that leads to a yes answer.
And the machine magically gives me one if there is one.
Which means if I end up saying no, that means there was absolutely no path that would lead to a yes.
So when you get a no, you get a lot of information.
When you get a yes, you get some information.
But hey, you were lucky.
Hard to complain.
So in 006, I often call this the lucky model of computation.
That's the informal version.
But nondeterminism is what's really going on here.
So maybe it's useful to get an example.
So here's a problem we'll-- this is sort of the granddaddy of all NP-complete problems.
We'll get to completeness in a moment.","Is there anything between polynomial and exponential? Is there anything that grows faster than n^C for every C, but slower than e^(n^) for every >0?
Could someone please help me understand what did he mean by, ""Computer would guess an option from polynomially many options""? I am not able to think(and relate) of an example when an algorithm guesses while solving a decision-based problem. e.g. while determining a chess move. His next statement only adds to my confusion, ""The guess is guaranteed to be a good one"" :(
should NP be the set of decision problem that can be solved in polynomial time with Non-deterministic machine, or, the set of decision problems whose ""TRUE"" instances can be solved in polynomial time with ND machine. if the former, how is it different from co-NP
Whether something is in P or NP, that something must be a decision problem. It does not make sense to talk about whether an algo is in P or NP. For example, we can talk about the problem ""Is 73642387 a prime?"" and ask whether it's in P or NP. However, we can't talk about whether a MergeSort algo is in P or NP becus it's an algo, not a decision problem. Here's an explanation by Alon Amit regarding this confusion: https://www.quora.com/Is-finding-prime-numbers-in-P-or-NP Hope this helps :)
See what you mean. But this is not what the guy says. He is talking about a ""good guess which leads to a yes answer"", which implies that a guess is a satisfying assignment for a set of variables. There are an exponential number of such guesses. In your interpretation he should have talked about about a set of guesses, one for each variable. I stick to my point ;-) But thanks for pointing this out to me. It explains why the confusion emerged....
This may clear things up: a non-deterministic Turing machine can solve a problem with an exponential number of options in polynomial time, and it can solve a problem with polynomial options in constant time. In his definition, of NP he's referring to the later case, where a deterministic machine would have to check through each of N^k possibilities, but an non-deterministic machine could guess which of the N^k possibilities is the right one. You can think of this process as occurring at each branching of an exponential problem, where there could be up to N^k branches to check. The non-deterministic machine knows which branch to take (and makes this choice N^k times moving down the tree), whereas the deterministic one has to check every branch."
"Um, and in the same way this, well, this is also 0, right? And this is also 0 because there'll be that one term in that product over there.
And so what that means is if you train a spam classifier today using all the data you have in your email inbox so far, and if tomorrow or- or, you know, or two months from now, whenever.",No n_i is the number of words in the i'th email but the term we add to the bottom in laplace smoothing is the number of possible labels which in andrews example is the dictionary size=10'000
"And, then, you correct that path by skipping over the duplicates.
So, finally, we'll end up with [INAUDIBLE] skipping all the duplicates we'll get a 1, 2, 3, 4, 5, 6, 1.
And that's a valid cycle.
So let's call this minimum spanning tree T.
So that's your MST.
And you are removing duplicate edges, and getting a cycle, C.
So now, actually, let's take another step back.
OK, let's define our cycle, first.
So let's call this cycle, this guy is C.
And then, you're going from C, and you're deleting the duplicates, and you're getting C dash.
So now what you have is cost of C dash.",what choice of path will guarantee you the shortest path always
"If x1 is true, Mario is going to visit all of the clauses that have x1 true in them.
And then it's going to go to the x2 choice.
Then it's going to choose whether x2 is true or false, and repeat.
Or Mario decides x1 should be false.
That will satisfy all the clauses that have x1 bar in them.
And then again, we feed back into x2.
So this is why we have two inputs into the x2 gadget.
One of them is when the previous variable was true.
The other is when the previous variable was false.
The choice of x2 doesn't depend on the choice of x1.
So they feed into the same thing.
And you have to make your choice.
So far, so good.
Now the question is, what's happening in these clauses.
And then there's one other aspect, which is after you've set all of the variables, at the very end, after this last variable xn, at the very end, what we're going to do is come and go through all the clauses.
And then this is the flag.
This is where you win the level.
Sorry, I drew it backwards.
But the goal is for Martin to start here and get to here.
In order to do that, you have to be able to traverse through these clauses.
So what do the clauses look like? This is a little bit more elaborate.
So here we are.
This is a clause gadget.
So there are three ways to dip into the clause.
It's actually upside down relative to that picture, but that's not a problem.
So if Mario comes here, then he can hit the question mark from below.
And inside this question mark is an invincibility star.
And the invincibility star will come up here and just bounce around forever.
We checked.
The star will just stay there for as long as you let it sit.","can someone explain the mario and clause reduction to me, or provide a link for theory.
Why create the last crossover gadget instead of using pipes to move you where you need to be? Or was the spirit of the exercise to use the most basic implementation of Mario possible? Is the whole goomba/mushroom thing better? (Also, the final mushroom check can be a small tunnel of fire vines short enough for the hit invincibility to last, but full enough that small Mario can't pass.)
i got lost once he started drawing the wheel. can anyone explain?
See what you mean. But this is not what the guy says. He is talking about a ""good guess which leads to a yes answer"", which implies that a guess is a satisfying assignment for a set of variables. There are an exponential number of such guesses. In your interpretation he should have talked about about a set of guesses, one for each variable. I stick to my point ;-) But thanks for pointing this out to me. It explains why the confusion emerged...."
"Right.
Because you just don't know if you end up with a really bad fit to the data like this, okay? Um, so oh and- and- and the other unnatural thing about using linear regression for a classification problem is that, um, you know for a classification problem that the values are, you know, 0 or 1.
Right.
And so it outputs negative values or values even greater than 1 seems- seems strange, um.
So what I'd like to share with you now is really, probably by far the most commonly used classification algorithm ah, called logistic regression.
Now let's say the two learning algorithms I probably use the most often are linear regression and logistic regression.",Since when these these basic statistics techniques become machine learning??
"So this while loop that has condition 2, that's the innermost loop that the break is found in.
So we're going to exit out of this inner most loop here.
And we're evaluating expression c.
And notice, we're evaluating expression c, because it's-- expression c is part of the outer while loop.
It's at the same level as this one.
And these ones are part of the inner while loop.
OK.
Last thing I want to say is just a little bit of a comparison between for and while loops.
So when would you use one or the other.
This might be useful in your problem sets.
So for loops you usually use when you know the number of iterations.
While loops are very useful when, for example, you're getting user input, and user input is unpredictable.
You don't know how many times they're going to do a certain task.
For both for and while loops, you can end out of the loop early using the break.
The for loop uses this counter.
It's inherent inside the for loop.
A while loop you can use a counter in order-- you can use a while loop to count things.
But you must initialize the counter before the while loop.
And you have to remember to increment it within the loop.
Otherwise, you maybe lead to an infinite loop.
We've seen as the very first example of a for loop that the while-- the for loop could be rewritten as a while loop, but the vice versa is not necessarily true.
And the counterexample to that is just user input.
So you might not know how many times you might do a certain task.
All right.
Great.
That's all for today.
","the number you mention in parenthesis in for loop doesn't printed on the screen
for the maze game, "" n=input(""you are in a deep forest.\n****************\n <-> \n****************\nGo left or right?"")""under the while loop doesn't need any print command and it just repeats itself?
The 'if' nested in an 'if' would only be considered if the parent 'if' is true, the 'else' nested in that won't ever be considered, then why use it at all? Also if all the nested 'if's have same condition during nesting then if they are true, it'll execute them all at once, so what is the use of that? What we want is that it asks us the question and show us the forest screen until we enter a 'left', so what we want it to do is to repeat this 'if' condition again and again so that it asks us that question again and again X=input ("" left/right..?"") If x== ""right"": <Show forest screen> Else: <Show exit screen> How can we do that? I didn't understand the need for while here, because for it to ask us the same question again an again we will have to run this code again and again, and that can be done with an 'if-else' condition as well.
How come the infinite loop didnt work with 0 but worked with p
We used a while loop because we don't know how many times the user will enter right. x = input ("" left/right..?"") If x== ""right"": <Show forest screen> Else: <Show exit screen> will take the input only once. What if I want to go right twice? Let's say, you use 7 nested if statements, the program would ask the user for direction 7 times. Not more, not less. What if I want to go right 10 times? I wouldn't be able to. Notice the condition of her while loop. It doesn't matter whether I type ""right"" once or 10 or 1000 times. The output is dependent on my answer, not on the number of times I type that answer. I hope you understand."
"We check is the problem in the memo table? If so, return memo of sub-problem.
And otherwise check if it's a base case and solve it if it's a base case.
And otherwise, write the recurrence recurse via relation.
And set the memo table of the sub-problem to be one of those things.
OK, so this is the generic dynamic program.
And implicitly, I'm writing Fibonacci in that way.
And all of the dynamic programs have this implicit structure where I start with a memo table which is empty and I always just check if I'm in the memo table.
If I am, I return it.
Otherwise I compute according to this recursive relation by recursively calling f.","Thanks. Do parallelism of recursion hurts the idea of memorization, imagine in your recursion tree there is F(n-1) in two branches ,since they are running in parallel no one can use the value of the other since when the programme arrive to F(n-1) in the two branches no one of them is done ! Is there any explanation to that please"
"Here it says, ""in repeated independent tests with the same actual probability, the chance that the fraction of times the outcome differs from p converges to 0 as the number of trials goes to infinity."" So this says if I were to spin this fair roulette wheel an infinite number of times, the expected-- the return would be 0.
The real true probability from the mathematics.
Well, infinite is a lot, but a million is getting closer to infinite.
And what this says is the closer I get to infinite, the closer it will be to the true probability.
So that's why we did better with a million than with a hundred.
And if I did a 100 million, we'd do way better than I did with a million.
I want to take a minute to talk about a way this law is often misunderstood.
This is something called the gambler's fallacy.
And all you have to do is say, let's go watch a sporting event.
And you'll watch a batter strike out for the sixth consecutive time.","What's the main difference between the law of large numbers and the monte Carlo experiment? Monte Carlo is using the law of large numbers to run the experiment? Why do we need the monte Carlo experiment if we know the characteristic of the law of large numbers? Can anyone tell?
the wording of the last sentence was confusing and made it sound like the opposite of reality!  How it is written makes it sound that it's a 50% chance to get 26 consecutive reds, if the previous were 25 black...The correct statement is just to say, if you had 25 reds in a row, the 26th spin is still 50% to be red regardless of what happened previously as all spins are independent of one another (Gamblers Fallacy to think otherwise). Also how do you get 1/67,108,86*5* for a power of 2?
Sir, is their way to account actually experimented pattern for this random event simulation ? (like some numbers are more likely to result in this gambling event because of the biasness of the roulette). Thank you
Nice. Can I just argue something, there is always an example with coins but I don't think I have ever heard someone just adding a disclaimer that tossing a coin is not a random process. In theory, if you could start the tossing under the same initial conditions you would get the same outcome. So it is possible to get an infinite number of heads if you just manage to toss the coin the same way an infinite number of times (i.e. with the same initial conditions). I don't think that would be too difficult to achieve either. An example of a true random process is nuclear decay of radioactive atoms.
In the slide ""Gambler's Fallacy"" it reads at the bottom: ""Probability of 26 consecutive reds when the previous 25 rolls were red is:"" The wording is poor in my opinion. Does it mean: ""What is the probability of the next roll being red?"" Or Does it mean: ""What is the probability of the next 26 rolls being red?"" Or maybe : ""What is the probability of 26 consecutive reds occurring in the next roll if the previous 25 rolls were red?"" Based on his answer I think that the question should have read: ""What is the Probability of the next outcome being red when the last 25 outcomes where red?"" And then he goes on to talk about it being independent after this question. He didn't establish at the beginning that the outcomes were independent.
One question about the roulette game - if your odds of winning are 1/36 but the payout is 35x your bet. So infinite spins should converge to a -100% return, no? If I am wrong, please explain how so I can understand.
I think that explaining the gambler fallacy should take into account how the gambler thinks. The gambler thinks that one rare event has to compensate by another very rare event, counter to the one the gambler just experienced. In fact, the counter-event is as rare as the currently observed one, and is not likely to happen, well, because it is rare as well! What is likely to happen is a less rare event, rather than another extreme one. Would that be one of the ways to explain the gambler fallacy?
So we cannot calculate confidence interval when empirical rule does not hold?
Why is the minimum number of tries required to find the return is 35?
if the +10 will not even out, what would bring about they quasi 100% confidence in a fair game yielding 0 as it approaches infinity? How can the outcome approach expected outcomes(0,4%etc) over 1m tries? Remember, they each started with a huge variance, sometimes 44%. A rare event given enough trials ought to reoucurr with a fixed frequency, that's the premise of reversion to the means. Note, a rare event not repeating is itself rare, and getting rarer still with more tries. The set of events is distributionally connected, not individually, so, because heads/tails are =, you'd expect to see that over longer sequences approaching =. So, I don't subscribe to the ""black now has higher chances"", that's almost sure fire way to lose. Rather, keep betting what you were betting, i.e. random, and the irregularity ought to even itself out.
 You're viewing 1000 flips as being meaningless in respect to greater scheme, but what i'm saying about ""gambler's fallacy"" is exactly regarding what you said i quote: ""most basic tenets of probability that independent events have no effect on each other"" and this is my point exactly! If during 1000 spins you randomly get a lot more BLACKS then yes - you don't know when ""regression to mean"" will occur, BUT!! Probability of having a random sequence ""anomaly"" within 1000 spins is different than having 26 consecutive spins ""anomaly"" within those 1000, no?? I mean even if within those 1000 came more REDS, what is the probability that within those 1000 we'll get 26 consecutive BLACKS ?
Can someone explain about the regression to the mean? If the first 10 trials are all red, why the second 10 trials will be less extreme, i.e. fewer than 10? If according to the gambler's fallacy, the independent property, the second 10 trials should have a MEMORYLESS property, why the second 10 trials must have fewer red?
But, regarding the following example, will my results in the next semester be independent from my results in this one?
Any one figured out it had a little to do with Monte Carlo, but with fundamentals of probability?
I still don't get how regression to means is consistent with the independence of events. Isn't the fact that the first 10 spins resulted in red (extreme) affect the next 10 spins (make it less likely to be as extreme)? Can someone pls explain that?
the next toss is independent of the previous toss ;but there is a different question that can be asked :what is the probability of of x tail(heads) in a row=1/2^x .Two completely different betting strategies
Thank you Professor John guttag. You're a great teacher and reading your book --""introduction to computation and programming with python"" has been a great experience thus far. Please, if you don't mind could you clarify me on this: When is an event said to be truly random? Or better still do we have truly random events? Randomness implies causal nondeterminism and from my little knowledge that's almost nonexistent. Events that yield uncertain outcomes are better delineated as predictively nondeterministic which doesn't imply that they are random but in stead reveals the limitations of our probing instruments and/ or statistical technique in unsheathing the nature of such events and correctly predicting their future states. No event to me qualifies as random, the outcome of coin flips, die throws could sufficiently be predicted to very high degrees of accuracy if only we could be patient enough to understand the physics of the processes - the coin/ die initial position, throwing/ flipping force, air drag, et cetera. So what processes are random?
When he is talking about roulette he has 20 trial of 1000 spins how is this any different than 1 trial of 20,000 spins? Why does he make this distinction I assume he is just averaging the mean of all 20 trials anyways what is the difference?"
"And if we believe in that assumption, then the maximum likelihood principle tells us that we need to minimize the squared loss, right? And similarly over here, um, if we make a generalized, uh- if we assume that our data is from a generalized linear model where the exponential family is- is- is Gaussian, then our hypothesis should be, you know, uh- maybe I erased it.
Our hypothesis should be just Theta transpose x.
You know, which is basically the, um- um- um, linear, uh- uh, linear regression hypotheses.
And if you perform maximum likelihood expectation on your, uh, generalized linear model, then you get the squared loss as well.","So, Y(i)'s that we are given(Dependent variable) are nothing but different points (points that are determined by 'eeta') of an exponential family(Gaussian for Linear Regression). In our case h(x; teta) will be a linear combination of x values because we assumed the distribution to be Gaussian. What if, in reality Y(i) does not follow Gaussian distribution and has some deviations. Will developing a regression make sense in that case?"
"And that large tuple, I'm then assigning it to my own tuple in my program.
And then I'm just writing out-- printing out some statement here.
So I'm getting the minimum year, the maximum year, and then the number of people.
So I can show you that it works if I replace one of these names with another one that I already have in here.
So instead of writing a song about five people, she would have wrote a song about four people.
Yay, it worked.",What does that \ do after max year?
"So it probably will take me years to play enough hands to actually get a good estimate, and I don't want to do that.
So he said, well, suppose instead of playing the game, I just simulate the game on a computer.
He had no idea how to use a computer, but he had friends in high places.
And actually talked to John von Neumann, who is often viewed as the inventor of the stored program computer.
And said, John, could you do this on your fancy new ENIAC machine? And on the lower right here, you'll see a picture of the ENIAC.
It was a very large machine.
It filled a room.
And von Neumann said, sure, we could probably do it in only a few hours of computation.","Thanks Professor. I have a doubt. With a mathematical model, I have estimated aircraft parameters using the extended Kalman filter. Now I have to verify whether the proposed system is robust against uncertainties. I have estimated data and true sensor data. How can I do Monte Carlo analysis with this information? I am not using any physical model of the aircraft but I am using Kinematic relationships of the aircraft i.e Kinematic model. How to perform the Monte Carlo analysis of my FDD algorithm using MS-excel or matlab? Could you please reply me out on this?
Only one question how does the computer randonize numbers and is that consistantly random!!! Just asking and it may not matter that much with a large number of trials it would probable even out...
i have one question, for simulation we used different softwares, which method (discrete, continuous or Monte Carlo) is used in CST ( computer simulation technology)
Can anyone make me understand the programming language and concept used at 14.00
The computer program is no really random as the random samples are generated using psuedo-random generators.
What is the purpose of the MC simulation? Where is the advantage if a user implements it? Whatever advantage it serves for the user (gambler), isn't there any equal advantage to the house?"
"We can do some analysis for both of the merge sort that we're going to do now and for the tiling puzzle, once we get to that.
But that's something to keep in mind.
So merge sort is the canonical recursive divide and conquer approach.
And I'll put up code for it, but I'm not going to show it to you.
You can look at it in the website.
But I'm going to describe the algorithm to you using a trivial example.
And so I want to sort these numbers in ascending order.
So I have these four numbers that I want to sort in ascending order.
And obviously, I could do this many different ways.",the merge sort code shown only works on arrays are a length of a power of 2. Adding another condition for len(L) == 1 in mergeSort that returns a single element array and a 2 element array will take care of the general case. Otherwise you'll get stuck in an endless recursion loop since it can't handle the base case of a 3 element array.
"So the most interesting part is what happens if data attempts to exfiltrate itself.
So essentially, TaintDroid can sit at the network interfaces and they can see everything that tries to go over the network interface.
We actually look at the taint tags there and we can say if data that's trying to leave the network has one or more taint flags, then we will say no.
That data will not be allowed to go in the network.
Now what happens at that point is actually kind of application-dependent.
You could imagine that TaintDroid shows an alert to the user which says hey, somebody's trying to send your location over the network.",I'm not sure if the purpose of TaintDroid is to protect users from malicious apps or help the developer to protect his users.
"I'm not even sure why we're devoting our lecture to this, because it's clear that what we're trying to do is we're trying to take our weights and our thresholds and adjust them so as to maximize performance.
So we can make a little contour map here with a simple neural net with just two weights in it.
And maybe it looks like this-- contour map.
And at any given time we've got a particular w1 and particular w2.
And we're trying to find a better w1 and w2.
So here we are right now.
And there's the contour map.
And it's a 6034.
So what do we do? AUDIENCE: Climb.
PATRICK WINSTON: Simple matter of hill climbing, right? So we'll take a step in every direction.
If we take a step in that direction, not so hot.
That actually goes pretty bad.
These two are really ugly.
Ah, but that one-- that one takes us up the hill a little bit.
So we're done, except that I just mentioned that Hinton's neural net had 60 million parameters in it.
So we're not going to hill climb with 60 million parameters because it explodes exponentially in the number of weights you've got to deal with-- the number of steps you can take.
So this approach is computationally intractable.
Fortunately, you've all taken 1801 or the equivalent thereof.
So you have a better idea.
Instead of just taking a step in every direction, what we're going to do is we're going to take some partial derivatives.
And we're going to see what they suggest to us in terms of how we're going to get around in space.
So we might have a partial of that performance function up there with respect to w1.
And we might also take a partial derivative of that guy with respect to w2.","Is Conway's Game of Life hard to do with neural nets?
why does he say starting with the same weights, they would stay the same? They won't have the same derivative since one of them goes through an extra sigmoid function.
I don't quite get the last point: the computation with respect to width is w^2 (width squared).Can someone explain?"
"But we'll think about it as though we could.
Let's think about this matrix again.
So suppose A is this set of elements, a, b, s, t, d, e.
I'm scrambling up the alphabet on purpose, because I don't want you to get the idea that we're assuming that A is countable, that you can list all the elements of A.
I'm not assuming that.
But I'm just writing out a sample of elements of A.
And let's suppose that I was trying to get a surjection from A to the power set of A.
So suppose I have a function f that maps each of the successive elements of A to some subset of A.",how do we know that the set D is not empty?
"ROFESSOR: [INAUDIBLE].
""Thus Spake Zarathustra"" was made famous and popular by 2001.
And that is music played at this magic moment when some primate suddenly gets an idea, presumably one of our ancestors.
So how do we explain all that? We've got all of the ingredients on the table.
And today I want to talk about various ways of putting those ingredients together.
So we talked about representations, we've talked about methods, and today we're going to talk about architectures.
And by the end of the class you'll know how to put one of those things together.
Actually, no one knows how to put one of those things together.
But what you will know is about some alternatives for putting those things together so as to make something that is arguably intelligent in the same way we are.",What was the topic covered in Lecture 20??
"So it means that nodes that have higher degree, uh, will be more likely to be chosen as a negative sample.
Um, there are two considerations for picking k in practice, which means number of negative samples.
Higher values of k will give me more robust estimate.
Uh, but higher values of K also correspond, uh, to, uh, to more, uh, to more sampling again, to higher bias on negative events.
So what people tend to choose in practice is k between 5 and 20.
And if you think about it, this is a very small number.","I had a quick question. Do you generate the negative samples every epoch, and then use them for each node, or do you generate new negative samples for every single node you sample in every generation?
What do you mean by the negative examples in negative sampling?
 I don't think that's entirely correct, as neighbouring nodes are positive samples, similarly to word2vec where context words are ""positive samples"". Negative sampling samples from the negative examples, in this case nodes that are NOT neighbors to the central node.
To my understanding: The loss has two terms: 1, (z_u, z_v) corresponding to the positive example (neighbor and central node's similarity) 2, (z_u, z_ni where z_ni are elements of the negative samples i = 1,...,k) corresponding to the selected k negative samples. The loss can be optimized by either increasing term 1 or decreasing term 2. Now, if k is increased, the sum in term2 will have more elements and that part will have more weight, resulting in stronger emphasis on the negative samples, that is, the mapping will be adjusted so that the similarity to the negative examples are smaller. This way there will be a higher bias for negative events, as the loss will punish greater values for similar non-neigbor nodes, that it will reward similar neighbor nodes."
"So this graph is trying to determine-- this is a flow network to determine if team 5, Detroit, is eliminated.
This is just the flow network.
I'm going to have a bunch of edges.
I'm going to just draw this once and we'll be moving things around on this.
2 to 3, 1 to 4, 2 to 4, and lastly 3 to 4.
And the reason you don't see 5 here is because this is an analysis for team 5.
So the other 4 teams show up here.
They have edges going to t.
s is over here, and it's got a bunch of edges going to all of these nodes.","The capacities of inner edges (between the ""team-pairs"" layer and ""teams"" layer) are irrelevant and so are set to infinity. Each of these edges indicates the number of games a particular team won when playing against some other team (for example, flow through edge ""1-2"" -> ""1"" shows how many times Team 1 won while playing against Team 2). Clearly, the flow through each of these edges is bounded by the number of games each team-pair has to play. Since these numbers are already specified as capacities of source-edges and sink-edges, they effectively restrict max flow that can go through inner edges. Thus, there is simply no need to put additional restrictions on inner-edge capacities."
"So that motivates the idea that we can think about the left-hand circuit as some kind of a generic part.
We call that generic part a one-port.
Think about this circuit on the left-- the thing that's in the red box-- it's got at its terminals-- the terminals are the things that poke through the red box.
First off it has two terminals, two terminals is just like all of our other parts.
And it's like a resistor, it's like a voltage source, it's like a current source, it's a two terminal device.
And just like a voltage source or a resistor or a current source, there's some voltage across those terminals, and there's some current that flows in those terminals.
So there's some current that goes in the plus.
And that same current comes out the minus.
That'll be true for this circuit, just the same as it's true for a resistor or voltage source or anything else, the only difference is, the thing that's inside the box-- the thing that's inside the one-port-- is more complicated than it was for a simple resistor or voltage source or current source.
So what we can think about, is this whole red box just looks like a super part.
So the interesting thing that happens is, this red box behaves like a one-port, like a super part.
And just like a resistor has a relationship between V and I, V equals IR, Or a voltage source has some kind of a constraint, V equals V0.",I didn't quite get how is current in second branch is 4A. Can someone explain it or give a resoure to figure that out?
"So here, you can see that most of the probability mass is on the first word.
And that kinda makes sense because our first word essentially means ""he"" and, uh, were gonna be producing the word ""he"" first in our target sentence.
So once we've got this attention distribution, uh, we're going to use it to produce something called the attention output.
So the idea is that the attention output is a weighted sum of the encoder hidden states and the weighting is the attention distribution.
So I've got these dotted arrows that go from the attention distribution to the attention output, probably there should be dotted arrows also from the encoder RNN but that's hard to depict.
[NOISE] But the idea is that you're summing up these encoder RNN, uh, hidden states, [NOISE] but you're gonna weight each one according to how much attention distribution it has on them.
So this means that your attention output which is a single vector is going to be mostly containing information from the hidden states that had high attention.
In this case, it's gonna be mostly information from the first hidden state.
So after you do this, you're going to use the attention output to influence your prediction of the next word.
So what you usually do is you concatenate the attention output with your decoder hidden state and then, uh, use that kind of concatenated pair in the way you would have used the decoder [NOISE] hidden state alone before.","Number of encoders is fixed, and inputs are given directly to encoders, then how variable length of sentences is taken care? Also only after traversing through the whole input sentence, the vector is passed to decoder network, in this case how attention mechanism is captured if size of encoder is fixed but sentence is of variable length??"
"Because initially everything is none.
But when we're inserting, we never empty a structure.
Now we're doing delete.
This is the one situation when v dot min becomes none from scratch.
In that case, no recursive calls.
So that means this algorithm is efficient.
Because if I had to delete from the summary structure, this only had a single item, which is this situation.
Then I just set v dot min equals to none.
And I'm done.
So this will, overall, run in log log u time.
Now, it could be we're deleting the min, but it was not the only item.
So that's this situation.
In that situation, we want to find out what the min actually is.","recursion in insert operation is not necessary since we can do it in constant time, I don't understand why he use recursion during insertion operation
here's my implementation in python. I am thinking that space complexity can be further reduced. But right now I am not able to do it. https://github.com/pctablet505/van-Emde-Boas-Tree/blob/master/van%20Emde%20Boas%20Tree%20O(n(log(log(u))))%20space%20.py there were few lines missing in the delete algorithm given in the book. it missed the lines: if self.min==None: return if x<self.min or x>self.max: return
i have question.. so we are basically having 3 layer ( 1. space O(u) the leaf part/// 2. the OR operation part(middle thing O(rootu))/// 3. summary) in every tree level? or do i have just ""one"" tree level made of 3 layer i think former is right because otherwise we don't need to consider recursion time complexity..?
You could do it in constant time on the initial data structure (the bit vector) but then you wouldn't have access to a summary vector that allows you to do the successor operation in O(log log u), and to efficiently consult the summary, it needs to have a summary, and so on But that means you need to insert into the summary too, which may need to insert into its summary, etc. Thus the need for recursion. Remember that the point of this data structure is the successor operation : we already have performant data structures for the insert and delete operations."
"Dead trivial, well, that trivial is unfair, but it's very simple.
Right? I simply write something, so let me describe it, I need to say how big of tower am I moving and I'm going to label the three stacks a from, a to, and a spare.
I have a little procedure that just prints out the move for me, and then what's the solution? If it's just a stack of size one, just print the move, take it to from-- from from to to.
Otherwise, move a tower of size n minus 1 from the from spot to the spare spot, then move what's left of tower size one from to two, and then take that thing are stuck on spare and move it over to two, and I'm done.
In that code that we handed out, you'll see this code, you can run it.
I'm not going to print it out because, if I did, you are just going to say, OK, it looks like it does the right kind of thing.
Look at the code, nice and easy, and that's what we like you to do when you're given a problem.
We asked you to think about recursively.","I am so confused about the tower thing
Would inclusion in the lecture of the 'call stack' or Python's symbol table concept help explain recursion? As you recursively call the function object's return value, the frames get 'popped' off the stack (symbol table on Python I think?) Sorry, self-taught. Still learning every day.
Hold up. Nice lecture but I'm not satisfied with the solution to the Towers of Hanoi exercise: You changed the rules. That would be the obvious solution, but the premise of the challenge was that the priests could only move one disc at a time (like the river crossing problem). Isn't that the whole point?
In his Towers of Hanoi code, I can figure out the process when n==1 and n==2, but I can't figure out steps in the process when n==3 or more. I think he wants us to think of it recursively: if it is true for smaller versions of the problem, then it is true for the bigger version of the problem and the code turns out to be true when n==3
One can think n==0 as base case. Which was not considered as it is preceded by n==0 base case in the lecture. One can re-write code assuming n==0 as base case."
"And what we want to do is we want to find f of x equals to This is in general a hard and intractable problem.
Sometimes it will work, sometimes it won't work.
If it were really nasty function, if it's continuous, great.
So why does this have anything to do with what we care about? Just as an aside.
Remember your thinking here.
If you want to minimize, say, l theta and it's convex or has a nice shape, that's the same as l prime theta equals 0, sorry if that's too small, right? So if I want to minimize something, it's the same as finding the roots of its derivative, right? Assuming it's convex both shape.
So they're related clearly.
All right.
So how does this thing actually work? So the idea here is-- and probably you've seen this method at some point.","Quick question! In newtons method we find a point where lprime theta =o. But what if the equation is not quadratic which makes it not covex anymore?
thnx for the response. If so whats the point of finding a point where l prime theta = 0 if that point can be any of the local minima?"
"So this, this is some function, right? This thing is a function of the data as well as a function of the parameters theta.
And if you view this number, whatever this number is, if you view this thing as a function of the parameters holding the data fixed, then we call that the likelihood.
So if you think of the training set the data as a fixed thing, and then varying parameters theta, then I'm going to use the term likelihood.
Whereas if you view the parameters theta as fixed and maybe varying the data, I'm gonna say probability, right? So, so you hear me use- well, I'll, I'll try to be consistent.
I find I'm, I'm pretty good at being consistent but not perfect, but I'm going to try to say likelihood of the parameters, and probability of the data even though those evaluate to the same thing as just, you know, for this function, this function is a function of theta and the parameters which one are you viewing as fixed and which one are you viewing as, as variables.
So when you view this as a function of theta, I'm gonna use this term likelihood.
Uh, but- so, so hopefully you hear me say likelihood of the parameters.
Hopefully you won't hear me say likelihood of the data, right? And, and similarly, hopefully you hear me say probability of the data and not the probability of the parameters, okay? Yeah.",Can you guys help me out ? I can't get my head around likelihood of theta thing ....why this is equal to product of probabilities of Y
"If we plug in a Bernoulli, then this will turn out to be the same hypothesis that we saw in logistic regression, and so on, right.
So, uh, one way to kind of, um, visualize this is, right.
So one way to think of is, of- if this is, there is a model and there is a distribution, right.
So the model we are assuming it to be a linear model, right.
Given x, there is a learnable parameter Theta, and Theta transpose x will give you a parameter, right.
This is the model, and here is the distribution.
Now, the distribution, um, is a member of the exponential family.
And the parameter for this distribution is the output of the linear model, right.
This, this is the picture you want to have in your mind.
And the exponential family, we make, uh, depending on the data that we have.
Whether it's a, you know, whether it's, uh, a classification problem or a regression problem or a time to vent problem, you would choose an appropriate b, a and t, uh, based on the distribution of your choice, right.
So this entire thing, uh, a-and from this, you can say, uh, get the, uh, expectation of y given Eta.
And this is the same as expectation of y given Theta transpose x, right.
And this is essentially our hypothesis function.","Bernoulli distribution is for discrete random variable, PMF is defined for Discrete random variable"
"If you know how many you want, it's a good choice because it's much faster.
All right, the algorithm, again, is very simple.
This is the one that Professor Grimson briefly discussed.
You randomly choose k examples as your initial centroids.
Doesn't matter which of the examples you choose.
Then you create k clusters by assigning each example to the closest centroid, compute k new centroids by averaging the examples in each cluster.
So in the first iteration, the centroids are all examples that you started with.
But after that, they're probably not examples, because you're now taking the average of two examples, which may not correspond to any example you have.
Actually the average of N examples.
And then you just keep doing this until the centroids don't move.
Right? Once you go through one iteration where they don't move, there's no point in recomputing them again and again and again, so it is converged.","Pretty sure he is talking about the number of comparisons which need to occur to create the group, n-squared meaning the number of comparisons is on the order of the square of the number of objects to compare, and n-cubed on the order of the cube of the number of objects to compare. Sort of like big-O notation.
what is the average of examples in the same cluster?"
"Those are some things that it thinks are a school bus.
So it appears to be the case that what is triggering this school bus result is that it's seeing enough local evidence that this is not one of the other 999 classes and enough positive evidence from these local looks to conclude that it's a school bus.
So do you see any of those things? I don't.
And here you can say, OK, well, look at that baseball one.
Yeah, that looks like it's got a little bit of baseball texture in it.
So maybe what it's doing is looking at texture.","a question what does he mean by positive and negative examples?
When it guesses the wrong thing (school bus on black and yellow stripes) isn't the ""real problem"" there that it doesn't have enough data or good enough data?"
"Now, what's going to make this manageable is that this expression, the expectation of f squared when f is greater than 1, will turn out to be something that we can easily convert into a nonconditional probability, and find a value for.
So the limit that we're using here is the following.
What I'm thinking about in mean time to failure-- if I think of any function whatsoever, g of the mean time to failure.
And I'm interested in the expectation of g of f, And I'm interested in the expectation of g of f, given that f is greater than n.
That is, it's already taken n steps to get where I am.
Then the thing about the mean time to failure is that at any moment that you haven't failed, you're starting off in essentially the same situation you were at the beginning in waiting for the next failure to occur.
And the probability of failing in one more step is the same probability-- is the same p.
And the probability of you're failing in two more steps is qp, and three more steps is qqp.
The only difference is that the value of f has been shifted by n.
In the ordinary case, we start off with f equals 0 and look at the probability that we fail in one more step, two more steps.
Now we're starting off with f having the value f plus n, and asking about the probability that it fails in the next step or the next step, or the next step.
So the punchline is that the expectation of g of f, given that f is greater than n, is simply the expectation of g of f plus n.","Hi, I think the formula here was Markov's one, not Chebyshev's one Pr[R >= c * E(R) ] <= 1 / c then we will have <= 1/4"
"All these things are too small to read.
Let me make it a little bigger.
Too small to read, but that number on the right there is not the product of the probabilities, actually.
It's the sum of the logarithms of the probabilities.
They go together, right? And the reason you use this instead of the probabilities is because these numbers get so small that was a 32-bit machine, you eventually lose.
So use the log of the probabilities rather than the product of the probabilities.
You use the sum of the logs instead of the product of the probabilities.
And eventually, you hope that this thing converges on the correct interpretation.
But you know what? This thing is so flat as a space and so a large and so telephone pole-like that it's full of local maxima.","Thank you Prof.Winston for this lecture. starting minute 38 the professor shows that multiplying the probabilities of heads and tails will get us to a point where the probability of the fair coin is bigger (if we get an equal number of heads and tails). However, on each new sample this number will get smaller and smaller since we are multiplying fractions. however, later on in the demonstration (5 coins) i noticed that the probability of the right coin is getting increased till it hits 1. I didn't understand this part, if anyone can explain what happened i would appreciate it?"
"Right? Yeah, I just plugged that in.
Of course, plus our theta n term.
Now, what's the sum of this guy? Any guesses? n square? n cube? or n? OK.
It's probably a messy.
More cleanly, I can pull this B out.
It's just the sigma of j if I change my sum, decrease the range of sum by 1.
What is the sigma some of j? What order first? AUDIENCE: n square.
LING REN: n square.
OK.
Yeah, definitely n square.
But we need to be a little bit more precise than that.
So what's the coefficient before the n square? So I claim this coefficient is 3 over 8.
Can anyone see that? AUDIENCE: Why did you assume that the expected value is theta n.
LING REN: Oh, that's just a guess.
If it's wrong, we'll have to assume something else, which we'll see in the next example.
But good question.
OK.
Yeah.
Let me ask the question again.
I claim this sum is roughly 3 over 8 n square.
Can anyone see that? Any ideas? So I don't know how to calculate this term.
But I do know how to calculate sigma from 1 to n, right? This is easy.
What's that? It's half of n, n minus 1.","seems missing the probability Pr(j) term
why does the summation of j from ceiling(n/2)-1 to n-1 equal n^2 times a constant??? Is it because we're replacing the summation with the closed form of the summation?"
"And if they are correct, then I have a pretty good idea that I have in fact identified the object on the right as either an obelisk or an organ, depending on which of the model choices and the unknown choices I've selected.
So the only thing I have left to do is to demonstrate that what I said about this is true.
So I'm going to actually demonstrate that what I said about this is true using the configuration in this demonstration.
Because it's much too hard for me to remember matrix transformations for generalized rotation in three dimensions.
So here's how it's going to work.
The z-axis is going up that way.
Or, it's going to be pointing toward you.",I am just a B Tech first year student.... just a small query.... if we have orthographic projections... we take views from mutually perpendicular directions... if my coordinate system is set with axis parallel to our viewing direction then won't the computation be much easier.... view along x axis and view along y axis will always have same z coordinate and that along x and Z would have same y coordinate... so won't these condition actually give the object in 3d(I mean a 3 dimensional array with known XYZ coordinate of all vertices)... later then we can rotate and check if we can generate similar 2d images from 3d view?)....
"Okay? Cool.
Um.
All right.
Great.
So you've now seen your first learning algorithm, um, and, you know, gradient descent and linear regression is definitely still one of the most widely used learning algorithms in the world today, and if you implement this- if you, if you, if you implement this today, right, you could use this for, for some actually pretty, pretty decent purposes.
Um, now, I wanna I give this algorithm one other name.
Uh, so our gradient descent algorithm here, um, calculates this derivative by summing over your entire training set m.
And so sometimes this version of gradient descent, has another name, which is batch gradient descent.
Oops.
All right and the term batch, um, you know- and again- I think in machine learning, uh, our whole committee, we just make up names and stuff and sometimes the names aren't great.","I didn't understand the linear regression algorithm is there any way to understand it better ??
I have been wondering why we need such an algorithm when we could just derive the least squares estimators. Have you seen any research comparing the gradient descent method of selection of parameters with the typical method of deriving the least squares estimators of the coefficient parameters?"
"If batch gradient descent doesn't cost too much, I would almost always just use batch gradient descent because it's one less thing to fiddle with, right? It's just one less thing to have to worry about, uh, the parameters oscillating, but your dataset is too large that batch gradient descent becomes prohibit- prohibitively slow, then almost everyone will use, you know, stochastic gradient descent or whatever more like stochastic gradient descent, okay? All right, so, um, gradient descent, both batch gradient descent and stochastic gradient descent is an iterative algorithm meaning that you have to take multiple steps to get to, you know, get near hopefully the global optimum.
It turns out there is another algorithm, uh, and- and, um, for many other algorithms we'll talk about in this course including generalized linear models and neural networks and a few other algorithms, uh, you will have to use gradient descent and so- and so we'll see gradient descent, you know, as we develop multiple different algorithms later this quarter.",I have been wondering why we need such an algorithm when we could just derive the least squares estimators. Have you seen any research comparing the gradient descent method of selection of parameters with the typical method of deriving the least squares estimators of the coefficient parameters?
"So here's some graphs to show you how fast goes, boom.
That gives you the curves of how well the robot arm can go along a straight line, after no practice with just some stuff recorded in the memory.
And then with a couple of practice runs do give it better values amongst which to interpolate.
So I think that's pretty cool.
So simple, but yet so effective.
But you still might say, well, I don't know, it might be something that can be done in special cases.
I wonder if old Winston uses something like that when he drinks his coffee? Well we' ought to do the numbers and see if it's possible.
But I don't want to use coffee, it's the baseball season.
We're approaching the World Series.
We might as well talk about professional athletes.
So let's suppose that this is a baseball pitcher.
And I want to know how much memory I'll need to record a whole lot of pitches.
Is there a good pitcher these days? The Red Socks suck so I don't do Red Socks.
Clay Buchholz, I guess.
I don't know, some pitcher.
And what we're going to do, is we're going to say for each of these little segments were going to record 100 bytes per joint.
And we've got joints all over the place.
I don't know how many are involved in doing a baseball pitch, but let's just say we have had 100 joints.
And then we have to divide the pitch up into a bunch of segments.
So let's just say for sake of argument that there are 100 segments.
And how many pitches does a pitcher throw in a day? What? SPEAKER 2: In a day? PROF.
PATRICK WINSTON: In a day, yeah.","Can anyone please help me? 1. Regarding the Robotic Hand Solutions Table: If I understand correctly in the case of the robotic hand, we start from an empty table and drop a ball from a fixed height on the robotic hand. When the robotic hand feels the touch of the ball, we give a random blow as we record the robotic hand movements. Now, only if the robotic arm detects after X seconds that the ball has hit the surface again, it realizes that the previous movement was successful and records the movements it made for the successful result in the table for future use. I guess there is a way to calculate where on the surface the ball fell and then in case the robotic hand feels that the ball touched a region close to the area it remembers it will try the movement closest to these points in the table. Now there are a few things I do not understand: A. The ball has an angle, so that touching the same point on the board at different angles will lead to the need to use a different response, our table can only hold data of the desired point and effect and do not know the intensity of the fall of the ball or an angle, the data in the table will be destroyed or never fully filled ? B. How do we update the table? It is possible that we will drop a ball and at first when the table is empty we will try to give a random hit when the result of this is that the ball will fly to the side so we will not write anything in the table, now this case may repeat itself over and over and we will always be left with an empty table? It seems to me that I did not quite understand the professor's words and therefore I have these questions. I would be very happy if any of you could explain to me exactly what he meant by this method of solution. 2. In relation to finding properties by vector: If I understand correctly, we fill in the data we know in advance, and then when a new figure is reached, and we do not know much about it, we measure the angle it creates with the X line (the angle of the vector) and check which group is the most suitable angle. Now there is a point I do not understand. Suppose I have 2 sets of data, 1 group have data with very low Y points and very high X points and a second group having data with high X and Y points when I get a new data with a low Y and low X , the method of the vector angle will probably associate them with group 1 although it appears on paper that the point is more suitable for group 2. It seems that if we used a simple surface distribution here (as in the first case presented by the professor) we would get more accurate results than the method of pairing according to vectors angle?"
"And then they said, maybe what we ought to do is we ought to think about generalizing this guy here so that we don't care about it.
So now we don't care about that guy.
And then he went down through here saying, well, let's see when we have to stop generalizing.
Because we've screwed everything up and we can no longer keep the z sound words separated from the s sound words.
So that eventually distilled itself down to the following algorithm.
First thing they did was to collect positive and negative examples.
And there's a positive example and a negative example.
That's not enough to do it right.
But that's enough to illustrate the idea.
So the next thing they did was something that's extremely common in learning anything.
And that is to pick a positive example to start from.
It's actually not a bad idea in learning anything to start with a positive example.
So they picked a positive example and they called that a seed.","When Mr. Winston says that neural networks weren't a good approach to this problem, I guess it's because he actually tried them, and I'm very intrigued about the result. Or did he just know it wasn't a good method? In that case, how did he come to that conclusion?"
"So 0 0 0.
Um, and I should say that this alarm system you bought was, uh, is really good- really good.
So it's, um, it detects earthquakes and burglaries, uh, perfectly.
Okay.
So if there is no burglary and no earthquake, then the probability of the alarm not going off should be 1.
Right? It's perfect.
And this is, uh, the failure case which is 0, because, um, if there is a burglary- no burglary and no earthquake, the alarm shouldn't be going off.
And, um, this is- I'm not gonna bother you with the details, you can, uh, you can just fill in the rest of this.
So there is a burglary and earthquake that should be, uh, maybe someone should check them during this, right? Um, this should be a 1, this should be a 0, and this should be a 1.
Something like that? Okay? So now I've defined the local conditional distributions so remember I'm not defining the joint distribution yet.","No idea. I feel like it should just be 0.5. Both mathametically and intuitively, if there is an alarm and we have no other information, and the probability of an earthquake = probability of burgalary, then it should be equally as likely for it to be an earthquake that triggered the alarm as it is a burgalary to trigger the alarm."
"But this last line that we have here, where we have the case that the two letters are not equal is the most interesting line of code.
That's the most interesting aspect of this algorithm.
So does someone tell me what this line is going to be out here? Yeah, go ahead.
AUDIENCE: If x of L i plus 1j or L i [INAUDIBLE].
PROFESSOR: Beautiful.
That's exactly right.
So what you're going to do is you're going to say, I need to look at two different subproblems, and I need to evaluate both of these subproblems.
And the first subproblem is I'm going to-- since these two letters are different, I'm going to have to drop one of them.
And I'm going to look inside.
I'm going to say-- in this case, I'm going to drop the i-th letter, and I'm going to get L i plus 1j.
And in the second case, I'm going to drop the j-th letter, and I'm going to get ij minus 1.
And that's it.","I am not sure what you mean by sum(i,j) here. It looks to me as if the point that your and Praveen's point can be made more clearly by omitting the references to sum, like so (using C ""?"" syntax): V(i,j) = (i == j) ? v[i] : max(v[i]-V(i+1,j), v[j]-V(i,j-1))."
"So if I have this list L is equal to 9,6,0,3, sorted-- you can think of it as giving me the sorted version of L-- gives you back the sorted version of L.
So it returns a new list that's the sorted version of the input list and does not mutate L.
So it keeps L the exact same way.
So this will be replaced by the sorted version of the list, which you can assign to a variable, and then do whatever you want with it.
Like L2 is equal to sorted L, for example.
And it keeps L the same.
On the other hand, if you just want to mutate L, and you don't care about getting another copy that's sorted, you just do L.sort.","I'll make a comment on it, since for some reason this really confused me: why .sort() returns none. (If I get something wrong, feel free to correct me and I will edit this comment, I am new to coding.) The reason that list.sort() and list.reverse() return None, while sorted(list) returns a list's values, is basically that the methods are completed by different means. Code that returns None is completed ""in-place,"" which means that the original variable itself is not duplicated, only changed. Python does not want you to think that a new variable has been/could be created after running the method .sort() or .reverse() (or any other in-place method), and so it returns None. It is basically telling you that any method that is done in-place CANNOT generate a new variable because it has not duplicated the old variable. Meanwhile, sorted(list) is creating a new list entirely, and so you can assign it to a new variable name. ( sortedList = sorted(originalList) ) So basically, don't assign in-place methods to any new variables.
For the last L1, L2 question, I've tried it with the first code but got L1 = [2, 3]. Is that because Python has been updated to fix that issue??"
"Why can't we use it? AUDIENCE: Local maxima.
PATRICK WINSTON: The remark is local maxima.
And that is certainly true.
But it's not our first obstacle.
Why doesn't gradient ascent work? AUDIENCE: So you're using a step function.
PATRICK WINSTON: Ah, there's something wrong with our function.
That's right.
It's non-linear, but rather, it's discontinuous.
So gradient ascent requires a continuous space, continuous surface.
So too bad our side.
It isn't.
So what to do? Well, nobody knew what to do for 25 years.
People were screwing around with training neural nets for 25 years before Paul Werbos sadly at Harvard in 1974 gave us the answer.
And now I want to tell you what the answer is.
The first part of the answer is those thresholds are annoying.
They're just extra baggage to deal with.
What we really like instead of c being a function of xw and t was we'd like c prime to be a function f prime of x and the weights.","Is Conway's Game of Life hard to do with neural nets?
why does he say starting with the same weights, they would stay the same? They won't have the same derivative since one of them goes through an extra sigmoid function."
"And so 1/2 the time, the steps are diverging by a small fraction.
And if we think about it, 0.1 1/2 the time.
We divide it.
We get 0.05.
So at least we need to do some more analysis, but the data is pretty compelling here that it's a very good fit.
Well, let's look at the ending location.
So here you see a rather different kind of plot.
Here I'm showing that you can plot these things without connecting them by lines.
And giving them different shapes.
So what here I've said is that the masochistic drunk we're going to plot using red triangles, and the usual drunk I'm going to plot using black plus signs.
And since I'm going to plot the location at the end of many walks, it doesn't make sense to draw lines connecting everything, because all we're caring about here is the endpoints.
So since I only want the endpoints, I'm plotting them a different way.
So for example, I can write something like plot( xVals, yVals, and then if I do something like let's see.","If someone could help me, in the textbook there's another exemple of drunk: the EW Drunk, moving only in the horizontal axe (-1, 0) and (1, 0). However this drunk is also getting farther away from the origin. But why? If after n number of steps he has equal chance to step etiher W or E, wasn't he supposed to be back to the origin according to the law of big numbers? Isn't it the same as to flip n coins and count number of head and tails?
i did't understand how it became .05 ?? if any one can enplane what he divide ?"
"I think one easy way to do that is to just do the same chain of reductions, but start with Hamiltonian path instead of cycle and follow through, and pretty much every step should still be path-based.
So it's actually useful.
We'll want Hamiltonian path and grid graphs also at some point.
But what I showed was Hamiltonian cycle.
So for pretty much everywhere they seem to be the same, but it would be cool to find an example where they have different complexity.
OK.
So here's another problem.
Euclidean TSP.
Traveling salesman problem.
You're given a set of points in the plane.
Better not make it too big because this might be hard.
And I want to find a tour.
It visits every point exactly once, and has minimum total length, Euclidean length.
So the usual notion of length.
I claim this is NP-hard.
Now, this is a little scary.
Most of the problems we've been working with are very discrete.
I mean the output here is sort of discrete.
You choose a permutation on the points.
But it feels like there's almost no control here.
I mean how would I construct localized gadgets when I can just jump from any point to any other point at any time as long as that point hasn't already been used.
But it's really easy to prove this is NP-hard given what we just did.
Oh, I should say, here's a full worked out example, so this is what I should be pointing at.
This was the parity-preserving graph that we drew before.
And this is what you get in the reduction, along with actual solution, which is the one drawn in orange here.",Is this algorithm already applied to search patterns in non-deterministic cases?
"Here is the r_0, and now, we are multiplying it over and over and over again, and you know, after some number of iterations, it's going to converge, and it converges to 7 over 33, 5 over 33, and 21 over 33.
So it means that node m in this graph will be the most important, followed by y, followed by a, right? And the why m is so important is because it's kind of a spider trap, but we also are able to teleport out.
Now, if intuitively, this is, uh, you know, node m kind of collects too much importance, you can increase, uh, value of beta, and the importance of node m is going to, uh, decrease.","I have a question. Before adding teleport, the original matrix is super sparse, which makes computation cost affordable. However, after adding the teleport part, the matrix becomes dense. How do we solve the issue?
According to the slides, we solve the rank vector r via Power Iteration, we don't use sparse properties of the matrix."
"And then what the loss function looks like is-- this is a probability.
We'll just minimize the negative log probability.
So we'll take the log of this and minimize this with respect to theta.
Yeah.
Is there something over the negative examples? [INAUDIBLE]? Oh, yeah, sorry.
This is something over the negative examples.
And so yeah, great catch.
So this is over n.
And then this is z minus n.
[INAUDIBLE] Yeah, so we need to know-- during training, we need to know what the positives and negative examples are.
Yeah.
[INAUDIBLE] We don't a priori know how good of a negative they are.
But this loss function will basically take into account all of them.","Also, doesnt SimCLR use positive AND negative examples in denominator? + it uses temperature parameter in num/denominator tau. Why Stanford prof ignore this??"
"Yeah? AUDIENCE: Still going to overwrite [INAUDIBLE] as a return value.
PROFESSOR: Yeah.
So that's actually clever, right? So this is the stack frame for redirect.
I guess it actually spans all the way up here.
But what actually happens when you call getS() is that redirect makes a function call.
It actually saves its return address up here on the stack.
And then getS() starts running.
And getS() puts its own saved EBP up here.
And getS() is going to post its own variables higher up.
And then getS() is going to fill in the buffer.
So this is still problematic.
Basically, the buffer is surrounded by return initials on all sides.
Either way, you're going to be able to overflow something.
So at what point-- suppose we had a stack growing up machine.
At what point would you be able to take control of the program's execution then? Yes, and that is actually even easier in some ways.
You don't have to wait until redirect returns.
And maybe there was like, stuff that was going to mess you up like this A to i.
No.
It's actually easier, because getS() is going to overflow the buffer.
It's going to change the return address and then immediately return and immediately jump to wherever you sort of tried to construct, makes sense.
So what happens if we have a program like this that's pretty boring? There's like no real interesting code to jump to.
All you can do is get it to print different x values here.
What if you want to do something interesting that you didn't-- yeah? AUDIENCE: I mean, if you have an extra cable stack, you could put arbitrary code that, for example, executes a shell? PROFESSOR: Yeah yeah yeah.
So that's kind of clever, right, because you actually can supply other inputs, right? So at least, well-- there's some defenses against this.
And we'll go over these in subsequent lectures.
But in principle, you could have the return address here that you override on either the stack up or stack down machine.
And instead of pointing it to some existing code, like the printf inside of main, we can actually have the return address point into the buffer.
So it's previously just some location on the stack.
But you could jump there and treat it as executable.",Can anyone explain what he is doing with the code? I don't understand what he is doing. Thanks
"It was actually very tricky.
Jeremy, the TA who wrote this was very clever, and it makes it really great case study for you guys.
All right, so Rule 1, if x lives in Gryffindor tower then x is a protagonist.
By the way, for conciseness I'm going to be using GT for Gryffindor tower when I write these in later on, and I'll use SD for Slytherin dungeon.
Speaking of which, Rule 2, if x lives in Slytherin dungeon, then x is a villain, x is ambitious.
Why there are two things here? Well, after the ""if"" we have what we call antecedent, that's something that needs to be true in order for this rule to match.
After the ""then"" we have what is called the consequent, and in this case there are two consequents.
Anyone who lives in Slytherin dungeon is automatically a villain and also ambitious.
So you can think of there being sort of an ""and therefore"" for the purposes of both of those assertions would be added to the knowledge base.
Rule 3, if x is the protagonist or x is a villain, and x is ambitious, then x studies a lot.
By the way, the scope for this, just to be sure that we're clear, is this.
So we need them to be a protagonist or villain and no matter what they have to be ambitious.
Rule 4, if x studies a lot, and x is a protagonist, x becomes Hermione's friend.
And Rule 5, if x snogs y, and x lives in Gryffindor tower and y lives in Slytherin dungeon, then x has a bad term.","For the part 3 of the question 1. can we say that someone can live in 2 places at the same time as an assertion ?
In rule P2, it states: IF ?x lives in SD THEN ?x is a villain ?x is ambitious. Why does the word AND not appear there? Shouldn't it say this: IF ?x lives in SD THEN AND(?x is a villain, ?x is ambitious) Or is adjacency meaning multiple things written next to each other automatically assumed to be conjunction? If that is the case, why is AND ever used in these rules?"
"And so this kind of specific equation for what this will look like is you have your meta-training loss function, that I've just written out as l here.
And the way that you write out a typical information bottleneck is the KL divergence between your distribution over theta.
And some prior distribution, which is just kind of a standard Gaussian distribution.
And the reason why this corresponds to adding noise is Q is going to be a Gaussian distribution with a mean of theta and a variance of kind of a diagonal variance.
And so Gaussian distribution-- basically it's a sample from that Gaussian distribution.
It corresponds to adding noise to the mean of the Gaussian distribution.
And hence why you can think about it as adding noise to the variable.
And so essentially what this is going to do is it's going to try to place precedents on using information from the support set over using information from theta because using information from data is now going to be harder because now you have to use information from theta after noise has been added to theta.
And you resample the noise every time you use the meta parameters theta.
Yeah? [INAUDIBLE] to calculate, actually calculate the [INAUDIBLE] which predicts the task identifier from theta and then minimize it, so [INAUDIBLE] so for example you can actually take the labels from like, [INAUDIBLE] you know which option they're coming from, right? So you can have a loss that tries to disentangle the information? Yeah.
So it's interesting idea.
So here we're just kind of blindly trying to add noise and reduce information from theta.
Maybe we can try to do it in a slightly more targeted way by trying to explicitly say you shouldn't be able to predict the memorized information from theta.
You were suggesting that you shouldn't be able to predict the kind of the task identifier which wouldn't quite work.
The tricky part is that like in the pose example, the memory information basically corresponds to the canonical orientation of each of the meta-training objects and has it for all the objects.
And it's a-- well, I don't see any way to try to hit that information in a targeted way.
But we can also discuss it.","I didn't fully get why we couldn't maximise the mutual information between D_tr and y_ts like this: min loss_meta_training - mutual_information (D_tr, y_ts). Is it correct to say that it is better to minimise the mutual info between theta and y_ts instead because D_tr distribution is very complicated?"
"And func is going to get mapped to f, and x is going to get mapped to 2, right? So we're taking the variables in order and mapping them to those.
Func is f, and x is 2.
First thing the function does, sq, is create this variable y is equal to x squared.
So we're going to have y is equal to 4.
And then we're going to return func y.
So func of y, this is going to be-- we're just replacing the parameters f of 4, right? So now this is another function call.
We know what f is.
This program knows what f is.
f is going to be this part right here, which returns x squared.","I thought we will get error because in the definition we did not put the brackets def sq(func(),x) we put func not func()
I don't fully grasp it. I've written the code below. def sq(func, x): y = x**2 return func(y) def f(x): return x**2 calc = sq(f, 2) print(calc) And I get that if you follow the steps with sq(f, 2). You get the following: func = f (because func = f) y = 2**2 (because x = 2) y = 4 return func(4) But now func(y) = f(x). Since func(4), f is also f(4), which means that 4**2 = 16 The part where func(y) suddenly becomes f(x) is pretty confusing though.
How did the program binded func(x) to f(x), that really got me confused ):"
"So the first operation is union.
It's the set of points shown here in magenta.
It's the set of points that are in either A or B, all of them.
And so if we were defining this in terms of set-theoretic notation or predicate notation, the union symbol-- the U is the union symbol.
So A union B is defined to be those points x that are in A OR are in B.
And you can already begin to see an intimate relationship between the union operation and the propositional OR connective.
But don't confuse them.
If you apply an OR to sets, your compiler is going to give you a type error.
And if you apply union to propositional variables, your compiler is also going to give you a type error.
So let's keep the propositional operators and the set-theoretic operators separate [? and ?] clearly distinct even though they resemble each other.
All right.
Next basic operation is intersection where, again, it's the points that are both in A and B, the points in common, which are now highlighted in blue.
So the definition of A intersection B-- we use an upside-down union symbol for intersection-- it's the set of points that are in A and are in B.
So let's stop for a minute and make use of the similarity between the set-theoretic operations and the propositional operators.
Let's look at a set-theoretic identity, which I claim holds no matter what sets A, B, and C you're talking about.
And we're going to prove it by making the connection between set-theoretic operations and propositional operators.
And so let's read the thing.
It says that if you take A union the set B intersection C, that's equal to the set A union B intersected with A union C.","how about rewriting (A U B)  (A U C) as (AA) U (B  C) ? I mean that what it LHS implies... right? I mean that's what gonna happen if u calculate that... rest will fall out. 
When proving that the set identity A union (B intersect C) = (A union B) intersect (A union C) is true, is it sufficient to ""go from the LHS to the RHS"" instead of ""going from the LHS to the RHS and back to the LHS"" as shown in the video?"
"Uh, but in TransR we cannot do these compositional relations.
Um, and the reason for this is that each relation has a different space and we know how to model from the original space to the relation-specific space, but we don't know how to model between different, uh, relation-specific, uh, spaces.
So these relation-specific spaces are not naturally, uh, compositional.
So, uh, this means that the, uh, compositional relations, uh, TransR, uh, cannot, uh, do.
So, um, this was second method, uh, we discussed.
So let's now talk, uh, about the third method as well that, uh, is, uh, -that is based on what is called bilinear modeling.
Um, and so far, right, we were using, uh, the scoring function simply as a, as a distance in the embedding space.",Can we use non-linear algorithms that are more expressive and more expansive that can capture all the relationships?
"If it's an illegal URL or if they're trying to access a file they are not authorized to access, the web server is going to return an error.
But otherwise, it's going to access some files, maybe on disk, and send them back out in some sort of a reply.
So this is a hugely common picture, almost any system you look at.
What's the policy? Or what's the threat model? So this is a bit of a problem in many real world systems, namely that it's actually pretty hard to pin down what is the exact policy or threat model that we're talking about.
And this sort of imprecision or ambiguity about policies, threat models, et cetera, is what sometimes leads to security problems.","Can you provide for reference sources citing the three folded security approach (policy, tm and mechanisms)? Regarding the threat model, is there a generally accepted methodology you could mention (preferably free from product bias) specifically advised for system protection endevors ?"
"That's the first option.
Just use vertices 1 up to k minus 1.
Maybe we don't need to use k.
Or it could be that the path goes through k.
Well, it starts at u, and it goes to k, and then it goes from k to v, all using-- for simple paths, if we assume there are no negative weight cycles and you have to run Bellman-Ford to detect them-- but assuming no negative weight cycles, the shortest path from u to v through k must start by using vertices less than k and then use vertices less than k again.
And that's what I've written here.
It's from u to k using k minus 1 up to k minus 1 labels, and from k to v using labels up to k minus 1.
The cool thing here is, the min only has two terms.
So this takes constant time non-recursive work.
So this is, k is not on the shortest path.
And this is, k is on the shortest path.
Cool.
So this is constant non-recursive work.
If we jump ahead to the running time for this algorithm, we get the number of subproblems, which is v cubed, times the amount of work per subproblem, which is constant.
So that's just a v cubed algorithm.
For dense graphs, this is really good.
When E is v squared, then this is the same thing as v times E, so as good as Bellman-Ford.
It's not as good as Johnson for sparse graphs.
Sparse graphs, remember-- or, in general, with Johnson, we got v squared log n plus v.",Why bellman ford base case k is equal to 0. Shouldn't it be 0 and positive values?
"So, we had previously defined that, um, when we are talking about, like, Monte Carlo methods, et cetera, [NOISE] that we could always just look at, um, rt prime at I.
This is just equal to the return.
The return for the rest of the episode starting in time step t on episode i.
So, that, that should look very familiar to what we had seen in Monte Carlo methods, where we could always say from this state and action, what was the sum of rewards we get starting at that state and action until the end of the episode? Okay.
So, that means we can re-express the derivative [NOISE] with respect to Theta as approximately one over m sum over all of the trajectories and we're summing over, sum over all the time steps.","Why does rewriting the gradient with respect to theta of V reduce the variance? I thought the two were the same equation?
Not sure, but I think they rearranged the sum terms such that in the later notation, only the actions in the trajectory which add to the current reward are considered in expectation calculation."
"But length is cool, too.
And then of course, there are a lot of different ways that we can interact with our set.
So for instance, we could say, is this student taking 6.006? So in set language, one way to understand that is to say that the key-- each person in this classroom is associated with a key.
Does that key k exist in my set? In which case, I'll call this find function, which will give me back the item with key k or maybe null or something if it doesn't exist.
Maybe I can delete an object from my set or insert it.
Notice that these are dynamic operations, meaning that they actually edit what's inside of my set.",could anyone help me with the recitation 2 followed by exercise where Set from Sequence has to be built? I am not getting the part self.S.get_at(i).key inside the insert method. What kinds of items are we feeding to build the sequence seq() and later set from seq? Are we making custom item object with key attribute as intrinsic property? Please help me with this.
"Uh, yeah.
Okay.
[NOISE] So the optimal value when the clock is at time t is choose the action a that maximizes immediate reward plus the expected value of, you know, your future rewards when the clock has now ticked from time t to time t plus 1, you're going to state S_t plus 1 at time t plus 1, right? So, um, let's see.
So, ah, this is a pretty beefy piece of algebra to do.
Um, I think I feel like showing this full result is, I don't know, is like at the level of complexity of a, you know, typical CS 229 homework problem which is quite hard [LAUGHTER].
But let me just show the outline of how you do this derivation and why, you know, why this inductive step works.
Well, but I think you- but, but if you want you could work through the algebra details yourself at home.
Um, which is that- let me do this on the next board.
So V star_t of S_t is equal to max over a_t of the immediate reward, right? So that's the immediate reward.
And then plus the expected value with respect to S_t plus 1, is drawn from a Gaussian with mean AS_t plus Ba_t and covariance Sigma w.
Ah, so remember S_t plus 1 is equal to AS_t plus Ba_t plus W_t, where W_t is Gaussian with mean 0 and covariance Sigma w.
Right? So ah, if you choose an action a_t, then this is the distribution of the next state at time t plus 1.","max is applied for both the terms and to understand what expectation of quadratic function turns out be it is considered separately leaving out the first term. But it is true that he did not mentioned the 1st term when solved for a_t , but ig the result is when it is included"
"OK? Well, we can do the same trick with x minus.
If we've got a negative sample, then y sub i is negative.
That gives us our negative w times dot over x sub i.
But now, we take this stuff back over to the right side, and we get 1 plus b.
So that all licenses to rewrite this thing as 2 over the magnitude of w.
How did I get there? Well, I decided I was going to enforce this constraint.
I noted that the width of the street has got to be this difference vector times a unit vector.
Then, I used the constraint to plug back some values here.
And I discovered to my delight and amazement that the width of the street is 2 over the magnitude of w.
Yes, Brett? STUDENT: So your first x plus is minus b, and x minus is 1 plus b.
PATRICK WINSTON: Yeah.
STUDENT: So you're subtracting it? PATRICK WINSTON: Let's see.
If I've got a minus here, then that makes that minus, and then, the b is minus, and when I take the b over to the other side it becomes plus.","I have a question. At the start of the lecture, the professor says that we can imagine W to be a normal vector of any length and that the distance of a point from the hyperplane is the projection of any point onto W. But how can the dot product of WX be equal to the length of the projection of X onto W if W is any length? This can only work if W is a unit vector unless I'm mistaken. WX = ||W||*||X||*cos(theta) and the projection length of X onto W is ||X||*cos(theta). So WX can only equal ||X||*cos(theta) if ||W|| = 1. Otherwise ||X||*cos(theta) = WX/||W||.
thanks! Khan academy's multivariable calculus playlist helped too! But i dont understand the first derivation step though. How'd the derivative of w's magnitude turn into the vector itself? Is that a formula? Cuz when i tried I could just get to |w|d(|w|)/dw
Hi everyone, can i ask why when professor substitute for w, he used sub i and sub j when the there is only sub i after taking the derivative with respect to w. He kinda explained a little bit. But maybe because i'm not a native speaker, i kinda didn't get the intuition behind it?
very nice course. But i m confused, why min ||w|| is equal to min 1/2||w|| square. Can someone explain? thx
Can anyone please explain why we are maximizing the width here?
why did he take wx+b>=1? why 1? it could be anything right?
anyone have doubt how he got width=2/|w|,ask me i help you
Was choosing +1 and -1 arbitrary
why did he multiply it by 1/2 and square it?
I have a small question. How can we assume that the conditions for the points outside the gutters would be this - y(w.x + b) >= 1. I mean why is this term greater than equal to 1 and not some other constant.
Anyone can explain to me why we set w.x+b>=1 for positive points here instead of set it as w.x+b >=0.5 for example. Thank you in advance.
one more doubt I have when we found w vector then why we are keeping it in auxiliary function..As per langrange we should keep it in constraint
Why it is not the magnitude ?
1-b - (1+b) = -2b ....is what he did wrong??
how do we get i and j there :/ is all xi positive points and xj negative ?
I don't follow the constraints - he only retains the equality type constraints in the Lagrange's Multipliers step. This (equality) happens only for the few samples at the gutters. What about the non-gutter samples? The constraints for these are greater than or equal to zero type, hence Lagrange's method can not be used. Aren't those getting ignored therefore? What am I missing here?
thank you for replying. I get your first sentence. Just to clarify, it would be equal to 1 if b=0, right? As for your second sentence, I don't get it because he wrote less than or equal to -1. Also, if say b=5 then the first inequality is ok but the second gives -1+5<=-1 which is not true. What am I misunderstanding?
Why is width equal to the first equation?
I dont get how he is findeing the width of the street! x1- x2 =1-b-(1+b)= -2b!? correct me if i am wrong!
nice one....I understand more easily than before this...but one doubt I satisfied mathematical convenience of Yi = + or - 1. But I didn't understand mathematical convenience of 1/2 ||w|| ^2
Can anyone answer the question the student was posing around the 20 minute mark which the professor didnt answer properly? He says hes subtracting 1+b from 1-b but what he actually does is add them!
Just my thoughts here, Sir: The derivation of ||w||^2 is 2w just because of this relationship: ||w||^2 = (sqrt(w1^2 +...+wn^2))^2= w, w= w  w = w^2. So deriving with the formula zx^(z-1) can only be applied on the right side of the equation, because magnitudes result in scalar and are not well made for derivation of variables. Assume, we would have: ||w||^3 = (sqrt(w1^2 +...+w^n)(w1^2 +...+wn^2))^2= ||w|| w, w=||w||  w  w = ||w|| w^2 Then, the Derivation of this would be: ||w|| 2w. Note, that ||w|| is a scalar. So the formula zx^(z-1) is only qualifying for squares, since 3||w||^2 !=||w|| 2w. Does it make sense to you or am I missing a point here?
Isn't the projection of vector u on the vector w = (dot_product(u,w)/||w||) ? Why is he considering only the dot product? Can someone please explain it to me? If we consider value of ||w||, the width comes to be exactly 2. How can we find extremes of it? Am I interpretating anything in a wrong way?
Still don't get how (1 - b + 1 + b).w/||w|| = 2/||w|| is it because (1 - b + 1 + b) = 2 and 2*unit vector w = 2 therefore 2/||w|| ?
I don't understand it, too. How can 2*w/||w||=2/||w||. w = vector. 2*w = vector, 2*w/||w||= vector != skalar = 2/||w||. I think there must be a mistake
Before simplifying take w vector inside ((x+) - (x-)) it will look like ((w.x+) - (w.x-) ). Post that while simplying 1+b for -x(-ve) not just for x, you can check that carefully when a student asks the question he shows it. the substitution will look like (1-b+1+b) not (1-b-(1+b))
How do you specified the xi on those two edge???
In this Equation: Margin Width= ((x+) - (x-)) dot with Unit vector of W Why ((x+) - (x-)) dot with Unit vector of W = Margin Width Can anyone help me?
Why is he claiming and using the dot product to be the length of a vector projection onto another vector? Shouldn't he also divide by the other vector length??
I believe the length of the projection is w.u/|w| isn't it?
dot(x,w) ?= dot(w,x) i get lost in this part
And how do I compute 'b'?
Can anybody explain me why the differentiation of 1/2 * ||w||^2 wrt w is just w and not ||w||?
wx + b >= 1 or <=1 , this means we are assuming that perpendicular distance between the two support vectors will always be 2 , how wx+b>=0 or <=0 makes sense but >=1 and <=1 how this is gonna hold ?
Great explanation, BUT I thought we were looking for the minimization of the Lagrange equation while the Professor keeps saying ""the maximization"". P.S. I know in general we need to find the maximum width
why minimizing (|w|^2)/2 is a convenience rather than |w|?
Can someone explain me why he compares two equations against >=+1 and <=-1 where the original equation was >= 0 and <= 0? Wouldn't the sample to be positive W*Xplus + b should be >= 0 and W*Xminus + b should be <= 0? i don't get that part
at 9.33 ,he mentioned that w.x(+)+b >= 1 how exactly is that before this he mentioned it as 0 actually can anyone clarify my doubt
Why is x sub i equal to vec w dot x sub +?
I dont't understand how he uses Lagrangian multipliers! It made sense that he differentiated with respect to the different variables but then he talked about maximizing L, the lagrangian function!? i thought you use the equations you derive to solve a system of equations and get a solution. What I'm also missing are complementary slackness equations since is constraints are inequalities. I expected there to be a ton of equations, depending of the number of x's and a solution for w and b that gives you your hyperplane...
Bhullar, I can not get your logic. How come you use the value of a non-expanded variable on the same variables expanded values where if you expand the non-expanded variable, the outcome will completely change? Also, how come you say that, he did not expanded the the w(vector) in first equation and expanded it in second equation where he did say that at 16.39 ""w(vector) is a normal and what i can do is take the w and divide by the magnitude of w and that will make it unit normal "" ? So it was not unit normal, he made it unit normal by dividing with magnitude to get the width of the street. can you prove me wrong? I am just trying to understand. May be I am wrong. Also I am agree with Sebouth's last question.
Mainul Quraishi Things start to make sense when you take w to be a unit normal vector, but fall apart afterwards. Based on the ""Hesse normal form"", w.x + b = 0 is the equation of the hyperplane s.t. w is a unit normal vector. But then it's as if we calling some other vector w and substituting it.
Somewhat related, and there's a question from the audience too: We go form dot ( x_plus - x_minus, w ) / len(w) and y_i * (dot(w,x) + b) >= 1 to deducing that dot(x_plus, w) = 1-b and dot(x_minus, w) = 1+b. How does this work? I get it to 1) dot(w,x_plus)+b-1= 0 and 2) -dot(w,x_minus)=-1-b. Substituting in the first eq doesn't seem to eliminate b. What am I missing? :-/
I think we can consider that the best possible distance between positive and negative sample is taken as 2 for this particular example and the calculation is done accordingly. But the two distances from the median line are itself dependent on finding the median line(if you see it it's actually distance c(-b) thats represents the median line).We can assume the +1,-1given here as any real value >0 but will always be equal and will change according to datapoints we have .Here it's safe to assume the data sets are at a distance 2 but we are still to find the median. Hope that helps.
cuz 1*- = 1 unit of - object.. same way 1*+ = 1 units of + object... 2*- or 2*+ is not equal to one unit of -/+.. Why do you want to use 2 ? is there any reason ? because it doesn't make sense to use 2/10/1000... because you would be increasing the number of positive/negative objects that actually exist in your data which your trying to classify for... if you use 2/10 etc.. you would have to rescale your formula and divide by 1/2 or 1/10 etc later on.. and think about VC-dimensions when you have to take logs.. if you scale with 2/10 (thereby increasing the true number of objects in your data set which are to be classified).. then that would give you log2 and log10... which are not the same as log1... so that would lead to errors in classification if you don't adjust for them in your formula later on.. hope that helps
We have got historical reports and we need to find the score (whether report is effective or useful or not) based on supervised learning. While doing the supervised learning process, we have to upload the words lists that are available in the reports. We have to assign the weightage for the word lists and run through all the reports to find the number of occurrences. Now I am not able to understand how these word lists can be used to find the usefulness of the report? Based on your experience can you please explain how this word lists can be used as a supervised data in finding the score of the report?
+ranvideogamer I see he made a mistake: x(pos) = 1 - b while x(neg) = -1 - b so x(pos) - x(neg) = 1-b - ( -1-b ) = 2. I also have another question as to why it's just 2 over the length/magnitude of w, what happened to the vector w on top of the magnitude?
but why is it xplus + xminus? On the board, it says xplus - xminus. Shouldn't it be minus?
This is really helpful. Ive only got one small question, at 21 minutes why is min(w) = min (1/2 w)?
Nice Tutorial Question : 1) At the time of calculating the width of the street, why the unit vector component is multiplied ?
jupiter 6 Isn't the projection of vector u on the vector w = (dot_product(u,w)/||w||) ? Why is he considering only the dot product? Can you please explain it to me? Also, If we consider value of ||w||, the width comes to be exactly 2. How can we find extremes of it? Am I interpretating anything in a wrong way?
Haebichan Jung But why did he subtract SUM of the constraint function instead of just the function in the Langrangian?
***** beause (x+) - (x-) is not perpendicular to the median line. For calculate the width of the street, you need a perpendicular vector. When you multiple it with the unit vector, it becames perpendicular.
+Murat Bykaksu How do we know multiplying the perpendicular vector by the difference vector gets us the perpendicular width between the boundary samples? The way I'm thinking of the width between the margin is as the opposite side of a triangle. The projection of x- onto x+ is the base, and the difference vector is the hypotenuse. Is there an error in my way of thinking?
Question.. w = sum over i ( y_i * a_i * x_i ) => weights are linear combination of the samples. When prediction is perfomed, using linear svm, thanks to the linearity of the dot products, we do not need to perform the inner product among x_test with all the x_i on the ""gutt"" of the wider street, in fact sum_over_i (y_i * a_i * x_i * x_test) = sum_over_i (y_i * a_i * x_i) * x_test. We need just to precompute the first term. However, what if non linear kernel functions K(x_i,x_test) is involved, for instance gaussian kernel exp(-(|| x - y ||^2/sigma)? For prediction, I need to do product scalar with all the samples in the training set that are on the gutt of the wider street?
in Lagrangian fuction how can we detect constraint?"
"Google state-of-the-art virtual reality glasses.
And I have them right here.
Yea.
I delivered on my promise.
That's a statement of fact.
So pretend I'm a machine.
OK? I don't know anything except what you tell me.
I don't know.
I know that you tell me this statement.
I'm like, OK.
But how is someone going to win a Google Cardboard before class ends, right? That's where imperative knowledge comes in.
Imperative knowledge is the recipe, or the how-to, or the sequence of steps.
Sorry.
That's just my funny for that one.
So the sequence of steps is imperative knowledge.
If I'm a machine, you need to tell me how someone will win a Google Cardboard before class.
If I follow these steps, then technically, I should reach a conclusion.
Step one, I think we've already done that.
Whoever wanted to sign up has signed up.
Now I'm going to open my IDE.
I'm just basically being a machine and following the steps that you've told me.
The IDE that we're using in this class is called Anaconda.
I'm just scrolling down to the bottom.
Hopefully, you've installed it in problem set zero.
I've opened my IDE.
I'm going to follow the next set of instructions.","finally, they didn't answer the title question ""what is computing ?""
What are PSETS? Are they like assignments that students of the lecture received?
Video never explained: ""What is computation?""...
Did she say the 6 primitives correctly?"
"And so, a is congruent to b mod n.
So I can cancel, in that case, trivially.
And in fact, you can work out the converse implications.
The punch line of this-- well first of all, this is the cancellation rule.
You can cancel providing that the gcd of k and n is 1 if k is relatively prime to n.
So, this is the summary.
[? k is ?] cancelable mod n if and only if k has an inverse mod n, if and only if the gcd of k and n is 1, which I can restate as k is relatively prime to n.
And that's the story.
",in step a.1 equivalent to b.1 how we omit (mod n). I would guess it to be a.1 (mod n) equivalent to b.1 (mod n) (mod n). please clarify.
"But of course, that's not the way composition works.
When you apply, one function-- R to V to something, you're applying V first.
And you write it on the right.
So R o V is written like function composition where V applies first, but the logical statement, the natural way to write it, is to follow the way the picture works.
And D, Vs, and Rs get reversed.
So watch out for that confusion.
Well, I want to introduce one more relation to flesh out this example, and that'll be the teaches relation.
So the teaches relation is going to have-- as domain professors, again-- and it's codomain, subjects.
And it's simply going to tell us who's teaching what.
So here we're going to see that ARM is teaching 6.042, as you well know.","""D  inverseR(J),"" states __the set of all students__ (denoted by D) happens to be a subset of __the set of subjects that have registered students__ (denoted by inverseR(J)). It's NOT true because the set of all students (that is, D) includes/contains the element Adam but the set of subjects that have registered students (that is, inverseR(J)) does not include/contain some subject that has registered the student Adam -- D  inverseR(J) is {Jason, Joan, Yihui, Adam}  {Jason, Joan, Adam} and is False."
"And that's potentially problematic.
That can cause a bad update, where you take a huge step and you end up at a weird and bad parameter configuration.
So you think you're coming up with a-- to a steep hill to climb.
And while you want to be climbing the hill to high likelihood, that actually the gradient is so steep that you make an enormous update.
And then suddenly, your parameters are over in Iowa.
And you've lost your hill altogether.
There's also the practical difficulty that we only have so much resolution in our floating point numbers.
So if your gradient gets too steep, you start getting not a numbers in your calculations, which ruin all your hard training work.","Regarding vanishing gradients: h are not parameters - W are. Right? And for W, those gradients sum up, i.e., no vanishing. It seems unclear why dJ/dh should be important. Aren't we only interesting in updating W?"
"To do that, we're going to look at the largest factors in the runtime.
Which piece of the program takes the most time? And so in orders of growth, we are going to look for as tight as possible an upper bound on the growth as a function of the size of the input in the worst case.
Nice long definition.
Almost ready to look at some examples.
So here's the notation we're going to use.
It's called Big O notation.
I have to admit-- and John's not here today to remind me the history-- I think it comes because we used Omicron-- God knows why.
Sounds like something from Futurama.
But we used Omicron as our symbol to define this.
I'm having such good luck with bad jokes today.
You're not even wincing when I throw those things out.
But that's OK.
It's called Big O notation.
We're going to use it.
We're going to describe the rules of it.
Is this the tradition of it? It describes the worst case, because it's often the bottleneck we're after.","n^30 grows faster than 3^n. Then why is Prof taking it to be O(3^n)?
How come 3**n is the dominant term ? Excuse my ignorance, it would be helpful if someone can clarify."
"But essentially here the idea is that this vector z captures how the labels around the node of interest, uh, v are- are distributed among, uh, the neighbors of node v in the network.
Um, and there are a lot of different, uh, choices, uh, how we come up the, uh- how we can come up with this vector.
Um, so this iterative classifiers are going now to work in two steps.
In the first step or in the first phase, we are going to classify node labels based on node attributes alone.","What is the point on the feature-only classifier phi-1 ? Professor never explained this. Just train one model, Phi-2, which uses features and neighbors and use regularization. One model is enough. Use XG-Boost. You don't even need to iterate. Train on the known labels and predict on the unknown. I believe that a data scientist would never do this method because it is so unnecessary with extra programming work for no benefit.
Thank you for the lecture! I haven't come across methods such as Iterative classification before. The idea - using a separately trained classifier in an iterative non-trainable process - sounds a bit unnatural to me. My intuition tells me it's better to train the classifier inside each iteration, thus making the iterative process trainable. I suppose, that's when is going to happen in GNNs. A couple of comments about the training set. We don't need any graph structure to train phi1 classifier, therefore it can be done with traditional methods. In order to train phi2 classifier, we need labeled nodes that have all their neighbours also labeled - that's necessary to set z_v. We don't necessary need to split all graphs into training and test sets. Instead, for phi2 training we can use all the nodes with mentioned property - as it's not going to be evaluated during the iterative stage anyway (as the labels are known - and thus 'converged' already)."
"So now, I can start using these sets to make assertions about my database that can be useful to know.
So for example, if I want to say that every student is registered for some subject-- which, of course, they are-- what I would say is that D, the set of all students, is a subset of R inverse of J.
So this concise set theoretic containment statement-- d is a subset of R inverse of J-- is a slick way of writing the precise statement that says that all the students are registered for some subject.
Now, happens not to be true by the way.
Because if you look back at that example, Adam was not registered for a subject.","""D  inverseR(J),"" states __the set of all students__ (denoted by D) happens to be a subset of __the set of subjects that have registered students__ (denoted by inverseR(J)). It's NOT true because the set of all students (that is, D) includes/contains the element Adam but the set of subjects that have registered students (that is, inverseR(J)) does not include/contain some subject that has registered the student Adam -- D  inverseR(J) is {Jason, Joan, Yihui, Adam}  {Jason, Joan, Adam} and is False.
Why is the label for that group called stuDent when no course registers him?"
"e continue with our investigation of traditional machine learning approaches uh, to uh, graph level predictions.
And now we have- we are going to focus on link pre- prediction tasks and features that capture, uh, structure of links, uh, in a given, uh, network.
So the link le- level prediction tasks is the following.
The task is to predict new links based on the existing links in the network.
So this means at test time, we have to evaluate all node pairs that are not yet linked, uh, rank them, and then, uh, proclaim that the top k note pairs, as, um, predicted by our algorithm, are the links that are going to occur, uh, in the network.
And the key here is to design features for a pair of nodes.
And of course, what we can do, um, uh, as we have seen in the node level, uh, tasks, we could go and, uh, let say concatenate, uh, uh, the features of node number 1, features of the node number 2, and train a model on that type of, uh, representation.
However, that would be very, um, unsatisfactory, because, uh, many times this would, uh, lose, uh, much of important information about the relationship between the two nodes, uh, in the network.","hi~ i'm wondering should we ignore the diagonal values of A^(k)? because this value doesn't make sense in the link prediction or it allows a link connected to itself?
It depends on the interpretation of your graph (can you befriend yourself?), but generally speaking, we set the diagonals of A to be 0 in graphs meaning there are no self loops (simple graph)."
"AUDIENCE: It has to be one-way [INAUDIBLE].
SRINIVAS DEVADAS: It has to be one-way.
And explain to me-- so I want a description of it has to be one-way, because why? AUDIENCE: Because you want all the c x's to be hidden from all the other options.
SRINIVAS DEVADAS: Right.
C of x should not reveal x, all right? All right.
That's good.
Do you have more? It has to be collision resistant.
OK.
I guess.
A little bit more.
You're getting there.
What-- why is it collision resistant? AUDIENCE: Because you want to make sure that Alice, when she makes a bid that she commits that bid.
If she's not going to resist it then she could bid $100 and then find something else.
SRINIVAS DEVADAS: That's exactly right.
So CR, because Alice should not be able to open this in multiple ways, right? And in this case it's not TCR in the sense that Alice controls what her bids are.
And so she might find a pair of bids that collide, correct? She might realize that in this particular hash function, you know $10,000 and a billion dollars collide, right? And so she figures depending on what happens, she's a billionaire, let's assume.
She's going to open the right thing.
She's a billionaire, but she doesn't necessarily want to spend the billion, OK? So that's that, right? But I want more.
Go ahead.
AUDIENCE: You don't want it to be malleable.
Assuming that the auctioneer is not honest because you don't want to accept a bribe from someone and then change everyone else's bid to square root of whatever they bid.
SRINIVAS DEVADAS: That's exactly right.
Or plus 1, which is a great example, right? So there you go.
I ran out of Frisbees.
You can get one next time.
So yeah, I don't need this anymore.
You're exactly right.
There's another-- it turns out it's even more subtle than what you just described.
And I think I might be able to point that out to you.
But let me just first describe this answer, which gives us non-malleability.
So the claim is that you also want non-malleability in your hash function.
And the simple reason is, given C of x-- and let's assume that this is public.
It's certainly public to the auctioneer, and it could be public to the other bidders as well.",What is the diff bet TCR and CR as both of them looked same.
"Okay, so recurse is going to return the minimum edit distance between two things, the first ""m"" letters of ""s"", and the first ""n"" letters of ""t"".
Um, I'm gonna post this online so you guys don't have to, like, copy- try to copy this.
Um, okay, so, um, okay, suppose I'm gonna- I'm gonna define this function.
Uh, if I have this function what should I return? Recurse of-.
[inaudible] So ""m"" is an integer, right? So ""n"" is an integer, so I'm going to return the length of ""m"" and the length of ""n"".
Okay, so that's kind of, uh, the initial state.
[OVERLAPPING] Sorry.
Yup.
Okay.
Um, All right.
So now you need to fill out this function.
Okay, so let's- let's um, consider a bunch of cases.
So here's some easy cases.
Suppose that, um, ""m"" is zero, right? So I have- comparing an empty string with something that has ""n"" letters.
So, what should the cost of that be? [NOISE] I heard some mumbling.
[OVERLAPPING].
It should be ""n"" [NOISE] and symmetrically if ""n"" is 0 then result should be ""m"", um, and then if now we come to the kind of initial case that we consider which is the end [NOISE] match a match.
So, if ""s"" um, the last letter of ""m"", you know, this is 0-based indexing.
Um, so that's why there's a minus 1.","You check the cache FIRST before running all the computation ""if (m,n) in cache => return cache(m,n)"" lines at the top before everything else. So basically if the result is already in the cache then there is no need to run 3 computations again, just return the result"
"I want an incremental way of doing this.
How could I do this? I have a starting time, and I want to compute the same densities in an incremental way.
Someone else other than Fadi? Yeah, back there.
AUDIENCE: Well, you have to start the n time thing just for each list of whatever tuple or whatever you could keep like a count-- a different count, a dictionary of the start time, and then increment through each of the start times until the end and just add one each time.
So you're basically iterating through each set of times and finding the count of each time and whichever time is the most.
SRINI DEVADAS: You're absolutely on the right track.
And it turns out you don't even need dictionaries.",I keep thinking this can be done in better then O(n^2). Any ideas?
"But he also worked a little on the syntactic structure of Sanskrit sentences.
And essentially what he proposed was dependency grammar over Sanskrit sentences.
And it turns out that sort of for most of recorded history when people have then gone on and tried to put structures over human sentences, what they have used is dependency grammars.
So there was a lot of work in the first millennium by Arabic grammarians trying to work out the grammar structure of sentences.
And effectively what they used was akin to I've just presented as a dependency grammar.
So compared to 2,500 years of history, the ideas of having context-free grammars and having constituency grammars is actually a really, really recent invention.
So it was really sort of in the middle of the 20th century that the ideas of constituency grammar and context-free grammars were developed first by Wells in the 40s, and then by Noam Chomsky in the early 50s leading to things like the Chomsky hierarchy that you might see in CS103, or a formal languages class.
So for modern work on dependency grammar using the terminology and notation that I've just introduced, that's normally attributed to Lucien Tesniere, who was a French linguist in around the sort of the middle of the 20th century as well.
Dependency grandma was widely used in the 20th century in a number of places.
I mean, in particular it tends to be much more natural and easier to think about for languages that have a lot of different case markings on nouns.
Like nominative, accusative, genitive, dative, instrumental kind of cases like you get in a language like Latin or Russian.
And a lot of those languages have much freer word order than English.
In English, the subject has to be before the verb and the object has to be after the verb.
But lots of other languages have much freer word order, and instead use different forms of nouns to show you what's the subject or the object of the sentence.
And dependency grammars can often seem much more natural for those kinds of languages.
Dependency grammars were also prominent in the very beginnings of computational linguistics.
So one of the first people working in computational linguistics in the US was David Hayes.
So the professional society for computational linguistics is called the Association for Computational Linguistics, and he was actually one of the founders of the Association for Computational Linguistics.
And he published in the early 1960s and early perhaps the first dependency grammar --parser, sorry, dependency parser.
OK.
Yeah, a little teeny note just in case you see other things.
When you have these arrows, you can draw them in either direction.",What does handwriting a grammar mean?
"But I have one drawn up here just so you recognize it when you see it written.
Note that it looks a whole lot like the block diagram symbol for a gain.
And that's intentional, and we'll cover that later.
But in the meantime, the other sources that we're going to be using are independent current, and voltage sources.
We're going to use resistors to adjust the amount of voltage and current that we're actually dealing with and then sample either the current or the voltage at a particular point in our circuit to get the desired values that we're after.
On a circuit diagram, when you're interested in the voltage drop across a particular element, you'll indicate it by putting a plus and minus sign.
This also indicates the directionality of the voltage drop.
Likewise, when you're interested in the current flowing through a particular element, you'll usually see an indication of it by labeling the current i, and then maybe i with some sort of subscript, and an arrow indicating the direction of current flow through that element so that you avoid making sign errors with the person that might be reading or writing your diagram.
A quick note here.
This is the reason that electrical engineers use j to symbolize values in the complex plane.","if the output of voltage of RL circuit is taken to be the voltage across the inductor, the circuit is said to be what
How would i2 relate to it being correlated to R3 being over the sum and for i3 visa versa? Why can you not use i2 with R2 over the sum and so on?
What would you use, Maxwell equation within time or frequency domain?"
"So here's our graph.
1, 2, 5, 3, 4.
OK.
And let's think about the traversal order that the depth-first search is going to do.
Right.
So here's our source.
And now what does the source do? It rec-- so let's think about our recursion tree.
So we have the source all the way up in here.
And now he's going to start calling the visit function recursively.
So.
And I'll go ahead and number these the same as on the screen.
Well, he has one outgoing neighbor, and it hasn't been visited yet.","exactly,why did the proffesor told it is O(|E|) time about the depth first search"
"I'm going to expand D.
D one transition available to a node.
And in plain old fashioned breadth-first search and depth-first search, I run my goal test when I visit a node.
So at this point, I would test whether or not E was my goal test.
I would discover it's my goal test.
My search would return successfully and return the path found.
So AB and ACB remain on the agenda.
I popped off ACD in order to expand D.
And I found this path.
At this point, I've concluded depth-first search.
I'm going to do one more around of breadth-first search to demonstrate an important rule.
If I pop ABC off the agenda and move all these over, if I'm looking at ABC, and I look at the children of C, the two children of C are B and D.
So the first partial path that I would end up adding to the agenda as a consequence of expanding C in this case would be ABCD.
And it would look like this.
You'll notice that we're going to create an infinite loop.
And there are two basic rules of basic search that I need to emphasize now to prevent you from doing things like creating an infinite loop.","For the DFS, before finding the path ACDE, does it should visit ACDA first because you don't have a visited node constraint here?"
"And so what I just did here is for any- this curve just represents for any p-hat c, what the cross entropy loss would look like.
Okay.
And so we can come back to this, for example, right? And if we look at this parent here right, this guy has a 10%, right? It's sort of like p-hat, p-hat for this guy is 0.1, it's 10% basically or, or I guess no, in this case, would be 0.9 sorry.
And then versus here, in these two cases, right, your p-hat, in this case, is 1 since you've got them all right, all right, and then, in this case, it's 0.8, okay? So you can sort of see since these are equal, there's the same number of examples in both of these, the p-hat of the parent is just the average of the p-hat's of the children.
Okay.
And so that's how we can sort of take this LR_parent, this LR_parent is just halfway, if we projected this down, all right.",In cross entropy what if phat goes to 0. It also decreases the loss?
"Fundamentally, it does two things.
One, performs calculations.
It performs a lot of calculations.
Computers these days are really, really fast, a billion calculations per second is probably not far off.
It performs these calculations and it has to store them somewhere.
Right? Stores them in computer memory.
So a computer also has to remember results.
And these days, it's not uncommon to find computers with hundreds of gigabytes of storage.
The kinds of calculations that computers do, there are two kinds.
One are calculations that are built into the language.
These are the very low level types of calculations, things like addition, subtraction, multiplication, and so on.
And once you have a language that has these primitive calculation types, you, as a programmer, can put these types together and then define your own calculations.
You can create new types of calculations.
And the computer will be able to perform those, as well.
I think, one thing I want to stress-- and we're going to come back to this again during this entire lecture, actually-- is computers only know what you tell them.","Would a mouse be a fixed use computer?
finally, they didn't answer the title question ""what is computing ?""
Video never explained: ""What is computation?""...
what does 'primitives'' mean ?
My doubt is why computer follow programing?
Is basic arithmetic like 2+2 equals 4 a type primitive programing statement? Two and Two are the operands, and the addition sign is the operator. The output is four. Has programing been around for as long as we've had math?
So what is computation?!
So... What is Computation?"
"JOHN GUTTAG: Wow.
You want to raise your hand if you're born on February 12? [LAUGHTER] So you are exceptional in that you lie about when you're born.
But if you hadn't lied, I think we would have still seen the probabilities would hold.
How many people were there, do we know? AUDIENCE: 146 with 112 unique birthdays.
JOHN GUTTAG: 146 people, 112 unique birthdays.
So indeed, the probability does work.
So we know you're exceptional in a funny way.
Well, you can imagine how hard it would be to adjust the analytic model to account for a weird distribution of birthdates.","Can someone please explain what the ""N=100"" means in the birthday problem? > Actual prob. for N=100... <. I know it's the actual probability for each different number of people, but what does it has anything to do with the "" N=100"" ? Thank you!"
"We can use them to return more than one value from a function.
So tuples are great.
Might seem a little bit confusing at first, but they're actually pretty useful, because they hold collections of data.
So here, I wrote a function which I can apply to any set of data.
And I'll explain what this function does, and then we can apply it to some data.
And you can see that you can extract some very basic information from whatever set of data that you happen to collect.
So here's a function called get_data, and it does all of this stuff in here.
And in the actual code associated with the lecture, I actually said what the condition on a tuple was.
So it has to be a tuple of a certain-- that looks a certain way.
And this is the way it has to look.","I have a confusion, If tuples are immutable how does nums = nums + (t[0],) work?
I am curious as to the advantage of using a tuple over a list. It seems as though lists have the exact same functionality as tuples but with the added advantage of you being able to modify them(mutability) if you so choose to. Why not just alway use lists?"
"Maybe I should say two methods, because there are multiple implementations of these methods.
The first is called hierarchical clustering, and the second is called k-means.
There should be an S on the word mean there.
Sorry about that.
All right, let's look at hierarchical clustering first.
It's a strange algorithm.
We start by assigning each item, each example, to its own cluster.
So this is the trivial solution we talked about before.
So if you have N items, you now have N clusters, each containing just one item.
In the next step, we find the two most similar clusters we have and merge them into a single cluster, so that now instead of N clusters, we have N minus 1 clusters.
And we continue this process until all items are clustered into a single cluster of size N.
Now of course, that's kind of silly, because if all I wanted to put them all it in is in a single cluster, I don't need to iterate.
I just go wham, right? But what's interesting about hierarchical clustering is you stop it, typically, somewhere along the way.
You produce something called a [? dendogram.
?] Let me write that down.
At each step here, it shows you what you've merged thus far.
We'll see an example of that shortly.
And then you can have some stopping criteria.
We'll talk about that.
This is called agglomerative hierarchical clustering because we start with a bunch of things and we agglomerate them.","Thanks for an amazing lecture! it tries to cluster data into two groups and see if it correctly differentiated people who dies of heart attack and those that didn't. To me this is using clustering for classification task, if yes, when would someone use clustering rather than classification?
Why we use clustering while we have the label? Like in the medical example, we already know the label (0,1).
Pretty sure he is talking about the number of comparisons which need to occur to create the group, n-squared meaning the number of comparisons is on the order of the square of the number of objects to compare, and n-cubed on the order of the cube of the number of objects to compare. Sort of like big-O notation."
"You don't actually care who serves it to you, as long as it matches a particular hash.
So there's actually a new spec out there for being able to specify basically hashes in these kinds of tags.
So instead of having to refer to jquery.com with an HTTPS URL, maybe what you could do is just say script source equals jquery.com, maybe even HTTP.
But here, you're going to include some sort of a tag attribute, like hash equals here, you're going to put in a-- let's say a shell one hash or a shell two hash of the content that you're expecting to get back from the server.
AUDIENCE: [INAUDIBLE].
PROFESSOR: Question? AUDIENCE: [INAUDIBLE].
PROFESSOR: Ah, man.
There's some complicated name for it.
I have the URL, actually, in the lecture notes, so [INAUDIBLE].
Subresource integrity or something like this.",what is an attacker change jquery and create new hash?
"Now, suppose that I have a way to test satisfiability of circuits.
How am I going use this multiplier circuit to factor? Well, the first thing I'm going to do is let's suppose the number that I'm factoring is n and is the product of two primes, p and q.
Those are the kinds of n's that we've been using in RSA, and let me also observe that it's very easy to design an n tester-- that is, a little digital circuit that has 2k input lines and produces on its one output line precisely when the input is the binary representation of n.
So let's attach this equality tester that does nothing but ask whether it's being fed the digits of n as input and it produces an output, 1 for n and 0 if the input pattern is and the digital representation, the binary representation of anything other than n.","But how do you solve the satisfiability problem in the first place? How do you know whether a SAT problem is satisfiable or not? Are you saying it can be reduced and solved only IF someone finds a solution to the SAT problem?
That means we can break RSA using this ???"
"Some of the vectors in the sample set, and I say some, because for some alpha will be 0.
All right.
So this is something that we want to take note of as something important.
Now, of course, we've got to differentiate L with respect to anything else it might vary, so we've got to differentiate L with respect to b, as well.
So what's that going to be equal to? Well, there's no b in here, so that makes no contribution.
This part here doesn't have a b in it, so that makes no contribution.
There's no b over here, so that makes no contribution.
So we've got alpha i times y sub i times b.","Question: Why does the dot product in the Lagrangian equation depends on the sample vectors? Time stamp: 36 minute
Why we add alpha with the constraints when doing langrangian???
Hi everyone, can i ask why when professor substitute for w, he used sub i and sub j when the there is only sub i after taking the derivative with respect to w. He kinda explained a little bit. But maybe because i'm not a native speaker, i kinda didn't get the intuition behind it?
I am still confused about how to determine a support vectors. Could anyone recommend me some links about it?
How to calculate alpha the langrange multipler in it?
Hi, What is that constant alpha? How did it appear there?
I don't follow the constraints - he only retains the equality type constraints in the Lagrange's Multipliers step. This (equality) happens only for the few samples at the gutters. What about the non-gutter samples? The constraints for these are greater than or equal to zero type, hence Lagrange's method can not be used. Aren't those getting ignored therefore? What am I missing here?
thank you for replying. I get your first sentence. Just to clarify, it would be equal to 1 if b=0, right? As for your second sentence, I don't get it because he wrote less than or equal to -1. Also, if say b=5 then the first inequality is ok but the second gives -1+5<=-1 which is not true. What am I misunderstanding?
Great video! This was a very helpful and thorough explanation of SVM's. I just have one question, though: how is the optimal bias term solved for? The optimal weight vector as shown in the video can come from the optimal KKT multipliers, but there was never any way to calculate the bias shown in the video.
This is also my question, could somebody explain?
How do we find <alpha> and b?
Okey, can someone please tell me what is the unknown vector U? And what should be the value of b?
why are we partially differentiating w.r.t b, isn't it supposed to be a constant?
Still hard to understand the L part and the kernel part any one can help to give some more good resource for this? Thanks
And how do I compute 'b'?
At https://youtu.be/_PwhiWxHK8o?t=1757 , I remember that the formula of Lagrange Multiplier is partial differential with a_i, why is b on the video? Can anyone help me, thanks.
Can someone explain me why he compares two equations against >=+1 and <=-1 where the original equation was >= 0 and <= 0? Wouldn't the sample to be positive W*Xplus + b should be >= 0 and W*Xminus + b should be <= 0? i don't get that part
Did you figure out why? I have the same question
How do we calculate alphas?
Very good explanation but where do all alphas come from? I know the answer is ""from Lagrange multiplier"" but how we calculate them?
+Otto Fazzl Does that condition mean that all positive samples are required to be at least 1 ""unit"" away from the decision boundary, as well as all the negative samples?
+Abhi Sivaprasad I have the same question with Murat, although we only need w and b in the decision rule, but we need alphas to get w. I am really confused how to calculate alphas. Can anyone help me on this question?
Question.. w = sum over i ( y_i * a_i * x_i ) => weights are linear combination of the samples. When prediction is perfomed, using linear svm, thanks to the linearity of the dot products, we do not need to perform the inner product among x_test with all the x_i on the ""gutt"" of the wider street, in fact sum_over_i (y_i * a_i * x_i * x_test) = sum_over_i (y_i * a_i * x_i) * x_test. We need just to precompute the first term. However, what if non linear kernel functions K(x_i,x_test) is involved, for instance gaussian kernel exp(-(|| x - y ||^2/sigma)? For prediction, I need to do product scalar with all the samples in the training set that are on the gutt of the wider street?"
"And for an instance of a car, I'm going to assign the data attribute named wheels to whatever is passed in for w.
So notice that they're not the same name.
And the data attribute for ""doors"" is going to be the value that's passed in for d OK? And also notice that inside init, I can do any other sort of initializations that I'd like.
So it's not just assigning variables from the parameters to variables for my objects.
So in this case I'm creating a new data attribute named ""color,"" and I'm going to just create it to be an empty string OK, even though I didn't pass in any color to my object.
So the question says, ""Using the class definition above, which line creates a new Car object with 4 wheels and 2 doors?"" OK? So this first one is not right, because it's trying to call the class with a variable mycar for self, which isn't quite right.",What is the graph used for.??
"Because you've got to go deeper.
If you add to, for example, the inputs of h.
Or you could add to the outputs of h as well, or the outputs of the current h.
But you can basically go deeper, or need to go deeper in order to find the break one-wayness, in order to find an x, whatever you have, that produces the d-bit string that you have, right? So what's a simple way of creating an h prime such that it's going to be pretty easy to find targeted collisions even, not necessarily collisions, it's pretty easy to find targeted collisions, without breaking the one-way property of h? Yeah? AUDIENCE: So if you have x sub i, if i odd then return h of x of i.
So that's minus 1.
So return the even group.
SRINIVAS DEVADAS: Sure.
Yep.
AUDIENCE: Given x any x of i, you can usually find another x of i that was the same output? You can go backwards.
SRINIVAS DEVADAS: You can't go backwards.
Yeah, that's good.
That's good.
I'm going to do something that's almost exactly what you said.
But I'm going to draw it pictorially.
And what you can do, you can do a parity, like odd and even that was just described.",how are d = 128 and 2^37 related? anyone?
"You want to make sure that you have the absolute minimum set of commands, and you're going to have to use these intervals.
So you definitely want to look for all the contiguous intervals because obviously, if you say 1 followed and then that's a separate command, then you say 2 and that's a separate command, you end up having more commands than necessary.
So anyone want to tell me I guess roughly, in terms of pseudocode, how this would work? Yeah, go ahead.
AUDIENCE: Well, you could go down the line and look at the groups.
So start with the 0.
You've got to write 0.
Group them.
And then whatever the second one is-- so whatever the second group is, if it's F or B, those are the ones that need to be flipped because that's going to be the minimum number of commands.","Very poor explanation on the last optimal algorithm, ABABABA - here skipping first intervals of first A-label gives us the best result. ABABAB - here also skipping first intervals of first A-label gives us the best result. The above 2 configurations are the whole possibilities
Did this guy just populate the array randomly? I cant tell. I tried 2^(2n) -2n, -n, and -1 to check for indices and their evenness to determine where the Bs are being populated. Because if you know the loc of the Bs in the array, you know where the Fs are. In theory, iff you know the number of moves to flip the array to conform to one of the two, you know the other procedure. But, then you know based on which case you use for performance efficiency, the effectiveness of the algorithm . But for his array, the case of populating in B was like ceil(n/2) and F case was like floor(n/2) + c, c contains [1,2] for cases of 0...9 and 0...10. How it was populated is like the one thing I dont understand here. The best I can think of is using 2^i somewhere: Edit: parameters for c is [0,2], not [1,2]
Can you give a brief example of what you mean? How the array is populated and the size don't matter. Here are break downs of the simplest cases that illustrate all the possibilities: 1) F..F or B..B, i.e. they are all the same so no commands are needed 2) F..FB..B or B..BF..F, i.e. the pattern ends with the opposite of the state so the same number of flip commands are need for either case 3)F..FB..BF..F or B..BF..FB..B, i.e. the pattern ends with the same orientation as the start. In this case there will be one fewer commands needed to flip the second orientation. In any problem where they are not already all in the same orientation it will either be case 2 or 3 so flipping the second orientation will always require the least number of commands. Starting, you don't need to know the number of commands that will be needed since you know by this procedure you will take the minimum."
"That takes n time-- n times the number of digits.
I had to create each one of these tuples-- so n plus n times the number of digits-- log base n of u.
So here I had to loop through all the things.
And then here, for each thing, I broke it up into log base n of u digits, and that's how long the first thing took.
And then, how long did it take me to tuple sort? n time per digit-- so I also get this factor.
Does that make sense? How long is that? Is that good? Is that bad? For what values of u is this linear time? If u is less than n to the c for some constant c, then the c comes out of the logarithm, log n of n is 1, and we get a linear time algorithm.
Does that makes sense? OK.
So that's how we can sort in linear time, if our things are only polynomially large.
So in counting sort, we get n plus u.
In radix sort, we get also a stable sorting algorithm where the running time is n plus n times log base n of u.","What if you have all collisions in counting sort? Wont that be n^2?
I don't understand how n/2 is greater than (n/2)^(n/2) in 17 min of video
Why do we have to break our number up if u<n^2?"
"(2), OK, so what's going on? So A is an accumulator, we start it, we step it.
B is an accumulator, we start it, we step it, we step.
So what is going to be A, how did you get five? Well, A started at 0 that's what the start method does, right? Stepping it stepped it to 7.
B made a new one, we can ignore that because it's a separate instance.
The idea is that the state is associated with the instance.
So when we perturb the state of B by doing B.step, we are touching the state of A-- because they're instance variables.
So that means when we decrement it, we're decrementing the 7 to get 5.
OK, seems reasonable.
So the answer is either (1), or (2), or none.
Why is it the same (21, 21) (21, 21), when we called the A.getNextValues, and here we're calling the B.
Why are they the same? AUDIENCE: Because it sets the state [INAUDIBLE].
PROFESSOR: So getNextValues is a pure function.
So it gets passed in the state, the first number is the state, the second number is the input.
So it doesn't pay any attention to the instance variable state, it uses the thing they got passed in.
Furthermore, the getNextValues associated with A, is precisely the same as the getNextValues associated with B, because getNextValues is something that's in the accumulator subclass.
So they're exactly the same because it's the same method.
So what should be in your head, is a picture that looks like this.
When we said create a state machine, that was something we put in our library, and that makes this environment.
When we said subclass of state machine to make accumulator, that made a new class, a new environment.
And then when we made A and B by saying that A equals accumulator, and B equals accumulator, that made instances.","Why did the accumulator return two values? Did it start with 8 and step 13 then start with 13 and step with 8?
the scope of the variable ie global local static etc"
"But it's more than that.
It's also about communicating those solutions to others and being able to communicate that your way of solving the problem is correct and efficient.
So it's about two more things-- prove correctness, argue efficiency, and in general, it's about communication-- I can't spell, by the way-- communication of these ideas.
And you'll find that, over the course of this class, you'll be doing a lot more writing than you do in a lot of your other courses.
It really should maybe be a CI kind of class, because you'll be doing a lot more writing than you will be coding, for sure.
Of course, solving the computational problem is important, but really, the thing that you're getting out of this class and other theory classes that you're not getting in other classes in this department is that we really concentrate on being able to prove that the things you're doing are correct and better than other things, and being able to communicate those ideas to others, and not just to a computer-- to other people, convince them that it's correct.","How to understand this proof? What do I need to know (Im bad at math)
Can I take this course without discrete math?"
"But for now, we just want to look and see if we believe that this arrangement, where each of these H's is producing plus 1 or minus 1, we're adding them up and taking the sign, is that going to give us a better result than the tests individually? And if they look like this when draped over a sample set, then it's clear that we're going to get the right answer every time, because there's no area here where any two of those tests are giving us the wrong answer.
So the two that are getting the right answer, in this little circle here for H1, these other two are getting the right answer.
So they'll outvote it, and you'll get the right answer every time.
But it doesn't have to be that simple.
It could look like this.
There could be a situation where this is H1, wrong answer.
This is H2, wrong answer.
And this is H3, wrong answer.
And now the situation gets a little bit more murky, because we have to ask ourselves whether that area where three out of the three get it wrong is sufficiently big so as to be worse than 1 of the individual tests.","So, why several weak learners combine can become a strong learner? Can we prove it in probability?"
"So weak partial orders-- just what you get when you put these things together-- weak partial order is transitive, antisymmetric, and reflexive.
So in a weak partial order, we insist that every element be related to itself.
So there's a-- just a quick remark.
Asymmetric implies nothing's related to itself.
Reflexive implies everything is related to itself.
And it's possible that there be some graph in which some elements are related to themselves and some not.
That would be something that was neither a strict nor a weak partial order.
It would just be transitive and antisymmetric.
Those don't come up much and so we don't bother to give them a name or talk about them.
And, finally, the theorem that summarizes up this whole story is that R is a weak partial order if and only if R is equal to the walk relation for some DAG, including length zero walks.
",what is D*?
"Because then I can build what's referred to as a greedy dependency parser, which just goes bang, bang, bang, word at a time.
OK, here's the next thing.
Run classifier, choose next action.
Run classifier, choose next action.
Run classifier, choose next action.
So that the amount of work that we're doing becomes linear in the length of the sentence rather than it being cubic in the length of the sentence using dynamic programming, or exponential in the length of the sentence if you don't use dynamic programming.",Does this play list covers the coding aspect even a little?
"I'm going to write them as x_1 through x_n.
In this notation, what I mean is each x_i is the i-th example.
It does not mean x raised to the power of i or x raised to the power of n.
Ah, whenever I write the superscript with parentheses, it means it's the i-th example, right? Ah, and there are n such examples.
Each example is in R_d.
So it's each- each x_i is a d-dimensional vector, right? And we are given- um, let's assume we are given, um- um, n such examples, right? Now, the probability- we already saw the probability density of, um, of the Gaussian, which is P of x given Mu Sigma is equal to 1 over 2 pi to the d by 2 [NOISE] sigma half exponent [NOISE] Right.","In the case of the probability density of the multivariate gaussian distribution in the denominator, the normalising constant sqrt(2*pi) will be raised to the power of 'n' instead of 'd' because we are having n many random samples?"
"So maybe you take a1.
And then you follow your default policy and this is often random.
Though for things like AlphaGo you often want much better policies but, um, you can just use a random policy and you'll just take random actions and get to next states and you do this until the end and you see you either win or lose.
Now, um, in this case it's a two player game.
So we do a minimax tree instead of expectimax, but the basic ideas are exactly the same.
So that's my first roll out.
I'm gonna do lots of these roll outs before I figure out how I'm going to actually place my piece.
All right.
So the next time, so this is my second roll out.
This is the second roll out of my head [NOISE].
And say okay well, last time, um, you know, I, I took this action.
So now I have my default policy.","A question: will you alternate between tree and rollout policy in a single playout? Maybe like this s1 / \ a1 a2 / s2 / a2 / s3 / \ a1 a2 In the first and last level, you have enough experience, you can do the tree policy with UCT, with the s2 state, you do not have enough experience, you do rollout policy? But the video looks like starting rollout policy from s2 until terminal. I watched a lot of videos and slides, none of them clearly talk about this. No one clearly talk about how the tree is expanded and when to use tree and rollout policy."
"Right? We've got a set i.
We've got to compare i and potentially we've got to return.
So there's at most three steps inside the loop.
But depends on how lucky I'm feeling.
Right? If e happens to be the first element in the list-- it goes through the loop once-- I'm done.
Great.
I'm not always that lucky.
If e is not in the list, then it will go through this entire loop until it gets all the way through the elements of L before saying false.
So this-- sort of a best case scenario.
This is the worst case scenario.
Again, if I'm assigned and say well, let's run some trials.
Let's do a bunch of examples and see how many steps does it go through.
And that would be the average case.
On average, I'm likely to look at half the elements in the list before I find it.
Right? If I'm lucky, it's early on.
If I'm not so lucky, it's later on.
Which one do I use? Well, we're going to focus on this one.
Because that gives you an upper bound on the amount of time it's going to take."," My understand is, the worst case scenario only cares about the term with the highest degree of polynomial. For example, if it takes 3n + n + 1000000, then the worst case scenario is still O(n) because you don't care about the constant (1000000), the terms with lower degree of polynomial (n) and the coefficient of the highest degree of polynomial (3). Let's say we have 2 lists, L1 and L2 with the same numbers in them. [1, 2, 3, 4, 5, 6,...,n], and we try to compare them. Let's say in the first loop we do one operation, and in the nested loop we do 3 operations. The first time you compare 1 in L1 to 1 in L2, it will do 3 operation since you do 3 in the nested loop. Since the comparation is True, you continue on with the first loop. The second time you compare 2 in L1 and 1 in L2, you also do 3 in the nested loop. But since the comparation is False, you continue on with the nested loop. Now you compare the number 2 in L1 and 2 in L2, and you also do 3. In total, you do 3 + 3 = 6 operations in the second loops. The 3rd loop you will do 3 + 3 + 3, and so on. And so in total, you do 3 + 6 + 9 + ... + 3n = 3n(n+1), which is still O(n) because we don't care about the number 3 and the term n behind."
"So, the standard language for this as we call this the branching factor.
And in this particular case, b is equal to 3.
This is the depth of the tree.
And, in this case, d is two.
So, now that produces a certain number of terminal or leaf nodes.
How many of those are there? Well, that's pretty simple computation.
It's just b to the d.
Right, Christopher, b to the d? So, if you have b to the d at this level, you have one.
b to the d at this level, you have b.
b to the d at this level, you have [? d ?] squared.
So, b to the d, in this particular case, is 9.
So, now we can use this vocabulary that we've developed to talk about whether it's reasonable to just do the British Museum algorithm, be done with it, forget about chess, and go home.","When exactly does A-B prune the most nodes?
I paused the video just after he explained the 2^2 British museum method, and I thought ""hang on, shouldn't the player who's making the move be trying to figure out what the implications of planning 5 moves ahead are given that each second successive state has to make assumptions about what the opponent would do in the intermediate state? So I started thinking about how you might predict an opponents move and even got onto thinking if you could use a neural network to predict it based off the opponents previous moves!"" Then he explained minimax and I felt stupid... Although it does raise a second question, ""If you matched two of these algorithms against each other, is the result deterministic?"" Any ideas?
I was confused about it for a little bit too. From what I can understand, in our case, each node of the tree represents a board configuration. As someone else, we must have a heuristic to determine how good a particular board configuration (node) from the perspective of min and max. Lecturer mentioned that one of the possibilities for heuristics is the piece count for each player (just as an example). But obviously value of each piece is not equal, so that wouldn't be the best heuristics. But you get the idea of what could be used as a heuristic. If we didn't have any such heuristic, then your only values would be -1, 0, 1 at leaf nodes. They would represent final configurations - loss, draw, win. In this case yes, you wouldn't be able to determine values at intermediate nodes because there's nothing to tell you whether min or max are closer to winning or losing.
+Daniel Jones So it is like breadth first search in a way, right ?
Does this mean that the algorithm becomes top-down instead of bottum-up? And how does alpha-beta pruning work with this, how can you prune anything if you are going to process all nodes at each level?"
"So when we are working out the gradients of s with respect to b, the first two terms were exactly the same, it's only the last one that differs.
So to be able to build or to train neural networks efficiently, this is what happens all the time and it's absolutely essential that we use an algorithm that avoids repeated computation.
And so the idea we're going to develop is when we have this equation stack that this sort of stuff that's above where we compute z, and we're going to be sort of, that'll be the same each time and we want to compute something from that that we can then sort of feed downwards when working out the gradients with respect to Wx or b.
And so we do that by defining delta, which is delta is the partials composed that are above the linear transform.
And that's referred to as the local error signal, that's what's being passed in from above to the linear transform.
And we've already computed the gradient of that in the preceding slides.
And so the final form of the partial with respect to b will be delta times the remaining part.
And well, we've seen that for partial of s with respect to b, the partial of z with respect to b is just the identity, so the end result was delta.
But in this time, we're then going to have to work out the partial of z with respect to w and multiply that by delta.
So that's the part that we still haven't yet done.
So and this is where things get in some sense, a little bit hairier, and so there's something that's important to explain.
So what should we have for the Jacobian of ds / dW? Well, that's a function that has one output, the output is just a score of real number, and then it has n by m inputs.
So that Jacobian is a 1 by n by m matrix, i.e.
a very long low vector but that's correct math.
But it turns out that that's kind of bad for our neural networks.
Because remember, what we want to do with our neural networks is do stochastic gradient descent.
And we want to say theta new equals theta old minus a small multiplier times the gradient.",Question: How is f(z) Jacobian? My understanding: For a single neuron z is going to be a scalar. and f(z) output is also going to be scalar for a neuron. Can a Neuron ever output anything other than a scalar? Perhaps the jacobian holds for the overall network
"How you can translate.
You can sort of create these networks from tables or what have you, and add capacities to edges, find max flows, and solve these problems.
So this is a good example of that.
So let's go ahead and do that I am going to draw a flow network based on this table.
It's going to have a source and a sync that are basically these dummy nodes that are essentially going to source about infinite flow.
But the capacities of the edges are going to come from that table.
And the edges themselves are straightforward in the sense that the graph is going to look the same based on the number of teams, and the particular team that you're analyzing.","The capacities of inner edges (between the ""team-pairs"" layer and ""teams"" layer) are irrelevant and so are set to infinity. Each of these edges indicates the number of games a particular team won when playing against some other team (for example, flow through edge ""1-2"" -> ""1"" shows how many times Team 1 won while playing against Team 2). Clearly, the flow through each of these edges is bounded by the number of games each team-pair has to play. Since these numbers are already specified as capacities of source-edges and sink-edges, they effectively restrict max flow that can go through inner edges. Thus, there is simply no need to put additional restrictions on inner-edge capacities."
"But that's kind of why this is exciting and rewarding.
With that said, the goal of this course is to give you the foundations of machine learning.
This is the foundational layer.
So this is simultaneously a introductory course to machine learning.
We don't require you to have taken a machine learning course to take this, right? So it's an introductory course.
But on the other hand, we hope that, after you take this course, you feel somewhat comfortable that you know enough basics of machine learning so that you can apply machine learning to some of the applications.
Of course, if you really want to kind of be an expert in some of the applications like NLP and Vision, you probably have to take those courses.
But this course, probably, will set up the foundations for the machine learning component of the general kindof like AI or other applications of AI.",If I dont know multivariable calculus and linear algebra then learning ml is not possible ?
"Then the definition of L is a little funny.
What does it mean to say you start with A of n? Hm? AUDIENCE: If we define A of n.
ERIK DEMAINE: Right, A of n is not defined, so that's not so nice.
So maybe fix that.
n equals case.
OK, but I'm still going to draw an edge there-- conceptually say, oh, we're just done at that point.
That's the base case, where we have no string left-- cool.
And when I said from to to actually, I meant the reverse.
All the edges go from right to left.
And then what we're doing is looking for the longest path in this DAG.
Longest path is maybe a problem we've talked about in problem session, because it's a DAG-- well, longest path is the same thing as shortest path, if you just negate all the weights.
There are no weights in this picture, so if you just put negative 1 on all these edges and ask for the shortest path from the base to anywhere-- so single source shortest paths from this base-- then we would end up getting this path, which, if you look at it, is E-M-P-T-Y, empty.
And so that shortest path is indeed the right answer.
What I've drawn here is the shortest path tree.
So also, if you wanted the longest increasing subsequence starting at A, then it is A-T-Y, just by following the red arrows here.",Hard time understanding relation between graphs and answer of problems
"We don't want the point to be kind of close to the border or the edge of the box, we want it towards the center of the box.
So that's how we now define the- the distance between the entity and the box.
So now when we have the final embedding of the query, all we do is basically apply this distance, um, and say who are the points that are closest according to this distance, uh, to the center, uh, of the box.","In defining the entity-to-box distance metric, we considered alpha in (0,1). But, I think instead of penalising more, it is penalising it less. The research paper also says that ""the key here is to downright the distance inside the box by using 0 < alpha < 1. This means that as long as entity vectors are inside the box, we regard them as ""close enough"" to the query centre."" Thus, they are not penalising the points inside the box more to make it more robust. Do I understand something wrong? Please correct me if I am wrong."
"And then we will do the aggregation, uh, three times, uh, get the aggregated messages from the neighbors, and now we can further aggregate, uh, these messages into a single, uh, aggregated message.
And the point here is now that we- when we learn these functions a^1, a^2, a^3, we are going to randomly initialize parameters of each one of them, and through the learning process, each one of them is kind of going to converge to some local minima.
But because we are using multiple of them, and we are averaging their transformations together, this will basically allow our model to- to be- to be more robust, it will allow our learning process not to get stuck in some weird part of the optimization space, um, and, kind of, it will work, uh, better, uh, on the average.
So the idea of this multi-head attention is- is simple.
To summarize, is that we are going to have multiple attention weights on the- on the same edge, and we are going to use them, uh, separately in message aggregation, and then the final message that we get for a- for a node will be simply the aggregation, like the average, of these individual attention-based, uh, aggregations.","Why the multiple attention scores would have different values and not one value? Probably to several local minima.... If so, I assume attention scores are not very efficient since they converge to local minima. If we have single attention which converges to quasi-global minimum it would be more efficient?"
"We won't get into that here, but you have to do the delete at every level.
Yeah, question.
AUDIENCE: So what happens if you inserted 10s and you flip off a tail? So that's like your first element is not going to go up all the way, and then have you do search.
SRINIVAS DEVADAS: So typically what happens is you need to have a minus infinity here.
And that's a good point.
It's a corner case.
You have to have a minus infinity that goes up all the way.
Good question.
So the question was what happens if I had something less than 14 and I inserted it? Well, that doesn't happen because nothing is less than minus infinity, and that goes up all the way.
But thanks for bringing it up.
And so we're going to do a little warm-up Lemma.
I don't know if you've ever heard these two terms in juxtaposition like this-- warm up and Lemma.
But here you go, your first warm-up Lemma.
I guess you'd never have a warm-up theorem.
It's a warm-up Lemma for this theorem, which is going to take a while to prove.
This comes down to trying to get a sense of how many levels you're going to have from a probabilistic standpoint.
The number of levels in an n element skip list is order log n.
And I'm going to now define the term with high probability.
So what does this mean exactly? Well, what this means is order log n is something like c log n plus a constant.
Let's ignore the constant and let's stick with c log n.
And with high probability is a probability that is really a function of n and alpha.
And you have this inverse polynomial relationship in the sense that obviously as n grows here, an alpha-- we'll assume that alpha is greater than the 1-- you are going to get a decrease in this quantity.",Any idea how can I understand how log n is derived?
"So P OR Q AND R is equivalent to P OR Q AND P OR R.
So you can see this equivalence in purple has the same structure as the set-theoretic equality in blue, except that union's replaced by OR, intersection's replaced by AND, and set variables A, B, C is replaced by propositional variables P, Q, R.
So let's just remember that we've already proved this propositional equivalence, and we're going to make use of it in the middle of this proof that these two sets are equal.
So again, we said we were going to prove the two sets are equal by showing they have the same points.","how about rewriting (A U B)  (A U C) as (AA) U (B  C) ? I mean that what it LHS implies... right? I mean that's what gonna happen if u calculate that... rest will fall out. 
When proving that the set identity A union (B intersect C) = (A union B) intersect (A union C) is true, is it sufficient to ""go from the LHS to the RHS"" instead of ""going from the LHS to the RHS and back to the LHS"" as shown in the video?"
"So each month mature females are going to produce another pair.
And his question was, how many female rabbits are there at the end of a year, or two years, or three years? The idea is, I start off with two immature rabbits, after one month they've matured, which means after another month, they will have produced a new pair.
After another month, that mature pair has produced another pair, and the immature pair has matured.
Which means, after another month, those two mature pairs are going to produce offspring, and that immature pair has matured.
And you get the idea, and after several months, you get to Australia.
You can also see this is going to be interesting to think about how do you compute this, but what I want you to see is the recursive solution to it.
So how could we capture this? Well here's another way of thinking about it, after the first month, and I know we're going to do this funny thing, we're going to index it 0, so call it month 0.
There is 1 female which is immature.
After the second month, that female is mature and now pregnant which means after the third month it has produced an offspring.
And more generally, that the n-th month, after we get past the first few cases, what do we have? Any female that was there two months ago has produced an offspring.
Because it's taken at least one month to mature, if it hasn't already been mature, and then it's going to produce an offspring.
And any female that was around last month is still around because they never die off.
So this is a little different.","Ken MacDonald Yeah I know but as n goes up, there will be hundreds of steps. So do they recurse on the other because the same rules still applied?"
"And if it encounters a letter that's an i or a u, it's just going to print out this string here.
But this bottom one is a lot more pythonic.
It's an actual word created by the Python community.
And it just looks pretty, right? You can tell what this code's supposed to do.
Whereas this one is a little bit harder to decipher.
So that's sort of an illustration of a for loop over a sequence of characters.
So char is going to be a loop variable, still.
And the loop variable, instead of iterating over a set of numbers, it's going to iterate over every character in s, directly.
And char is going to be a character.
It's going to be a letter.
So here's a more complicated example.","can someone explain for me what is the meaning of iteration in programing in a simple way
What does she mean with we can use for loops to iterate ANY SEQUENCE OF VALUES?? Im learning coding from scratch and some terms and concepts are quite sneaky to comprenhend, dont be to harsh pleas, and thanks
could please tell why does the output not start with 'a' cause the default of start is 0
when the step stated is -1, it cannot start with 0 because there is no letter present physically before position 0 i.e. letter 'a' in this case. alternatively u can read -1 as [len(string) -1] position. string would be s in this case.
The professor mentioned that a previously assigned string stays in memory after the variable is reassigned. Does this mean the programmer has to free memory allocates to the string manually?
what is the return value of function len()??"
"Well, the theta n comes from the fact that I do have to do some sorting.
It's constant time sorting for every column, OK? Because it's only five elements.
So I'm going to do constant time sorting.
But there's order n columns.
Because it's-- then it's n over 5 columns.
So this is the sorting of all of the columns, all right? So that's it.
And I'll just leave you with-- you cannot apply the master theorem for solving this particular recurrence.
But if you make the observation-- and you'll see this in section.
You make the observation that n over 5 plus 7n over 10 is actually less than n.","Can someone explain for me that. Why Arrange S into columns of size 5 and sort each column take linear time? suppose that sorting take oder nlogn. So time complex here is k* 5log5 time which is k equal n/5.
Where did 7n+7/10 + 7 come from in the end?"
"So, it'll be helpful to have a diagram of all possible paths on the board.
We're going to start with a British Museum search.
From the starting position, it's clear, you can go from my s to either a or b.
And already there's an important quiz point.
Whenever we have these kinds of problems on a quiz, we ask you to develop the tree associated with a search in lexical order.
So, the nodes there under s are listed alphabetically, just to have an orderly way of doing it.
So, from a we can go either b or d.
And another convention of the subject, another thing you have to keep in mind in quizzes, is it we don't have these searches bite their own tail.","good lecture sir. I have one doubt. I understood the logic behind this problem but how to write a program on this. I don't know java but i do know C,C++ and C#. how should i start write this program."
"Yes, question? What is the benefit of doing this? Because we can do the same thing with PCA, where you can find the dimension and then use the data to those dimensions.
This would be a lossy encoding.
Good question.
So the question is, how is this different from PCA, right? In PCA, we are doing something very similar.
We start with a d-dimensional data and project it down into a low-- a k-dimensional hidden representation.
And then in PCA, the objective there was to minimize the distance between every point in this projection, right? The main difference between PCA and an autoencoder is that, in PCA, the transformation from x to z is strictly linear, right? Here, you can have multiple hidden layers, which have multiple nonlinearities.
So you can actually perform PCA, interesting.
So, yes, so technically, you could perform a PCA here with this thing, except in PCA, you might also require this-- you need the exact mapping.
Should this be the inverse? You can do something very similar to PCA with this without nonlinearities.
Yeah, you can do it.
OK, so what is the benefit of using it? Is that the same as PCA? So this is not the same as PCA because, here, things can be nonlinear.","Hi sir, could you please give more details on the relation between Factor Analysis and Auto-encoder? When learning Factor Analysis, I would thought that both of them are relatively the same. In Factor Analysis, if we are given a set of examples x, then we can construct the continuous latent z. In order to reconstruct back to x, we can simply apply formula: x_hat (\approx) mu + sigma * z + error. But I think maybe the difference(L2-norm) between (x and x_hat) is not smaller than when using Auto-encoder instead."
"So is this just some hand-wavy argument why you're doing this.
So let's actually try to do some analysis on this.
So let's say you do this transformation.
So do the clauses make sense? Does the first clause sense? So you have 0 xi, 0 xj for every i, j which is not in the graph.
So how does this work? So let's say you have V dash such that size of V dash is greater than or equal to k.
Actually, let's just make size is V dash equal to k.
So if you have a clique of size greater than or equal to k, of course, you have a clique of size equal to k.",why do we have to create a dummy variable z???
"Usually we use n as a variable for what the size of our input is, but that's not always the case.
So for example, if we have an array that I give you-- an n-by-n array, that-- we're going to say n, but what's the size of our input? How much information do I need to convey to you to give you that information? It's n squared.
So that's the size of our input in that context.
Or if I give you a graph, it's usually the number of vertices plus the number of edges.
That's how big-- how much space I would need to convey to you that graph, that information.
We compare how fast an algorithm is with respect to the size of the input.
We'll use the asymptotic notation.
We have big O notation, which corresponds to upper bounds.
We will have omega, which corresponds to lower bounds.
And we have theta, which corresponds to both.
This thing is tight.
It is bounded from above and below by a function of this form.
We have a couple of common functions that relate an algorithm's input size to its performance, some things that we saw all the time.
Can anyone give me some of those? STUDENT: [INAUDIBLE] JASON KU: Say again.
STUDENT: [INAUDIBLE] JASON KU: Sorry.
Sorry.
I'm not asking this question well, but has anyone heard of a linear algorithm-- a linear time algorithm? That's basically saying that the running time of my algorithm-- performance of my algorithm is linear with respect to the size of my input.
Right? Yeah? STUDENT: [INAUDIBLE] JASON KU: Say again.
STUDENT: Like putting something in a list-- JASON KU: Like putting something in a list-- OK.","i forgot, what does prime mean? specifically when he says K prime
Is this course related to data structures...?
why does he use k prime instead of k?
I would suggest you gave a definition of an algorithm first and only after that you go to a definition of a computational algorithm.
I believe an algorithm is not a function as functions have predefined output based on a set of sequential operations. It may be made up of multiple functions but Algorithm is more closely related to the technique to derive a function or procedure to find a solution to the problem.
The definition of the problem is a mathematical relation, but it is not necessarily a function since the problem may have many correct outputs for a given input"
"Right? So when we traverse the knowledge graph to find answers, each step will basically, uh, produce a set of reachable entities, a set of entities that the query covers, uh, so far.
Um, and we are going to now model this set of entities with a box that encloses all of them.
All right.
So, um, this means that we can, uh- that box provide a powerful obstruction because boxes kind of enclose, uh, sets of, uh, entities.
And then we can define geometric intersection operator, uh, over, uh, let's say, two, or multiple boxes, and when- when we intersect them, the intersection is still a box.
So we- it's very easy again to have this compositional property, where we can take boxes, intersect them in arbitrary ways, but, uh, we always are left of- are left out with a box, which we can further kind of manipulate, and change in shape however we like.
So that's why this is kind of cool, um, and, uh, exciting.
So, uh, what does it mean to be a- to be able to embed with boxes? Here is what we need to figure out or here's what we need to learn.
We need to learn uh, opa- uh, we need to learn entity embeddings.
So every embedding, entity will have an embedding, and it will be a box, but it will be a- a- a trivial box.","In defining the entity-to-box distance metric, we considered alpha in (0,1). But, I think instead of penalising more, it is penalising it less. The research paper also says that ""the key here is to downright the distance inside the box by using 0 < alpha < 1. This means that as long as entity vectors are inside the box, we regard them as ""close enough"" to the query centre."" Thus, they are not penalising the points inside the box more to make it more robust. Do I understand something wrong? Please correct me if I am wrong."
"And these elements here are going to be less than x.
So let me clear.
What's happened here is we've not only sorted all of the columns such that you have large elements up here.
Each of these five columns have been sorted that way.
On top of that, I've discovered the particular column that corresponds to the medians of medians.
And this is my x over here.
And it may be the case that these columns aren't sorted.
This one may be larger than that or vice versa-- same thing over there.","Can someone explain for me that. Why Arrange S into columns of size 5 and sort each column take linear time? suppose that sorting take oder nlogn. So time complex here is k* 5log5 time which is k equal n/5.
In median of median do we sort the ""medians"" row too? if not how can we guarantee those picture?
how do you get the medium of mediums?
If we have n/5 columns then to find the median of medians we'll have to call the Select function on the list of medians. We're basically computing 1 subproblem that is 1/5th the size of the original. Hence, the T(n/5) term for finding the median of medians."
"Because it means nothing more than that the remainders are equal.
So for example, we can say the congruence is symmetric, meaning that if a is congruent to b, then b is congruent to a.
And that's obvious cause a congruent to b means that a and b have the same remainder.
So b and a have the same remainder.
One that would actually take a little bit of work to prove from the division definition-- not a lot, but a little bit-- would be that if a is congruent to b, and b is congruent to c, then a is congruent to c.
But we can read it is saying the first says that a and b have the same remainder.
The second says that b and c have the same remainder.
So obviously a and c have the same remainder.
And we've prove this property that's known as transitivity of congruence.
Another simple consequence of the remainder theorem is a little technical result that's enormously useful called remainder lemma, which says simply that a number is congruent to its own remainder, modulo n.
The proof is easy.
Let's prove it by showing that a and the remainder of a have the same remainder.
Well, what if I take remainders of both sides, the left hand side becomes the remainder of a divided by n.","I don't quite understand the final steps of the last example. It seem's he arrived at the answer on the very first line: r(287,4)=3 so the answer is 287^9=3(mod4). Why did he bother with the last 3 steps?
Why does he say 'Equivalent' instead of Congruent?"
"It has a size and it has a head this list-- it has a size and it has a head.
And this head is a pointer to a node, and the node has just one-- two things stored in it.
It has who-- the name of the child that's there, and the next pointer to the next node.
That's what a singly linked list is.
So node has an item key and a next pointer.
This next pointer points to the next node in the sequence.
OK? And the question is asking, if we give you a linked list that has 2n nodes, I want you to take the last n nodes, and reverse their order, and do this to the data structure.",The last problem works the same for lists with a size of an odd number?
"So I think the most important prerequisite is probably some knowledge about probability on the level of CS109 or Stats For example, you probably should be-- at least, you should have heard of these terms like dispersion, random variables, expectation, conditional probability, variance, density, so on and so forth, right? You don't necessarily have to know exactly all of them off the top of your head, but this probably should be something you have seen in one of the previous courses.
Another thing is linear algebra.
Matrices multiplication, eigenvectors.
I guess linear algebra was offered in Math 104, 113, 205.
Actually, there's a longer list of kind of relevant courses in the logistic doc, which taught linear algebra.",If I dont know multivariable calculus and linear algebra then learning ml is not possible ?
"There's nothing left.
There's no other choice.
STUDENT: I have a question.
PATRICK WINSTON: Yeah, [? Lunnare ?]..
STUDENT: So when you jump from P sub 1 to P sub 2, that makes sense.
P sub 2 to P sub 3 you're saying that the probability PATRICK WINSTON: It's the probability depends on the first two choices.
STUDENT: Yeah, but the second choice had probability one minus P sub c times P sub c, not-- PATRICK WINSTON: Think about it this way.
It's the probability you didn't choose the first two.
So the probability you didn't choose the first one is one minus P sub c.","This is very helpful for me. But I have a question. What is Pc ? And how much is it. I watch the screen ,find the rank probability is 0.05. (1-Pc) equals 0.95,so 0.95^39 always more than 0.05,if Pc equals 0.05. I think I need some help.
I find interesting that if you choose Pc < 0.5, then Pn would be grater than Pn-1, because 1-Pc > Pc. Does this mean that you should always choose Pc >= 0.5?
Wischenbart Christian By Pn i was referring to the last probability. Let's say there are 4 individuals in your population. If you choose Pc = 0.3, you would have: P1 = 0.3 P2 = 0.21 P3 = (1-Pc)^(3-1) = 0.7^2 = 0.49 Thus P3 > P2. In the general case, let K be the number of individuals in your population, and Pc < 0.5. Pk-1 = (1-Pc)^(n-2) * Pc and Pk = (1-Pc)^(n-1) = (1-Pc)^(n-2) * (1-Pc). Because Pc < 0.5 => (1-Pc) > 0.5 => Pk>Pk-1"
"But taking hints from some of the questions that it did answer before I got haunted by the music, taking our cue from that, we know that in order to grasp something, in this particular world, you can't have anything on top of it.
So grasp, therefore, may call clear top in order to get stuff off from the target object.
And that may happen in an iterative loop because there may be several things on top.
And how do you get rid of stuff? Well, by calling get rid of.
And that may go around a loop several times.
And then, the way you get rid of stuff is by calling put-on.
So that gives us recursion, and it's from the recursion that we get a lot of the apparent complexity of the program's behavior when it solves a problem.
Now, in order to find space, you also have to call get rid of.
So that's where I meant to put this other iterative loop, not down here.
Cleat top has got the iterative loop inside of it.
So that's the structure of the program.
It's extremely simple.
And you might say to me, well, how can you get such apparently complex-looking behavior out of such a simple program? A legitimate question.
But before we tackle that one head on, I'd like to do a simulation of this program with a very simple blocks problem.","I'm wondering why ""Eats Meat"" has it's own AND gate? Could someone explain this?"
"o to, uh, talk about the third method, uh, we discussed today, I'm going now to, uh, talk about belief propagation.
So this is the final method around collective classification we are going to talk, um, in this, uh, in this lecture.
And really this will be all about what is called, uh, no- uh, message passing, right? We can think of these methods that we have talked about so far, all in terms of kind of messages, beliefs being sent over the edges of the network, a node receiving these messages update- updating its own belief so that in the next iterations its neighbors are able to- to get this, uh, new information and update their own, uh, beliefs as well.
So this is, um, basically what, uh, uh, what is, uh, the core intuition we are trying to explore today, which is all about passing information across the neighbors, either pushing it or receiving it, and updating the belief about, uh, oneself.","Summarising the lecture contents to myself, the majority of thoughts are concerned with potential functions. I suppose, definition of Phi and Psi is the part engineering work that depend on the specific task. It's obviously the most non-trivial step in the belief propagation approach. I'd fancy a couple of examples of how can they be defined in specific tasks."
"But if vi is positive, I'd actually prefer this over that.
So you can figure out which is better, just locally.
But then there's another thing I can do, which is maybe I hit this pin in a pair with some other pin.
Now, there's no pin to the left of this one.
We're assuming we only have the suffix.
And so the only other thing I can do is throw a ball and hit i together with i plus 1.
And then I get the product.
Now, what pins remain? i plus 2 on.
Still a suffix.
So if I remove one or two items, of course, I still get a suffix-- in this case, b of i plus 2-- and then the number of points that I add on are vi times vi plus 1.
So this is a max of three things.
So how long does it take me to compute it? I claim constant time.
If I don't count the time it takes to compute these other sub problems, which are smaller because they are smaller suffixes further to the right, then I'm doing a couple of additions-- product, max.
These are all nice numbers and I'll assume that they live in the w-bit word, because we're only doing constant sized products.
That's good.
So this takes constant, constant non-recursive work.
How many sub problems are? Well, it's suffixes, so it's a linear number of sub problems.
And so the time I'm going to end up needing is number of sub problems, n, times the non-recursive work I do per sub problem, which is constant.
And so this is linear time.
Great.
And I didn't finish SRTBOT, so there's another t, which is to make sure that there is a topological order and that is in decreasing i order.","What is n-bit addition? I did not understand that particular thing. Is it the representation of total digits?
Question: is the proposed algorithm optimal when pins are -1,-1,-9,1? In the first step it will choose to hit first and second pins -1 and -1. On the second step the algorithm will choose to skip -9. On the third step the algorithm will hit 1. Overall score will be 2 if we use the algorithm. But optimal solution is to hit the second and the third pins -1 and -9, and then hit the last, so the score would be 10. Or I misunderstood something?"
"This is 2, minus 1.
Okay, so if I take the dot product between this, I get minus 2 and then the sign of minus 2 is, is minus 1, okay, so that's a minus.
Um, and what about this one? So what's the dot product there? It's gonna be 0.
Okay.
So, um, so this classifier will classify this point as a positive.
This is a negative and this one I don't know.
Okay.
So we can fill in more points.
Um, but, but, you know, does anyone see kind of um, maybe a more general pattern? I don't wanna have to fill in the entire board with classifications.","How come the vector [2 -1] can be perpendicular to the vector [2 4]? Instead, maybe you could've meant [1 2].
Their dot product is apparently 0, they should be perpendicular (you're probably confusing ""perpendicular"" with ""parallel"")."
"Um, but for now, you think about them as a complete sentence.
And I'm using W, because sometimes you also call them, um, worlds.
Um, because a complete assignments slash a model, is both to represent the state of the world at any one particular point in time.
Yeah.
[inaudible] Yeah.
So the question is, can each propositional symbol either be true or false? And in logic, as I'm presenting it, yes.
Only true or false or 0 or 1.
Okay? So these are models.
Um, and next is a key thing that actually defines the semantics, which is the interpretation function.
So the interpretation function, um, takes a formula and a model and returns true if that formula is, uh, is true in this model and false, um, you know, otherwise.
Okay? So I can make the interpretation function whatever I want, um, and that just gives me the semantics.
So when I talk about what are the semantics, it's the interpretation function.
Function i of f, w.
[NOISE] So the way to think about this is, um, I'm gonna represent formulas as these, uh, horizontal bars.
Okay? So this is- think about this as, uh, a thing you'd say.
It sits outside though, uh, reality in so- in some sense.
And then this box, I'm gonna draw on the space of all possible models.
So think about this as a space of situations, uh, that we could be in the world, and a point here corresponds to a particular model.
So an interpretation function takes one, a formula, takes, um, a model and says, ""Is this statement true if the world looks like this?"" Okay? So just to ground this out, um, a little bit more, um, I'm gonna define this for propositional logic, again recursively.",I have a few doubts from this lecture: 1) can we say A <-> B based on one w alone (egs: A=0 & B=0 in w) ? 2) Can any provide an example (interms of truth values) where A -> B doesnt imply A <-> B
"So this was in terms of relational classification based on the labels alone.
And you saw how basically the- the probabilities kind of spread from labeled nodes to unlabeled nodes through these- um, through this iterative classification, where nodes were updating, uh, the probabilities, based on the probabilities of their nearby nodes or their neighbors in the network.
So now, we are going to look at the iterative, uh, classification, a different kind of type of approach that will use more, uh, of the information.
In particular, it's going to use both the node features as well as the labels, uh, of the nearby nodes.
So this will now combine both the network as well as the, uh, um, the feature information of the nodes.
So in the relational classifiers that we have just talked about, they do not use node attributes, they just use the network structure and the labels.
So now the question is, how do we label? How do we take advantage? How do we harness the information in node attributes? And the main idea of iterative classification, is to classify node v based on its attributes f, as well as the labels z of the neighboring nodes, uh, N sub v of this, uh, node v of interest.
So how is this going to work? On the input, we are given a graph.
Each node will have a feature vector associated with it, and some nodes will be labeled with, uh, labels y.
The task is to predict labels of unlabeled nodes and the way we are going to do this is that we are going to train two classifiers.
We are going first to train a base classifier that is going to predict a label of a node, uh, based on its feature vector alone.
That's the classifier Phi_1.
And then, we are also going to train another classifier that will have two inputs.","What is the point on the feature-only classifier phi-1 ? Professor never explained this. Just train one model, Phi-2, which uses features and neighbors and use regularization. One model is enough. Use XG-Boost. You don't even need to iterate. Train on the known labels and predict on the unknown. I believe that a data scientist would never do this method because it is so unnecessary with extra programming work for no benefit.
Thank you for the lecture! I haven't come across methods such as Iterative classification before. The idea - using a separately trained classifier in an iterative non-trainable process - sounds a bit unnatural to me. My intuition tells me it's better to train the classifier inside each iteration, thus making the iterative process trainable. I suppose, that's when is going to happen in GNNs. A couple of comments about the training set. We don't need any graph structure to train phi1 classifier, therefore it can be done with traditional methods. In order to train phi2 classifier, we need labeled nodes that have all their neighbours also labeled - that's necessary to set z_v. We don't necessary need to split all graphs into training and test sets. Instead, for phi2 training we can use all the nodes with mentioned property - as it's not going to be evaluated during the iterative stage anyway (as the labels are known - and thus 'converged' already).
phi1 is required at iteration 0 to create the initial labels at inference. In later iterations, only phi-2 is needed."
"From a- from a machine- from an applied machine learning operational what you write code point of view, it doesn't matter that much.
Uh, yeah.
But theta is not a random variable, we have likelihood of parameters which are not a variable.
Yeah.
Go ahead.
[inaudible].
Oh, what's the rationale for choosing, uh, oh, sure, why is epsilon i Gaussian? So, uh, uh, turns out because of central limit theorem, uh, from statistics, uh, most error distributions are Gaussian, right? If something is- if there's an era that's made up of lots of little noise sources which are not too correlated, then by central limit theorem it will be Gaussian.
So if you think that, most perturbations are, the mood of the seller, what's the school district, you know, what's the weather like, or access to transportation, and all of these sources are not too correlated, and you add them up then the distribution will be Gaussian.
Um, and, and I think- well, yeah.
So you can use the central limit theorem, I think the Gaussian has become a default noise distribution.
But for things where the true noise distribution is very far from Gaussian, uh, this model does do that as well.
And in fact, for when you see generalized linear models on Wednesday, you see when- how to generalize all of these algorithms to very different distributions like Poisson, and so on.
All right.
So, um, so we've seen the likelihood of the parameters theta.
Um, so I'm gonna use lower case l to denote the log-likelihood.
And the log-likelihood is just the log of the likelihood.
Um, and so- well, just- right.
And so, um, log of a product is equal to the sum of the logs.","Since when these these basic statistics techniques become machine learning??
Can you guys help me out ? I can't get my head around likelihood of theta thing ....why this is equal to product of probabilities of Y"
"We then say, well, actually, in each position, we know what word is actually next.
So when we're at time step 1, the actual next word is students, because we can see it just to the right of us here and we say, what probability estimate did you give to students and to the extent that it's not high? I.e., it's not 1.
We take a loss.
And then we go on to the time step 2, and we say, well, at time step 2, you predict the probability distribution over words.
The actual next word is opened.
So to the extent that you haven't given the high probability to open, you take a loss and then that repeats.
And time step 4, we're hoping the model will predict their.
At time step 4, we are hoping the model will predict exams.
And then to work out our overall loss within averaging out per time step loss.
So in a way this is a pretty obvious thing to do.
But note that there is a little subtlety here.
And in particular, this algorithm is referred to in the literature as teacher forcing.","This is great, this course is good for understanding basics of nlp and best methods, but why I never heard of comparing actual output with predicted output while talking about reducing the error did i miss something?"
"So, at least in theory, the hidden state at the end can have access to the information from the input from many steps ago.
Another advantage is that the model size doesn't increase for longer inputs.
So, uh, the size of the model is actually fixed.
It's just WH and WE,s and then also the biases and also the embedding matrix, if you're counting that.
None of those get bigger if you want to apply it to more, uh, longer inputs because you just apply the same weights repeatedly.
And another advantage is that you have the same weights applied on every time-step.
So I said this thing before about how the fixed-sized window neural model, it was less efficient because it was applying different weights of the weight matrix to the different, uh, words in the window.",What is the physical interpretation of the hidden state and corresponding weights matrix?
"So you can think of it as it's not median finding, but finding elements with a certain rank.
And we want to do this in linear time, OK? So we're going to apply divide and conquer here.
And as always, the template can be instantiated.
And the devil is in the details of either division or merge.
And we had most of our fun with convex hull on the merge operation.
It turns out most of the fun here with respect to median finding is in the divide, OK? So what I want is the definition of a select routine that takes a set of numbers s.
And this is the rank.
So I want a rank i.
And that i might be n over 2-- well, floor of n plus 1 over 2, whatever? And so what does the divide and conquer look like? Well, the first thing you need to do is divide.","Can someone explain for me that. Why Arrange S into columns of size 5 and sort each column take linear time? suppose that sorting take oder nlogn. So time complex here is k* 5log5 time which is k equal n/5.
In median of median do we sort the ""medians"" row too? if not how can we guarantee those picture?
If we have n/5 columns then to find the median of medians we'll have to call the Select function on the list of medians. We're basically computing 1 subproblem that is 1/5th the size of the original. Hence, the T(n/5) term for finding the median of medians."
"No space.
So this just joins every one of the elements in the list together.
So it'll return the string abc.
And then you can join on any character that you would want.
So in this case, you can join on the underscore.
So it'll put whatever characters in here in between every one of the elements in your list.
So pretty useful functions.
OK.
Couple other operations we can do on lists-- and these are also pretty useful-- is to sort lists and to reverse lists and many, many others in the Python documentation.
So sort and sorted both sort lists, but one of them mutates the list and the other one does not.
And sometimes it's useful to use one, and sometimes it's useful to use the other.","I'll make a comment on it, since for some reason this really confused me: why .sort() returns none. (If I get something wrong, feel free to correct me and I will edit this comment, I am new to coding.) The reason that list.sort() and list.reverse() return None, while sorted(list) returns a list's values, is basically that the methods are completed by different means. Code that returns None is completed ""in-place,"" which means that the original variable itself is not duplicated, only changed. Python does not want you to think that a new variable has been/could be created after running the method .sort() or .reverse() (or any other in-place method), and so it returns None. It is basically telling you that any method that is done in-place CANNOT generate a new variable because it has not duplicated the old variable. Meanwhile, sorted(list) is creating a new list entirely, and so you can assign it to a new variable name. ( sortedList = sorted(originalList) ) So basically, don't assign in-place methods to any new variables.
In that last example, couldn't one iterate through the list backwards?
What is the difference between append and extend of the list."
"And I want to have a non-null subsequence, so I'm going to have i less than or equal to j.
I'm good with i being equal to j, because I still have one letter, and, well, that happens to be a palindrome, and it'll have a length of 1.
So that's my lij.
And what I want to do is define a recursive algorithm that computes lij.
So we'll just try and figure out what the recurrence looks like, and then we can talk about memoization or iteration.
So if i equals equals j, then I'm going to return 1, because I know that that's a palindrome by default.","One thing I don't get about all those recursions is, why are you guys don't mention that stack is limited and able to hold limited recursive calls to functions, and for some big enough input number for problem to be solved for, you might get stack overflow. Or is this all just theory and one shouldn't be thinking about how to use it in practice?"
"But I want to start with recursion.
Perhaps one of the most mysterious, at least according to programmer's, concepts in computer science, one that leads to lots of really bad computer science jokes, actually all computer science jokes are bad, but these are particularly bad.
So let's start with the obvious question, what is recursion? If you go to the ultimate source of knowledge, Wikipedia, you get something that says, in essence, recursion is the process of repeating items in a self-similar way.
Well that's really helpful, right? But we're going to see that idea because recursion, as we're going to see in a second, is the idea of taking a problem and reducing it to a smaller version of the same problem, and using that idea to actually tackle a bunch of really interesting problems.
But recursion gets used in a lot of places.
So it's this idea of using, or repeating, the idea multiple times.
So wouldn't it be great if your 3D printer printed 3D printers? And you could just keep doing that all the way along.
Or one that's a little more common, it's actually got a wonderful name, it's called mise en abyme, in art, sometimes referred to as the Droste effect, pictures that have inside them a picture of the picture, which has inside them a picture of the picture, and you get the idea.
And of course, one of the things you want to think about in recursion is not to have it go on infinitely.","I am so confused about the tower thing
Would inclusion in the lecture of the 'call stack' or Python's symbol table concept help explain recursion? As you recursively call the function object's return value, the frames get 'popped' off the stack (symbol table on Python I think?) Sorry, self-taught. Still learning every day.
I'm a little confused on one point. I understand recursion, but from what I am reading online, most of the time it is more efficient in terms of processing times to write code using an iteration than a recursion. I get that some people might find one more readable than the other and that for many programs, most users wouldn't notice a difference, but wouldn't it be a best practice to normally iterate to make code as efficient as possible?"
"I'm going back to the last lecture just to kind of talk about some of the stuff that we didn't cover last time, okay? All right.
So if you remember last time, we were talking about search problems.
So big switch now.
Search problems, where we don't have probabilities, and we talked about A-star as a way of just making things faster, and we talked about this idea of relaxations which was, uh, a way of finding good heuristics.
So A-star had this heuristic.
Heuristic was an estimate of future costs.
We wanted to figure out how to find these heuristics, like, how do you go about finding these heuristics? And one idea was just to relax everything, that allows you to come up with an easier search problem or just an easier problem, and that helps you to find what the heuristic is, okay? So, um, [NOISE] so we talked about this idea of removing constraints, and when you remove constraints, then you can end up in nice situations.
Like in some settings, you have a closed-form solution.
In some other settings, you have just an easier search problem, and you can solve that, and in some other settings, you have like independent sub-problems.
So when you remove constraints then, then you have this easier problem.
You can solve that easier problem, and that gives you a heuristic.
You're not done yet, right? You're- you have a heuristic.
You take that heuristic, and then change your costs, and then just run uniform cost search on your original problem.",would not removing constraint increase search space making computationally inefficent?
"So let's say this guy is pointing in.
So if this is also pointing down, then this can point up.
But if this is pointing down, this must point up and to the left in order to satisfy this node.
So if this is coming in, this one must be pointing out.
So I'm going to say that's the output.
These are the two inputs.
I'm guessing symmetrically, yeah.
If this one is pointing into the vertex, then that must mean both of these red guys are pointing into that vertex, which means this guy must be pointing out of that vertex.
So if this one is active for this guy, this guy's inactive.
And vice versa.
So this is protected from those two.","at 628, i cannt understand the point of picture meaning,blue is 2,red is 1. the point between blue and red means =+, but the point between blue and blue means =, what about point between red and red means += or =condition of points is different?"
"You get these collisions and information, uh, gets lost, and that decreases the expressive power, uh, of the graph neural network.
So, let's summarize what we have learned so far.
We have analyzed the expressive power of graph neural networks, and the main takeaways are the following; the expressive power of a graph neural network can be characterized by the expressive power of its neighborhood aggregation function, right? So the message aggregation function.
Neighborhood aggregation is a function over multi-sets, basically sets with repeating elements.
Um, and GCN and GraphSAGE aggregation functions fail to distinguish some of the basic multi-sets.
Meaning these two aggregation functions, mean and maximum, are not injective, which means different inputs get mapped into the same output, and this way, the information gets lost.
Therefore, GCN and GraphSAGE are not maximally powerful graph neural networks.
They're not maximally expressive, uh, graph neural networks.
So let's now move on and say, can we design the most expressive graph neural network, right? So our goal will be to design maximally powerful graph neural network, uh, among all possible message-passing graph neural networks.",Are there applications where one would prefer GCNs or GraphSage over the most expressive GINs (Graph Isomorphism Networks)?
"And so that's the way this course is focused.
What we will do is, for example, for today, we'll learn a little bit about software engineering.
Then, we'll do two lab sessions where you actually try to use the things we talk about.
Then, we'll come back to lecture and we'll have some more theory about how you would do programming.
And then, you go back to the lab and do some more stuff.
And the hope is that by this tangible context, you'll have a deeper appreciation of the ideas that we're trying to convey.
So let me tell you a little bit about the four modules that we've chosen.
The course is going to be organized on four modules.","wait, isn't electronics a SUBFIELD in electrical engineering. So technically electronics engineering isn't part of electrical engineering because they're two different majors"
"We evaluate its value and then we try to improve it.
If we can't improve it any more, um then we can- then we can halt.
So, the idea is that we start by initializing randomly.
Here now you can think of the subscript is indexing which policy we're at.
So, initially we start off with some random policy and then _i is always going to index sort of our current guess of what the optimal policy might be.
So, what we do is we initialize our policy randomly and while it's not changing and we'll talk about whether or not it can change or go back to the same one in a second, we do value function policy.","we said if policy is deterministic we can simplify value function to Vk(s) = r(s, (s)) + Xs0Sp(s0|s, (s))Vk1(s0) but how we can write max(a) Q(s,a) >= V(s) when policy is deterministic and we can choose just one action?"
"And I've initialized the age and the name for you.
The next thing I'm doing in the __init__ is I'm going to set the name to whatever name was passed in, OK? So in the __init__, notice, I can do whatever I want, including calling methods.
And then, the last thing I'm doing here is I'm going to create a new data attribute for Person, which is a list of friends, OK? So an animal didn't have a list of friends, but a person is going to.
The next four methods here are-- this one's a getter, so it's going to return the list of friends.
This is going to append a friend to the end of my list.
I want to make a note that I actually didn't write a method to remove friends.
So once you get a friend, they're friends for life.
But that's OK.
The next method here is speak(), which is going to print ""hello"" to the screen.
And the last method here is going to get the age difference between two people.
So that just basically subtracts their age and says it's a five-year age difference, or whatever it is.
And down here, I have an __str__ method, which I've overridden from the Animal, which, instead of ""animal: name,"" it's going to say ""person: name : age,"" OK? So we can run this code.
So that's down here.
I have an animal person here.
So I'm going to run this code.
And what did I do? I created a new person.
I gave it a name and an age.
I created another person, a name and an age.
And here I've just run some methods on it, which was get_name(), get_age(), get_name(), and get_age() for each of the two people.
So that printed, Jack is 30, Jill is 25.
If I print p1, this is going to use the __str__ method of Person.","Excuse me but I don't quite understand why the _eq_ method will be called over and over again when comparing objects directly. Thanks in advance~~^^
Hi, Why with the class cat(Animal) example do you leave out the _init_ statement, but with the person (animal) example do you include the _init_ statement of animal.__init__ etc....? If the cat example inherits name, age etc.. of animal class then why do you have to write Animal.__init__(self, age)? Won't the person class inherit this without the statement and if not what makes it different to the cat class? Thank you
I can still call this outside the class though, so its not private in the same sense as in Java for example?"
"So we are summing over everyone but the j or multiplying go- everyone by j.
Where now for every neighbor, uh, k here, these three neighbors, we are asking, what is your belief about node, um, Y being in class, uh, Y sub i, right? So now basically what this means is that node i collects beliefs from its downstream neighbors, um, aggregates the beliefs, multiplies with the- its pra- its- its belief what its own class should be.
And then- and then applies the label potential, um, matrix to now send a message to node j about how i's label should influence j's, uh, label.
And this is the core of, uh, loopy belief, uh, propagation, right? And, uh, we can keep iterating this, uh, equation until we reach, uh, some convergence, right? When basically we collect messages, uh, we transform them, and send that message onward to the next level, uh, neighbor, right? And after this approach, this iteration is going to converge.","Summarising the lecture contents to myself, the majority of thoughts are concerned with potential functions. I suppose, definition of Phi and Psi is the part engineering work that depend on the specific task. It's obviously the most non-trivial step in the belief propagation approach. I'd fancy a couple of examples of how can they be defined in specific tasks."
"And so for a given query, you sum over all the keys to get your normalization constant, normalizing by that gives you a distribution over the sequence length T.
So now you have sort of a weight on all of the indices.
And again, we do our weighted average, right? So we've got our weights for our average and then the output, right, there's going to be one output per query.
And the output is the weights for that multiplied by the value vectors, right? So again if you set the keys, the queries, the values to all be x, this makes sense, but it's nice to have the qs and the ks to know sort of, which thing is doing what? You can think of the query as being sort of looking for information somewhere the key as interacting with the query, and then the value is the thing that you're actually going to weight in your average and output.
John, a question you might like to answer is, so if now we're connecting everything to everything, how is this different to using a fully connected layer? That's a great question.
A couple of reasons.
One is that unlike a fully connected layer, you get to learn the interaction weights.
Well, the interaction weights are dynamic as a function of what the actual values here are, right? So in a fully connected layer, you have these weights that you're learning slowly over the course of the training your network, that allow you to say sort of which hidden units you should be looking at.","Are matrices K, Q, V parameters to be learned?"
"And so here, Python is going to group these from left to right.
So 12 minus 6 minus 6 is actually 12 minus 6 minus 6, and so that would actually give us a 0.
If you're ever in doubt and you're not really sure what the precedent's operators are, I'm just putting in parentheses.
By using the parentheses, you completely control what order these things get carried out and it's totally acceptable to go ahead and put an extra parentheses.
And I often actually put in parentheses to just sort of emphasize what's really going on.
So one thing that happens when we're starting to carry out complex calculations is sometimes we'll want to store intermediate results.
So if I'm trying to carry out these operations in my head, I may just write some intermediate results on a piece of paper.
If we're working with a calculator, fancier calculator actually have calculator memory and the super fancy calculator actually have multiple memory so I could say, hey, here's an intermediate result I want to start that memory 1, here's intermediate result I want to start that memory 2, let me recall it's in memory 1, let me recall it to memory 2.",Why cant 12-6-6 be 12+(-6-6) or (12-6)-6? Is separating the - from the -6 allowed?
"Right.
So kernels can be seen as similarity metrics, which means if you can, um, so, ah, kernels are generally constructed such that similar examples evaluate to a higher value in the kernel and dissimilar examples evaluate to a small value in the kernel, right? And this idea of using kernels as similarity metrics will- will actually show up, in- in, uh, one of the future topics that we're going to cover called Gaussian processes, right? Gaussian processes is- is a kernel method, uh, algorithm, and, over there, um, kernel acting as a similarity metric is kind of highlighted.
[NOISE] In your example B, I- I can't read what's written there.
I'm sorry.
In the second set of parenthesis squared.
This one? Yeah.
So this is x transpose z plus c.
Oh, c.
Some constant c.
Cool.
Whole squared, and that C shows up over here.
Thanks.
Sorry.
Um, so, kernels can be seen as similarity metrics.
And we want k of x, z to be high for similar x, z.
And we want it to be low for not similar x, z, right? And- and this should be kind of - this, this might be obvious.
Uh, the reason why we- we, uh, we say this is because k of [NOISE] x, z is equal to Phi of x transpose Phi of z.
And we've seen that for similarly oriented vectors, Phi x and Phi z their dot- their dot product will be high if they are oriented somewhat similarly, which means they are, you know,  similar examples, and similar, you know, the dot-product between you know,  opposite vectors, pointing opposite is going to be negative, uh, which means if they're not similar, the- the kernel will, will, will evaluate them to, uh, be a smaller value.
And in fact, there is also this kernel which is very, uh, popular, k of x, z equal to exponent of minus x minus z squared over 2 Sigma squared.",what is z in thata kernel function?
"But, um, essentially what you wanna think of this, you give some shape as input, you know.
Uh, you can characterize what a matrix A is doing by thinking of it as first rotating it by some amount, and then scale the rotated version by different amounts along, you know, the x and y-axis, [NOISE] and then rotate it by a different amount, right? And it could be- it is- it is, uh, different for SVD in the sense it could be, you know, arbitrarily different, but for the eigenvalue decomposition, you are gonna just undo the initial rotation, right? And what you- what you, uh, what you see is that the- the direction that ends up aligning with the axis after the first rotation are the directions of your eigenvectors, right? Because you're gonna- you're gonna scale tho- scale the points along- the points along the eigenvectors after rotation will end up aligning with the x-axis.
And when you scale them along the axis, these points are not changing their direction once you undo the rotation.
Does that make sense? So, uh, what this means, again, in- is that, uh, for square matrices, um, especially square symmetric matrices where we don't care about complex eigenvalues, for all square symmetric matrices, we are- we- we gonna rotate the- the action of A can be summarized at- as rotated such that the eigenvectors align with the axis.
Scale it by different amounts.
The scaling could be negative, which means you're, kind of, mirroring it.
And then undo the rotation you did in the first step, which means the- the eigenvectors and the axes of the ellipsoid are gonna be the same, right? Now, for singular value, uh, decomposition, um, there are no eigenvectors.
Um, there are singular vectors, uh, but the- the, uh, interpretation is- is not- not- not as easy because, um, you're effectively rotating it by some amount, scaling it along the axes, and not undoing rotation, but just rotating by a different, you know- by U instead of V inverse.",I have a question at https://youtu.be/b0HvwszmqcQ?t=3327. If the eigen values are equal that means do we get a scaled sphere instead of an ellipsoid?
"It captures the structure of, uh, many, uh, real-world networks.
Uh, and it accounts for high clustering in- in those networks, um, but what is still missing is, uh, the degree distribution, right? The- the- in- in- how- the way we defined the small-world model, uh, all the nodes have the same degree.
You know, in our case, I think all the nodes had degree 4 which each- which is kind of unrealistic as we saw from the messenger network.
But just, um, basically through these two models that they have presented, I wanted to show you how really you can- you can think of this as a- as a way to explain formation of networks.
And you can think of it as a way to, um, capture different properties of the network.
And think about what kind of processes might be happening in the real world, in, you know, in our everyday social networks that give rise to the networks.
Uh, with properties that we actually observe, uh, in real life.
","Excuse for for asking such a basic question, but I couldn't find the answer anywhere. Why is low clustering bad? Why do we want clusters?
Hi Rahad, thanks for your answer. What do you mean by ""simpler similarities"" and ""character of each cluster""? And why do you speak of interpretation? What is there to interpret? As far as I understand, you use the small world model mainly to quickly find an approximate nearest neighbor, so it doesn't really seem to me like there is anything to interpret."
"So those are the sequences of numbers.
So in this first code right here, my sum is going to get the value 0.
And you're going to have a for loop.
We're going to start from 7, because we're giving it two numbers.
And when you give it two numbers, it represents start and stop with step being 1.
So we're starting at 7.
If step is 1, the next value is 8.
What's the value after that? If we're incrementing by 1? 9.
And since we're going until stop minus 1, we're not actually going to pick up on 10.
So this loop variable, i, the very first time through the loop is going to have the value 7.
So my sum is going to be 0 plus 7.
That's everything that's inside the code block.
The next time through the loop, i gets the value 8.
So inside the for loop, my sum gets whatever the previous value was, which was 7, plus 8.
OK.
The next time through the loop, my sum get the value 7 plus 8 plus 9.
Obviously, replacing that with the previous value.
So 15.
Since we're not going through 10, that's where we stop.
And we're going to print out my sum, which is going to be the value of 7 plus 8 plus 9.
Yeah? OK.
Yeah.
AUDIENCE: [INAUDIBLE] PROFESSOR: Do they have to be integers? That's a great question.
We can try that out.
I'm not actually sure right off the top of my head.
So you can go on Spider and say-- let's say in this example here.
So we can say 7.1, 10.3-- yeah.
So they have to be integers.
OK.
So that's that example.
And let's erase that.","the number you mention in parenthesis in for loop doesn't printed on the screen
How come the infinite loop didnt work with 0 but worked with p
Could someone please explain what's the difference between the ""+="" and ""=+"" operators?
on the 40th minute why i has the values 5 7 9 not 5 6 7 8 9 ????? please explain !!!"
"OK? So just find a motivation here before we start writing our own types of objects is the advantages of object oriented programming is really that you're able to bundle this data, bundle some internal representation, and some ways to interact with a program into these packages.
And with these packages, you can create objects and all of these objects are going to behave the exact same way.
They're going to have the same internal representation and the same way that you can interact with them.
And ultimately, this is going to contribute to the decomposition and abstraction ideas that we talked about when we talked about functions.
And that means that you're going to be able to write code that's a lot more reusable and a lot easier to read in the future.
OK.
So just like when we talked about functions, we're going to sort of separate the code that we talk about today into code where you implement a data type and code where you use an object that you create.","I understand what classes are.But what are the use of classes? What are the things that can be done in class but not in a normal function?
I get the idea but why and when do we should use OOP. Why don't we just use the existent classes such as tuples, lists, dictionaries? Any examples for comparision between normal programming and OOP?
This is a question I have had. I ""get"" OOP but what are real world examples of where OOP is beneficial over Procedural programming? Someone explaining that would be very helpful."
"If this is the case, then I'm going to put it on the unit circle.
The second thing that I notice is that it's not monotonic and it's not alternating.
This is oscillating.
So in order to determine what angle I'm going to sign to my unit simple response, I'm going to count out the time steps that it takes to cycle through an entire period and then from there figure out what the angle would have to be in order to determine a period of that length.
So I start here.
I'm just going to count 1, 2, 3, 4, 5, 6, 7, 8 -- to complete one full oscillation.
This means that my period is 8.
If I have to divide 2pi by a particular angle in order to get out 8, I want to divide by pi/4.
So at this point, I'm working with a magnitude of about 1, and I want this angle to be about pi/4.
This concludes my tutorial on solving poles.
Next time, we'll end up talking about circuits.
",How do you get .07 from 1.6 and .09 from -.63? I have tried looking this up but can't figure it out.
"So now, if I only had a unit normal that's normal to the median line of the street, if it's a unit normal, then I could just take the dot product or that unit normal and this difference vector, and that would be the width of the street, right? So in other words, if I had a unit vector in that direction, then I could just dot the two together, and that would be the width of the street.
So let me write that down before I forget.
So the width is equal to x plus minus x minus.
OK.
That's the difference vector.
And now, I've got to multiple it by unit vector.
But wait a minute.
I said that that w is a normal, right? The w is a normal.
So what I can do is I can multiply this times w, and then, we'll divide by the magnitude of w, and that will make it a unit vector.","I have a question. At the start of the lecture, the professor says that we can imagine W to be a normal vector of any length and that the distance of a point from the hyperplane is the projection of any point onto W. But how can the dot product of WX be equal to the length of the projection of X onto W if W is any length? This can only work if W is a unit vector unless I'm mistaken. WX = ||W||*||X||*cos(theta) and the projection length of X onto W is ||X||*cos(theta). So WX can only equal ||X||*cos(theta) if ||W|| = 1. Otherwise ||X||*cos(theta) = WX/||W||.
dot product dont give projection we need to multipy it either by |x| or |y| to give us the projection im i right ?
thanks! Khan academy's multivariable calculus playlist helped too! But i dont understand the first derivation step though. How'd the derivative of w's magnitude turn into the vector itself? Is that a formula? Cuz when i tried I could just get to |w|d(|w|)/dw
Why is the projection originally defined as w dot x = c and not the unit vector of w dot x?
anyone have doubt how he got width=2/|w|,ask me i help you
Why is width equal to the first equation?
I dont get how he is findeing the width of the street! x1- x2 =1-b-(1+b)= -2b!? correct me if i am wrong!
nice one....I understand more easily than before this...but one doubt I satisfied mathematical convenience of Yi = + or - 1. But I didn't understand mathematical convenience of 1/2 ||w|| ^2
why did we choose vector w perpendicular to the boundary ?. why wont w vector project on vector u when taken the dot product?
Isn't the projection of vector u on the vector w = (dot_product(u,w)/||w||) ? Why is he considering only the dot product? Can someone please explain it to me? If we consider value of ||w||, the width comes to be exactly 2. How can we find extremes of it? Am I interpretating anything in a wrong way?
Still don't get how (1 - b + 1 + b).w/||w|| = 2/||w|| is it because (1 - b + 1 + b) = 2 and 2*unit vector w = 2 therefore 2/||w|| ?
I don't understand it, too. How can 2*w/||w||=2/||w||. w = vector. 2*w = vector, 2*w/||w||= vector != skalar = 2/||w||. I think there must be a mistake
In this Equation: Margin Width= ((x+) - (x-)) dot with Unit vector of W Why ((x+) - (x-)) dot with Unit vector of W = Margin Width Can anyone help me?
Why is he claiming and using the dot product to be the length of a vector projection onto another vector? Shouldn't he also divide by the other vector length??
I believe the length of the projection is w.u/|w| isn't it?
wx + b >= 1 or <=1 , this means we are assuming that perpendicular distance between the two support vectors will always be 2 , how wx+b>=0 or <=0 makes sense but >=1 and <=1 how this is gonna hold ?
Bhullar, I can not get your logic. How come you use the value of a non-expanded variable on the same variables expanded values where if you expand the non-expanded variable, the outcome will completely change? Also, how come you say that, he did not expanded the the w(vector) in first equation and expanded it in second equation where he did say that at 16.39 ""w(vector) is a normal and what i can do is take the w and divide by the magnitude of w and that will make it unit normal "" ? So it was not unit normal, he made it unit normal by dividing with magnitude to get the width of the street. can you prove me wrong? I am just trying to understand. May be I am wrong. Also I am agree with Sebouth's last question.
Mainul Quraishi Things start to make sense when you take w to be a unit normal vector, but fall apart afterwards. Based on the ""Hesse normal form"", w.x + b = 0 is the equation of the hyperplane s.t. w is a unit normal vector. But then it's as if we calling some other vector w and substituting it.
I think we can consider that the best possible distance between positive and negative sample is taken as 2 for this particular example and the calculation is done accordingly. But the two distances from the median line are itself dependent on finding the median line(if you see it it's actually distance c(-b) thats represents the median line).We can assume the +1,-1given here as any real value >0 but will always be equal and will change according to datapoints we have .Here it's safe to assume the data sets are at a distance 2 but we are still to find the median. Hope that helps.
+ranvideogamer I see he made a mistake: x(pos) = 1 - b while x(neg) = -1 - b so x(pos) - x(neg) = 1-b - ( -1-b ) = 2. I also have another question as to why it's just 2 over the length/magnitude of w, what happened to the vector w on top of the magnitude?
but why is it xplus + xminus? On the board, it says xplus - xminus. Shouldn't it be minus?
Nice Tutorial Question : 1) At the time of calculating the width of the street, why the unit vector component is multiplied ?
jupiter 6 Isn't the projection of vector u on the vector w = (dot_product(u,w)/||w||) ? Why is he considering only the dot product? Can you please explain it to me? Also, If we consider value of ||w||, the width comes to be exactly 2. How can we find extremes of it? Am I interpretating anything in a wrong way?
***** beause (x+) - (x-) is not perpendicular to the median line. For calculate the width of the street, you need a perpendicular vector. When you multiple it with the unit vector, it becames perpendicular.
+Murat Bykaksu How do we know multiplying the perpendicular vector by the difference vector gets us the perpendicular width between the boundary samples? The way I'm thinking of the width between the margin is as the opposite side of a triangle. The projection of x- onto x+ is the base, and the difference vector is the hypotenuse. Is there an error in my way of thinking?"
"In particular, what we're going to do is compute our shortest path distances, the shortest path waits for every vertex in our graph, setting the ones that are not reachable to infinity, and the ones that are reachable through a negative weight cycle to minus infinity, and all other ones we're going to set to a finite weight.
And another thing that we might want is if there's a negative weight cycle in the graph, let's return one.
So those are the two kinds of things that we're trying to solve in today's lecture.
But before we do that, let's warm up with two short exercises.
The first one, exercise 1, given an undirected graph, given undirected graph G, return whether G contains a negative weight cycle.
Anyone have an idea of how we can solve this in linear time, actually? In fact, we can do it in order E.
No-- yes, yes.
Reachable from S.
I guess-- let's just say a negative weight cycle at all.
Not in the context of single-source shortest paths.
AUDIENCE: Detect whether there's a negative weight edge? JASON KU: Ah.
Your colleague has determined an interesting fact about undirected graphs.
If you have a negative weight edge in an undirected graph, I can just move back and forth along that edge.",I am pretty sure most of you are witness or a victim of a negative cycle when leaarning about Negative Cycle Witness proof. What the flying fox is the use of the Negative Cycle Witness? Jason this is the part that needs to be addressed for this lecture.
"And NP is contained in or equal to EXP.
We don't know whether there's a quality here or here.
Probably not, but we can't prove it.
But what is this class? A couple of different ways to define it-- you might find one way or the other more intuitive.
They're equivalent.
So as long as you understand at least one of them, it's good.
NP is just a class of decision problems.
So I define P and EXP and R arbitrary.
They can be problems with any kind of output.
But NP only makes sense for decision problems.
And it's going to look almost like the definition of P-- problem solvable in polynomial time.
We've just restricted to decision problems.
But we're going to allow a strange kind of computer or algorithm, which I like to call a lucky algorithm.
And this is going to relate to the notion of guessing that we talked about for the last four lectures in dynamic programming.
With dynamic programming, we said, oh, there are all these different choices I could make.
What's the right choice? I don't know, so I'd like to make a guess.",Aren't quantum computers lucky to an extent? Is there any NP problem that is P for Q computers?
"But for the traveling salesman problem, the constant approximation algorithms are also NP-hard.
So instead, we modify it slightly.
So, on the traveling salesman problem, we impose something called a metric.
So the important relation here is this one.
So, first of all, let's go through this.
So you have a distance metric for-- so xy are vertices.
Your distance is always greater than 0, which is reasonable.
You're undirected, which is this relation.
And you have the triangle inequality, which means that if you have 3 vertices, and you have distance like this, this distance is always smaller than the sum of these two distances, which is like real world-ish things that make sense, right? If this distance was longer, it would just take this path instead.
So the distance by taking other node is always greater than or equal to the direct distance.
So, turns out, the Metric TSP problem is also NP-hard.
So nothing very great there, but you can do a constant approximation here.
And you'll go through two approximations today.","What is five approximation algorithm ?
what choice of path will guarantee you the shortest path always"
"Then the origin, distances, and for t in range number of trials, we'll just do it, and then we'll return the distances.
So it's initialized to the empty list.
So we're going to return a list for however many trials we do, how far the drunk ended up from the origin.
Then we can average that, and we look at the mean.
Maybe we'll look at the min or the max.
Lots of different questions we could ask about the behavior.
And now we can put it all together here.
So drunkTest will take a set of different walk lengths, a list of different walk lengths, the number of trials, and the class.
And for a number of steps and walk lengths, distances will be simWalks of number of steps, numTrials, dClass.
And then I'm going to just print some statistics.
You may or may not have seen this.
This is something that's built in to Python.
I can ask for the name of a class.
So dClass, remember, is a class, and _name_ will give me the name of the class.
Might be usual, it might be drunk, in this case.
So let's try it.
So the code we've looked at.
So let's go down here, and we'll run it, and we'll try it for walks of 10, 100, 1,000, and 10,000 steps.
And we'll do 100 trials.
Here's what we got.
So my question to you is does this look plausible? What is it telling us here? Well, it's telling us here that the length of the walk actually doesn't really affect-- the number of steps doesn't affect how far the drunk gets.","can u explain what you mean by predicts? Look at trim sort once
This is a very misleading class in my humild opinion, because he is computing the average of distances and not the expected value of the random walk. The random walk should be a bell curve with its peak at distance zero, so the expected value of the walk is always zero for the Usual Drunk. What happens is that when the number of steps increases the bell curve becomes wider and you have small probabilities of finding bigger distances, hence the 'mean' distance increases a little bit. Hower the expected distance to the origin is still zero.
but why is the expected distance to the origin zero? for a point that is 1 step away from the origin, there is 3/4 chance for the second step to be even further away from the origin. So the distance will eventually get bigger and bigger.
I did not have run the simmulation because the result is presented in the textbook. The professor is computing the average of distances and not the expected value of the random walk. The random walk should be a bell curve with its peak at distance zero, so the expected value of the walk is always zero for the EW. What happens is that when the number of steps increases the bell curve becomes wider and you have small probabilities of finding bigger distances, hence the 'mean' distance increases a little bit. Hower the expected distance to the origin is still zero. Kind of misleading IMO but it is correct.
When talking about distances the sign is not relevant. Say you have two trials with one step each and lets only allow movements in x. Asumme the first trial ends at -1 and the secon one at +1. The mean of covered distance is then one while average distance from origin is zero. Its just tow ways to look at the probem: The expected value of distance from origin is 0 while the average distnace covered is not."
"What the regularization term is doing, is it's- it's saying that x should look natural.
It's not necessarily L2 regularization, it can be something else, and we- we will discuss it in the next slide, but don't think about it right now.
What we will do is we will compute the back-propagation of this objective function all the way back to the input, and perform gradient ascent to find the image that maximizes the score of the dog.
So, it's an iterative process, takes longer than the class activation map.
And we repeat the process, forward propagate x, compute the objective, back-propagate, and update the pixels and so on.","Why do we have the ||x||^2, it says x should look natural, what do they mean with that? PS. That is one hell of a smart pet shop"
"Now it's frequency.
For every frequency you're measuring, essentially, you're viewing this vector-- the waveform-- as a bunch of trigonometric functions-- say, sine of something times theta.
If you look at one of the entries in the vector, and it's a complex number-- if you compute the magnitude of the complex number-- the length of the vector, of the length of that two-coordinate vector-- that is how much stuff-- of that frequency-- you have.
And then the angle of the vector, in 2D, is how that trigonometric function shifted in time.
So if you take a pure note, like if I was playing a bell and it's exactly C major, it looks really wavy.","So what is the math doing in practical terms? If I understand correctly, it's using the behavior of a signal over time to determine specific properties of that signal at specific moments. Is that correct?
Does anyone have intuition as to why Fourier transforms pop up here?"
"OK, so at that point we're making some progress, but we still want to work out the derivative of this.
And so what we want to do is apply the chain rule once more.
So now here is our F and in here is our new Z equals G of VC.
And so, we then sort of repeat over, so we can move the derivative inside a sum always.
So we then taking the derivative of this, and so then the derivative of exp is itself, so we're going to just have exp of the UXTVC times there's is a sum of X equals 1 to V times the derivative of UX TVC.
OK, and so then this is what we've worked out before, we can just rewrite as UX.
OK, so now we're making progress.
So if we start putting all of that together, what we have is the derivative, well the partial derivatives with VC of this log probability.
All right, we have the numerator, which was just U0 minus-- we then had the sum of the numerator, sum over X equals 1 to V of exp UXTVC times U of X, then that was multiplied by our first term that came from the 1 on X, which gives you the sum of W equals 1 to V of the exp of UWTVC.
And this, the fact that we changed the variables became important.
And so by just sort of rewriting that a little, we can get that equals U0 minus the sum V equals sorry-- X equals 1 to V of this X, V of XTVC over the sum of W equals to V of exp UWTVC times U of X.","I had a question about ""observed - expected"" around Maybe I misunderstand but isn't the summation of p(x|c)*ux our prediction therefore making it our observed?"
"So there's a normal, some still have a heavy accent, and some persist in having odd accents.
So let's see.
Accent.
Four of them, right at the top, have no accent.
Two Nos and a Yes.
Heavy accent.
Three of those.
A Yes and two Nos.
That means we must have a plus here.
3, 6, plus and a minus.
So we can look at this data and say, well, what will be the best test to use? And the best test to use would surely be the one that produces sets here, at the bottom of the branches, that correspond to the outcomes of the test.","Sorry, I still don't understand how you arrived at 4 in the shadow group. Is it because there were 4 question marks? I fully understand the other homogenous sets. Thanks."
"We've got another 1 coming in here because it's symmetrical.
So 1 and a 1, 1 times WAC is 1.
1 times WBC is 1.
So we have two 1s coming in here, they're added, that's 2.
Then this has become negative 1, in fact, at this point.
So negative 1 times negative 1, that's 3, and the output is 3.
All right.
Cool.
We've now finished part b, which is over half of everything.
Oh no, we've not.
One more thing.
These are adders.
They're not sigmoids.
What if we train this entire neural net to try to learn this data, so that it can draw a line on a graph, or draw some lines, or do some kind of learning, to separate off the minuses from all the pluses.","hi, can you explain what is being graphed in the last question with i_1 and i_2 axes?"
"So, we're guaranteed to converge to the global optima and we'll see why for a second.
Okay.
All right.
So this is how it works.
You do this policy evaluation and then you compute the Q function and then you compute the new policy that takes an arg max of the Q function.
So, that's how policy improvement works.
The next critical question is Iris was bringing up is okay why do we do this and is this a good idea.
So, when we look at this, um let's look through this stuff a little bit more.
What we're going to get is we're going to get, um this sort of interesting type of policy improvements step and it's kind of involving a few different things.
So, I just want to highlight the subtlety of it.
So, what is happening here is that we compute this Q function and then we've got this.
We've got max over A of Q^_i(s,a) has to be greater than equal to R(s, (a)).
The previous policy that we were using before.
[NOISE].
So, what I've done there is I've said, okay, the max action over the Q has to be at least as good as following your old policy by definition, because otherwise you could always pick the same policy as before or else you're gonna pick a better action.
And this reward function here is just exactly the definition of the value of your old policy.
So, that means that you're- the max over your Q function has to be at least as good as the old value you had.
So, that's encouraging.
But here's the weird part.
So, when we do this, if we instead take arg max we're gonna get our new policy.
So, what is this doing? It's saying, I'm computing this new Q function.
What does this Q function represent? It represents, if I take an action and then I follow my old policy from then onwards.","we said if policy is deterministic we can simplify value function to Vk(s) = r(s, (s)) + Xs0Sp(s0|s, (s))Vk1(s0) but how we can write max(a) Q(s,a) >= V(s) when policy is deterministic and we can choose just one action?"
"Right.
Yep.
[BACKGROUND] That's exactly right.
Uh, so, uh, so the question is, um, are we training Theta to, uh, uh, um, to predict the parameter of the, um, exponential family distribution whose mean is, um, the, uh, uh, uh, prediction that we're gonna make for y.
That's, that's correct, right.
And, um, so this is what we do at test time, right.
And during training time, how do we train this model? So in this model, the parameter that we are learning by doing gradient descent, are these parameters, right.
So you're not learning any the parameters in the, uh, in the, uh, uh, exponential family.
We're not learning Mu or Sigma square or, or Eta.
We are not learning those.
We're learning Theta that's part of the model, and not part of, uh, the distribution.
And the output of this will become the, um, the distributions parameter.
It's unfortunate that we use the word parameter for this and that, but, uh, there, there are- it's important to understand what, what is being learned during training phase and, and, and what's not.
So this parameter is the output of a function.
It's not, it's not a variable that we, that we, uh, do gradient descent on.
So during learning, what we do is maximum likelihood.
Maximize with respect to Theta of P of y i given, right.","What is h(theta) for the last example of softmax regression?
what is h(theta) for the last example of softmax regression?"
"So let's say it's a fairly small world, so we just have S1 and S2.
So in this current state, I could either take action one or action two and after I take those actions I could either transition to state one or state two.
And then after I get to whatever I state- I get state, I get to, I again can make a decision A1 or A2.
Because that's my action space here.
And then after I take that action, I again my transition or maybe sometimes I terminate it.
So this is a terminal state.
Maybe my robot falls off or falls down or things like that, or maybe else I go to another state.","A question: will you alternate between tree and rollout policy in a single playout? Maybe like this s1 / \ a1 a2 / s2 / a2 / s3 / \ a1 a2 In the first and last level, you have enough experience, you can do the tree policy with UCT, with the s2 state, you do not have enough experience, you do rollout policy? But the video looks like starting rollout policy from s2 until terminal. I watched a lot of videos and slides, none of them clearly talk about this. No one clearly talk about how the tree is expanded and when to use tree and rollout policy."
"So-so the question was, uh, wouldn't they all learn the same, same, uh, same model? Aren't they gonna be just copies of the same model? Um, in- potentially it can and we'll- we'll address that shortly.
Was there another question? [inaudible] Yeah, we'll come to that, we'll come to that.
For now we are just constructing a network, uh, using base components that we've seen before.
Basically logistic regression, right? So, take logistic regression, create multiple copies of it.
Take the outputs of those logistic regressions.
Feed them as input to, you know, a nested logistic regression and so on.
And this kind of a network can be arbitrarily deep and arbitrarily wide, right? And it can be arbitrarily wide to different levels at different layers.","Hello, In this lecture, you mentioned that a Neural network for classification with a sigmoid activation can be considered as some combination of logistic regressions (one for each neuron starting from the 1st hidden layer). Knowing that logistic regression's loss function is convex and NN's are not convex, can we minimize each logistic regression individually and use the optimal parameters obtained from each model to reach a global minimum for the Neural Network?"
"And, if you ignore this minus sign, that's exactly what this is doing.
This is taking aj versus bk, pretend it's plus j.
So, that's the bj vector, but with all possible shifts k.
We compute this for all k.
That's really cool.
We're going to compute it in n log n time.
All different n shifts of b will take the dot product with a.
It's kind of magical because it looks like you're doing n squared work, but we will do it in n log n time.
The only issue is we have to reverse b.
Then the minus signs turn into plus signs.
And there's some boundary conditions, but it's basically the same.","should not that be O(n^3) as we have total work = n + 2n + 4n + ...+ n*n = n ( 1 + 2 ...+n) = n*n*(n+1)/2 = O(n^3), he says it is O(n^2)"
"JASON KU: Yeah, so we have some edge from u to v.
It has some weight.
If I already know the shortest path distance to u, and I know the shortest path distance to v, if the shortest path distance from s to u-- let's draw a picture here.
We've got s, we've got some path here to u, and we know we've got an edge from u to v.
If this shortest path distance plus this edge weight is equal to the shortest path distance from s to v, then it better-- I mean, there may be more than one shortest path, but this is certainly a shortest path, so we can assign a parent pointer back to u.","but I think d(s,v)>delta(s,u)+w(u,v) can be correct if vertex u comes before v in topological order.because d(s,u) is the same as delta(s,u) as there is no subroute that can reach vertex u from vertex s going through any vertex that come after u in the topological order. am I right?"
"I'm quickly going to introduce um, Eigenvector Centrality um, which we are going to further uh, work on and extend to the uh, seminal PageRank algorithm uh, later uh.
In the- in the course, I'm going to talk about between the centrality that will tell us how- uh, how important connector a given node is, as well as closeness centrality that we'll try to capture how close to the center of the network a given node is.
Um, and of course, there are many other, um, measures of, uh, centrality or importance.
So first, let's define what is an eigenvector centrality.","Thanks for this course, I'm getting interested to GNN and your courses help a lot. One question, how to compute GDV efficiently?"
"Otherwise, set the background to the exit background, and so on.
So you notice that there's sort of no end to this, right? How many times-- do you know how many times the user might keep going right? They might be really persistent, right? And they'll be like maybe if I go 1,000 times, I'll get out of the woods.
Maybe 1,001? Maybe.
So this would probably be-- who knows how deep? These nested ifs.
So we don't know.
So with what we know so far, we can't really code this cute little game.
But enter loops.
And specifically, a while loop.
So this code here that could be infinitely number of nested if statements deep can be rewritten using these three lines.
So we say while the user exits right, set the background to the woods background.
And with a while loop, it's going to do what we tell it to do inside the loop, and then it's going to check the condition again, and then it's going to do what we say it should do inside the code block, and it's going to check the condition again.
And then when the condition-- as long as a condition is true, it's going to keep doing that little loop there.
And as soon as the condition becomes false, it's going to stop doing the loop and do whatever's right after the while.","for the maze game, "" n=input(""you are in a deep forest.\n****************\n <-> \n****************\nGo left or right?"")""under the while loop doesn't need any print command and it just repeats itself?
The 'if' nested in an 'if' would only be considered if the parent 'if' is true, the 'else' nested in that won't ever be considered, then why use it at all? Also if all the nested 'if's have same condition during nesting then if they are true, it'll execute them all at once, so what is the use of that? What we want is that it asks us the question and show us the forest screen until we enter a 'left', so what we want it to do is to repeat this 'if' condition again and again so that it asks us that question again and again X=input ("" left/right..?"") If x== ""right"": <Show forest screen> Else: <Show exit screen> How can we do that? I didn't understand the need for while here, because for it to ask us the same question again an again we will have to run this code again and again, and that can be done with an 'if-else' condition as well.
We used a while loop because we don't know how many times the user will enter right. x = input ("" left/right..?"") If x== ""right"": <Show forest screen> Else: <Show exit screen> will take the input only once. What if I want to go right twice? Let's say, you use 7 nested if statements, the program would ask the user for direction 7 times. Not more, not less. What if I want to go right 10 times? I wouldn't be able to. Notice the condition of her while loop. It doesn't matter whether I type ""right"" once or 10 or 1000 times. The output is dependent on my answer, not on the number of times I type that answer. I hope you understand."
"You're adding up a column vector with a v.
That's the column vector the way I drew that.
You could do it with rows if you like, but it's just notation.
And you're going to compute an r prime here.
What can you say about Dr prime? Someone? Yeah.
Go ahead.
AUDIENCE: It's not 0.
SRINIVAS DEVADAS: It's not 0.
And I'll give you a Frisbee, but then you can explain-- can you stand up a little? I don't want to take this lady's head off.
So can you explain why? AUDIENCE: Because r prime is r plus v and Dr gives you Dr plus DB is not 0.
SRINIVAS DEVADAS: Absolutely right.
So essentially what we have is this is simply Dr plus v.
0 plus Dv, not equal to 0.","What does the T mean?
Wasn't the professor correct in making the jth value 1 instead of the ith value as pointed out by one of the students?"
"The first is that SimCLR sample is only one classification task per mini batch, and usually meta learning algorithms will sample a mini batch of tasks.
And the second difference is that SimCLR is really a look at all pairs of negative examples, or all pairs of examples in your batch.
Whereas meta-learning only compares the query examples to the support examples.
And never compares and contrasts, query examples with other query examples.
So in this way, you could perhaps view the SimCLR class as being a little bit-- the SimCLR loss as being a little bit more efficient with its batch because it's going to compare and contrast everything in the batch.
But otherwise, these algorithms end up being extremely similar.
And they also end up doing something extremely similar in practice as well.
So if you-- here's just one experiment in that paper.
You take an unlabeled data set.
I think they took the ImageNet unlabeled data set.
They augmented with the SimCLR augmentations, and compared using SimCLR versus using this approach with prototypical networks and R2-D2.
Where R2-D2 is an optimization-based meta-learner that has a number of kind of bells and whistles to it.
And you see that when you then kind of pre-train these representations on ImageNet, and then fine tune them on these other image classification problems, you see extremely similar performance between SimCLR and prototypical networks.
You also see very similar performance between SimCLR and R2-D2.
And in some cases, R2-D2 is able to do a little bit better as well.","Also, doesnt SimCLR use positive AND negative examples in denominator? + it uses temperature parameter in num/denominator tau. Why Stanford prof ignore this??"
"ROFESSOR: So, now we come to the place where arithmetic, modulo n or remainder arithmetic, starts to be a little bit different and that involves taking inverses and cancelling.
Let's look at that.
So first of all, we've already observed that we have these basic congruence rules that if a and b are congruent then c and d are congregant, then a plus c and b plus d are congruent, a times c and b times d are congruent.
So, that's the sense in which arithmetic mod n is a lot like ordinary arithmetic.
But here's the main difference.
Let's look at this one.
8 times 2 is 16, which means it's congruent to 6 mod 10, which is the same as 3 times 2.",in step a.1 equivalent to b.1 how we omit (mod n). I would guess it to be a.1 (mod n) equivalent to b.1 (mod n) (mod n). please clarify.
"[NOISE] So, I got a couple of questions about same point, um, about this, this G, so when we do that, it seems like we'll and, uh, sam- sampling G's that have reward- rewards over episodes of different lengths, [NOISE] but, so doesn't that close their distribution without stationary and more variance? This question [inaudible] there's a problem with the fact that, um, the returns you're taking are gonna be sums over different lengths.",Why is the teacher asking for the name for each question ?
"I get that node.
I take the method associated with it.
And I call it.
That returns the string.
And then I glue that together with the arrow.
I do the same thing on the destination.
And I just print it out.
Pretty straightforward, hopefully.
OK, now I have to make a decision about the graph.
I'm going to start with digraphs, directed graphs.
And I need to think about how I might represent the graph.
I can create nodes.
I can create edges, but I've got to bring them all together.
So I'll remind you, a digraph is a directed graph.
The edges pass in only one direction.
And here's one way I could do it.
Given all the sources and all the destinations, I could just create a big matrix called an adjacency matrix.",Theres one issue with the code that is given. Nowhere in the lecture notes or in the video defines the printPath() function. Also how does he print out in that format when the only way to do it is by calling on the Edge class method to print? especially when he is appending nodes and not edges. I am guessing it is done in the printPath() function
"So that's the kind of idea we're going to explore.
If we want to learn things, we could also ask, so how do you learn? And how should a computer learn? Well, for you as a human, there are a couple of possibilities.
This is the boring one.
This is the old style way of doing it, right? Memorize facts.
Memorize as many facts as you can and hope that we ask you on the final exam instances of those facts, as opposed to some other facts you haven't memorized.
This is, if you think way back to the first lecture, an example of declarative knowledge, statements of truth.
Memorize as many as you can.
Have Wikipedia in your back pocket.
Better way to learn is to be able to infer, to deduce new information from old.
And if you think about this, this gets closer to what we called imperative knowledge-- ways to deduce new things.
Now, in the first cases, we built that in when we wrote that program to do square roots.
But what we'd like in a learning algorithm is to have much more like that generalization idea.
We're interested in extending our capabilities to write programs that can infer useful information from implicit patterns in the data.","What if the machine isn't actually learning, but carrying out many steps by rote very quickly?
The professor mentioned ""Ch 22"" in a book. What book is the class using?"
"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
[MUSIC PLAYING] PATRICK H.
WINSTON: Well, what we're going to do today is climb a pretty big mountain because we're going to go from a neural net with two parameters to discussing the kind of neural nets in which people end up dealing with 60 million parameters.
So it's going to be a pretty big jump.
Along the way are a couple things I wanted to underscore from our previous discussion.
Last time, I tried to develop some intuition for the kinds of formulas that you use to actually do the calculations in a small neural net about how the weights are going to change.
And the main thing I tried to emphasize is that when you have a neural net like this one, everything is sort of divided in each column.
You can't have the performance based on this output affect some weight change back here without going through this finite number of output variables, the y1s.
And by the way, there's no y2 and y4-- there's no y2 and y3.
Dealing with this is really a notational nightmare, and I spent a lot of time yesterday trying to clean it up a little bit.",How did the professor trained the Neural Net...any idea??
"i, everyone.
My name is Yoonho Lee.
I'm a TA for this course.
And it's my first time giving a lecture.
So hopefully everything goes well.
Yeah, we're going to be talking about the second edition of advanced meta learning topics.
I think on Monday, Chelsea talked about memoization and task construction in meta learning.
And today, we're going to talk about large scale meta optimization.
As a quick reminder, homework three, the one on language models is out this Monday and it's due in a week.
So we're going to start with kind of a big picture question about meta learning and why we should even do it.
So we can think about learning methods as being on the spectrum between hand designed priors and data-driven priors.
And there's kind of a continual shift downwards towards more data-driven stuff.
So for example, a long time ago, people used to do directly modeling image formation.","Hi Yoon-ho, I have a question. If I understand correctly, we need these approaches to overcome memory burden as well as any kind of burden coming from the large scale. Is there any reason we restrict these techniques to large-scale meta learning? I mean, arent those approaches able to be applied to any sorts of large-scale single-task machine learning?"
"Or, let's say choose a symbol from my-- I have this alphabet that I'm using in my cell-- choose a symbol non-deterministically.
So I have order n choices made non-deterministically.
In the usual sense of NP, that if there's any way for the Turing machine to output, yes-- one of the instructions is output, yes-- then, I will find it.
These are guesses, and they're always lucky guesses, so I always end up finding the return, yes, if there is such a path.
Otherwise, all paths return, no, and then the machine returns, no.
Yeah? AUDIENCE: What does it mean for the number of states to change as the input changes? PROFESSOR: I mean the states are also given to you as part of the Turing machine.",So can our turing machines only move one cell at a time? Is that why you only need to account for k (seems like it should be 2k) memory cells?
"hat we're going to talk about today, is goals.
So just by way of a little warm up exercise, I'd like you to look at that integration problem over there.
The one that's disappeared.
So the question is, can you do it in your head? Probably not.
The question is, if a program can do that, is a program, in any sense of the word, intelligent? That's a background task I'd like you to work on as I talk today.
So today we're going to be modeling a little bit of human problem solving, the kind that is required when you do symbolic integration.
Now, you all learned how to do that.
You may not be able to do that particular problem anymore, but you all learned how to integrate in high school 1801, or something like that.
The question is, how did you do it, and is the problem solving technique that we are trying to model by building a program that does symbolic integration, is that a common kind of description of what people do when they solve problems.
So the answer to the question is, yes.
The kind of problem solving you'll see today is like generating tests, which you saw last time.
It's a very common kind of problem solving that we all engage in, that we all engage in without thinking about it, and without having a name for it.",I studied computer science but I never was a math guy. Can someone please explain me why this lectures of him matters for understanding ai?
"That make sense? And you're basically applying Hoeffding's inequality to this gap over here instead of some phi.
That's basically what you're doing.
Right? Now, that's good.
But there's a problem here.
The problem here is that, we started with some hypotheses, and then averaged across all possible data that you could sample.
But in practice, this is useless.
Because in practice we start with some data, and run the empirical risk minimizer to find the lowest H for that particular data.
Right? And when you, when, when- which means that H, and the data that you have are not really independent.
Right? You, you chose the H to minimize, ah, minimize the risk for the empirical risk for th- the particular data that you are given in the first place.
Right? So to, to fix this, what we wanna do is basically extend this result that we got to account for all H.",Why do we calculate hoeffding's equality in application?
"And optimal margin classifier plus kernels meaning basically take this idea of pi in a 100 billion dimensional feature space that's a support vector machine, okay? So I saw- one thing I didn't have time to talk about, uh, on Wednesday was the derivation of this classification problem, so  where does this optimization objective come from? So let me- let me just go over that very briefly.
Um, so, the way I motivated these definitions we said that given a training set, you want to find the decision boundary parameterized by w and b, um, that maximizes the geometric margin, right? And so again, as recap, your classifier will output g equals w transpose x plus b.
Um, and so you want to find premises w and b.
They'll define the decision boundary where your classifications switch from positive to negative, that maximizes the geometric module.
And so one way to pose this as an optimization problem is- um, let's see, is to try to find the biggest possible value of Gamma subject to that- subject to that the, um, geometric margin must be greater than or equal to Gamma, right? So, um, so, in this optimization problem, the parameters you get to fiddle with are, Gamma, w and b.
And if you solve this optimization problem, then you are finding the values of w and b that defines a straight line, that defines a decision boundary, um, so that- so, so this constraint says that every example, right? So this constraint says every example has geometric margin greater than or equal to Gamma.","Question for the kind hearted people who understood: in the geometric margin, y is either 1 or -1 right? But wasnt it mentioned before that y is either 1 or 0? I got a little messed up on that."
"That's one possible thing that can happen.
If my, if my, um, policy is to let's say stay, like there is no reason for for the game to end right here.
Right? Like I can have a lot of different types of random path.
I can have a situation where I'm staying three times and then after that ending the game and utility of that is 12.
We can have this situation where we have stay, stay, and end.
That's the situation it's all, like you had, you had an utility of eight and so on.
So, so you're getting all these utilities for all these random paths.
So, so these utilities are also going to be just random variables.
Okay? So I can't really play around with the utility.
That's not telling me anything.
Although it's telling me something but it's a random variable.
I can't optimize that.
So instead we need to define something that you can actually play around with it and, and that is this idea of a value which is just an expected utility.","Can in the Dice Game If choose to stay for the step 1 and then quit in the second stage: will I get 10 dollars if I choose to quit in the stage 2? Because If I am lucky enough to go to second stage i.e the dice doesn't roll 1,2 then I am in the ""In"" state and by the diagram I have option to quit which might give me 10 dollar but for that I should have success in stage 1. Then the best strategy might change. Let know what are your comments?"
"So if I take these intervals and I sort them, and basically sort them by the start times, then all I need to do is, in the sorted list of intervals, I just showed you the computation that we have to perform.
The computation that we have to perform is-- this was the first interval in the sorted list of intervals, because it was the leftmost interval.
And I take that and I go ahead and increment it to one, right? Then, the next thing I do is I don't have to worry about any time.
Even if there was a time, if this was 6:00 and this was 8:00, for example, as long as there's no celebrity that came in at 7:00, there's no reason for me to change anything in my data structure, right? The only times that are interesting are when celebrities enter and leave.
So the other nice thing about this algorithm is that it will work if Beyonce came in at 6:39:39, 39 seconds-- 39 minutes, 39 seconds and left at 6:51 whatever, because I'm only concerned with the entry of a celebrity and the exit of a celebrity that correspond to these points that you see here, right? Does the algorithm make sense from an intuitive standpoint? Do you see what's happening here? Hopefully the picture gives you a sense for how you're doing the correct computation.","Do we need to sort? Can we do this in O(N) where N will be total celebrity. After finding the start point and end point. We can iterate the schedule and update as for every entry as +1 and exit as - 1. That is if a celebrity enters at 6 and exit at 7. We update arr[6] +=1 and arr[7] - =1 In the send just keep on iterate this array and adding the values, maximum sum at any point will be the answer
Will it be more efficient if we find the Mode of the range of time. Say celebrity A comes in at 7 and leaves at 10. So we expand it as 7,8,9,10 , but we will not consider the last element of the range (10 here) since they have already left. Then we find the number whose frequency is maximum across all celebrities (Mode), which will be the best time to party."
"Well, one of the things we need to learn to do is whenever we build a simulation, we need to do what I call a sanity check to see whether or not the simulation actually makes sense.
So if we're going to do a sanity check, what might we do in this case? We should try it on cases where we think we know the answer.
So we say, let's take a really simple case where we're pretty sure we know what the answer is.
Let's run our simulation and make sure it gives us the right answer for this simple case.
So if we think of a sanity check here, maybe we should look at these numbers.
We just did it.
We know how far the drunk should get in zero steps.",think why logically it would work instead of computationally first. Why we can apply to a general case?
"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation, or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
ANA BELL: Let's look at this exercise.
So we have this function is even, same one as before, except now I'm giving you this implementation.",the path isn't complete cuz of the 0 cases haven't been covered since 0 is even and the method returns false for it!
"Or determine that they're unsolvable.
So that's another easy case for SAT to be careful about.
And the theorem is if you have one of these situations-- so you can't mix these.
If you have one clause of this type and another clause of this type, your problem will be NP-hard.
So in general, you say, otherwise SAT is NP-hard.
I guess it will actually be NP-complete here, the way we've set it up.
Well, assuming the relations are checkable.
So these are the only cases.
This is an easy case, this is an easy case, this is an easy case.
It could be that multiple of these things are true.
Maybe your 2CNF and your all Horn.
That will also be polynomial, of course.",You said the problem is about showing a satisfiability or unsatisfiability result when it's actually about coming up with all the vertexes that satisfy the CNF.
"So, if we can prove that every-- prove every negative weight cycle contains witness.
If we can prove that every negative weight cycle contains a witness, then every vertex reachable from one of those witnesses-- in particular, reachable from the negative weight cycle-- has shortest distance minus infinity, and that should prove the claim.
This thing has to be reachable from a negative weight cycle.
And so if we prove negative weight cycles contain witnesses, then all of these vertices are reachable from a witness.
OK, great, great.
Confusing myself there for a second.
OK.
So let's consider a negative weight cycle.
NG.
Here's a directed negative weight cycle.
Recall.
This will be my negative weight cycle C.
All of the sum of the edges in this thing, the weights has negative weight.
And I'm going to have a little bit notation-- if I have a vertex V here, I'm going to say that its predecessor in the cycle, I'm just going to call it V prime.
That's just some notation.
All right.
So, if I have computed these shortest-path distances to every vertex in my graph, shortest-path distance going through at most V vertices and the shortest path distance going through at most V minus 1 vertices, then I know the following thing holds.
Delta V going from S to V for any vertex in my cycle can't be bigger than delta V minus 1 from S to U plus the weight-- sorry, not U-- V prime, its predecessor, plus the weight going from the predecessor to my vertex.","Why can there be vertices with - shortest-path weight that are not witnesses? An example will be really helpful.
If you have a negative sub circle of 3 elements, while 1e6 elements in total, it may take only 1e4 steps to update to the stable state except for the negative circle. And then you have to update everything in the data like 3e5 times only to get the some node witness the max length of path to be greater than total number of nodes. Also I'm confused with the witness. Does it compare the total steps and sum of weight at the same time and tries to figure out something?
I am pretty sure most of you are witness or a victim of a negative cycle when leaarning about Negative Cycle Witness proof. What the flying fox is the use of the Negative Cycle Witness? Jason this is the part that needs to be addressed for this lecture."
"So that's it for key, query, value attention, that's how we implement it with tensors.
Next we'll look at the next thing that ends up being quite important for training transformers in practice, which is multi-headed attention.
So transformer encoder, multi-headed attention.
So the question is, what if we want to look at multiple places in the sentence at once? It's possible to do that with normal self attention.
But think about this, where do you end up looking in self attention? You end up looking where the dot products of Xi, your Q matrix transpose your K matrix, XJ is high.
So those are the Xij pairs, I'm sorry, those are the ij pairs that end up interacting with each other.
But maybe for some query, for some word that you want to focus on different other words in the sentence for different reasons.
The way that you can encode this, is by having multiple query, key and value matrices, which all encode different things about the Xi, they all learn different transformations.
So instead of a single Q, a single K and a single V, what we get are a Q sub L, K sub L, V sub L, all of a different n dimensionality now.","Good lecture, but I think there is some confusing usage of the K, Q, and V matrices. You introduce them by saying: (1) K*x_i = k_i, Q*x_i = q_i, V*x_i = v_i In this way you're saying that X, Q, and V operate on x_i column vectors from the left to yield another column vector. But then later, when you stack the transposes of the x_i to produce the X matrix for the tensor attention calculation, you multiple from the right with K: (2) softmax(XK(XQ)^T) = Attention weights I think both K and Q should be transposed in (2) as you're using them in order to be consistent with the way you define the matrix in (1).
Are matrices K, Q, V parameters to be learned?
Great lecture, very clear! Thanks for making it freely available! In multihead self-attention, each head is going to produce values vectors of dimension d/h whereas the single head self-attention produces value vectors of dimensions d. That allows the multihead self-attention to look at multiple places at the same time however reducing the dimension is going to discard some information. Could it be possible that single head self-attention works better in some cases than multihead self-attention because it doesn't reduce the dimensionality?
Agree. It seems that this lecture cannot even tell the reason of why we should use K, Q, V clearly..."
"I'll set up a flag that's initially false.
And then I'm going to loop over everything in the second list.
And if that thing is equal to the thing I'm looking for, I'll set match to true and break out of the loop-- the inner loop.
If I get all the way through the second list and I haven't found the thing I'm looking for, when I break out or come out of this loop, matched in that case, will still be false and all return false.
But if up here, I found something that matched, match would be true.
I break out of it.
It's not false.
Therefore, a return true.
I want you look at the code.
You should be able to look at this and realize what it's doing.
For each element in the first list, I walk through the second list to say is that element there.
And if it is, I return true.
If that's true for all of the elements in the first list, I return true overall.
OK.
Order of growth.
Outer loop-- this loop I'm going to execute the length of L1 times.
Right? I've got to walk down that first list.","Oh he's also counting the check again for the if condition? I don't get the second nested loop
 My understand is, the worst case scenario only cares about the term with the highest degree of polynomial. For example, if it takes 3n + n + 1000000, then the worst case scenario is still O(n) because you don't care about the constant (1000000), the terms with lower degree of polynomial (n) and the coefficient of the highest degree of polynomial (3). Let's say we have 2 lists, L1 and L2 with the same numbers in them. [1, 2, 3, 4, 5, 6,...,n], and we try to compare them. Let's say in the first loop we do one operation, and in the nested loop we do 3 operations. The first time you compare 1 in L1 to 1 in L2, it will do 3 operation since you do 3 in the nested loop. Since the comparation is True, you continue on with the first loop. The second time you compare 2 in L1 and 1 in L2, you also do 3 in the nested loop. But since the comparation is False, you continue on with the nested loop. Now you compare the number 2 in L1 and 2 in L2, and you also do 3. In total, you do 3 + 3 = 6 operations in the second loops. The 3rd loop you will do 3 + 3 + 3, and so on. And so in total, you do 3 + 6 + 9 + ... + 3n = 3n(n+1), which is still O(n) because we don't care about the number 3 and the term n behind.
There are two loops in second loop. First for each 'e' in tmp[] and then for that each 'e' it is checked if 'e' is already in res[]. So for two loops the second one is also quadratic.
top coder, I know that there is two loops in the second half of the code, in which ""e in res"" is an implicit loop. But when the teacher says ""I'm only looping over tmp, which is at most going to be len(L1) long"", that is false. To prove my point, imagine L1 and L2 to be filled with all 1's. (L1 = [1,1,1,1,1], L2 = [1,1,1,1,1]). In that case, the line if e1 == e2 will always be true and execute with every iteration of its inner loop, thus making tmp the size of len(L1)*len(L2) (and if len(L1)==len(L2) then tmp will be len(L1)^2). I agree though that in such case O(n^2) is still applicable to the lower code half aswell as all of the code."
"So in the spirit of NP-hardcoreness, we're going to take the classic of hardcore video games, Super Mario Brothers.
And our first proof will be that Mario Brothers is NP-hard.
This got lots of press, like here's Kotaku saying, ""Science proves old video games were super hard."" AUDIENCE: [LAUGHING].
PROFESSOR: This is proving the obvious, I guess.
But anyway.
So let me tell you a little bit about how this proof goes, and then we will see it.
All right.
So we need a problem to reduce from.
The problem is 3SAT.
This is probably the most common problem to reduce from.
We will spend a bunch of lectures on it.
It won't be our very next class, but we will get to it pretty soon.","Nice video and good explanations! However, I believe reductions should lead to the result ""at least as hard as"" instead of ""as hard as""."
"This loop counts, too, but it doesn't include V.
All this loop tells me is that the voltage drop across this element is equivalent to the voltage drop across this element.
Or, the voltage drop in this direction across that element is equal to the voltage drop in this direction across this element.
That's Kirchhoff's voltage law.
Kirchhoff's current law is that the current flow into a particular node is equal to 0.
Or, if you take all of the current flows in and out of a particular node and sum them, they should sum to 0.","if the output of voltage of RL circuit is taken to be the voltage across the inductor, the circuit is said to be what
How would i2 relate to it being correlated to R3 being over the sum and for i3 visa versa? Why can you not use i2 with R2 over the sum and so on?
What would you use, Maxwell equation within time or frequency domain?"
"If you can do that, then you prove that your problem is as hard as the original problem.
If you know that one is hard, than this one is hard.
Don't get this backwards.
You will anyway, but try not to get it backwards.
You're always reducing from the known hard problem to your problem.
OK.
So usually say, our proof is based on a reduction from-- pick your favorite problem.
Never two.
Easy to get wrong, because it's easy to make a sign error.
But that's life.
So good.
Anything else? Now if you've taken an algorithms class, you've seen lots of reductions.","Nice video and good explanations! However, I believe reductions should lead to the result ""at least as hard as"" instead of ""as hard as""."
"SRINIVAS DEVADAS: d, b is going out.
That's exactly right, d, b.
And the plus 2, it would be d, T, right? And so you have to do the enumeration.
It's worthwhile doing once.
And then it gets kind of boring.
We won't to do it again.
But you have to realize that you have to absolutely look at every pair of vertices.
And you have to use skew symmetry and ensure that, even though there's actually no edge going out, if there's an edge coming in, you've got to count that.
And that's going to get a negative.
Whatever is coming in, you've got to subtract, OK? So it's not that complicated.
Yeah, go ahead.
STUDENT: Do we not consider S, c? SRINIVAS DEVADAS: I'm sorry? STUDENT: Do we not consider S, c? SRINIVAS DEVADAS: So the beauty of this is that, when you don't have a particular edge from S to c, you can use skew symmetry to argue that S, c and c, S cancel out each other, all right? So that's the good part, right? And thanks for asking the question.
That's a good question.
All right.
Here you go.
So you can do that by just looking at the edges.
And you can add up the numbers, all right? And so I don't think this is going to be absolutely crucial to understand the rest of the lecture.
Keep this in mind, that there's a process by which you define the value of a cut.
And we're going to get back to this, when we prove the max-flow min-cut theorem next time.","How does f(u, v) work in skew symmetry when (u,v) are not connected by an edge?
What did that student mean at https://youtu.be/VYZGlgzr_As?t=3377?? We did consider c at f(d,c) which is -1 . Not sure what did he mean with his question."
"Right, or-or and then sometimes we train, our algorithms with thousands or tens of thousands of iterations.
And so- so this- this gets expensive.
So there's an alternative to batch gradient descent.
Um, and let me just write out the algorithm here that we can talk about it, which is going to repeatedly do this.
[NOISE] Oops, okay.
Um, so this algorithm, which is called stochastic gradient descent.
[NOISE] Um, instead of scanning through all million examples before you update the parameters theta even a little bit, in stochastic gradient descent, instead, in the inner loop of the algorithm, you loop through j equals 1 through m of taking a gradient descent step using, the derivative of just one single example of just that, uh, one example, ah, oh, excuse me it's through i, right.",I have been wondering why we need such an algorithm when we could just derive the least squares estimators. Have you seen any research comparing the gradient descent method of selection of parameters with the typical method of deriving the least squares estimators of the coefficient parameters?
"High of x is keeping track of which clusters are non-empty.
We've just inserted something into this cluster.
So it's non-empty.
We better mark that that cluster, high of x, is non-empty in the summary structure.
Why? So we can do successor.
So let's move on to successor.
Actually, I want to mimic the successor written here on the bottom of the board.
So what we had in the non-recursive version was three steps.
So we're going to do the same thing here.
We're going to look within x's cluster.
We now know that is the cluster known as high of x.
And either we find, and we're happy, or we don't.",Isnt the successor function wrong? It doesnt have a base case. It should never terminate.
"What do you do when you see that? AUDIENCE: [INAUDIBLE] What's that Rhana? Rhana: 1 + 6  * 1 - 6 Well wait a second.
We could do that.
But there's another thing we can do.
Christian, have you got something you can suggest? Where's our Hungarian? Our Turk, our young Turk.
Yeah, what do you think? AUDIENCE: I actually don't remember.
I mean, I think it might have been 10.
SPEAKER 1: Well, let's see.
Cosine squared plus sine squared equals 1.
So, what's that suggest to you? So it suggests that we make a transformation that involves x equals sine y.","There is one question, that when they apply x=sin(y), there is a subtle constraint of the range of x applied, which means abs(x) cannot be bigger than 1. But the original formula does not have the constraint of x value range. Are we losing something here?!
I got lost when he apply C and got to sin^4y/cos^4y. can someone explain it in detail for me? thanks...
Why was tan4 chosen instead of cot4? I couldn't understand it.
Out of curiosity: What level of mathematics should I have before watching these videos. I have never taken a calculus course, so I don't know why those transformations work. Do I need calculus, or can I get by without it? Or can I learn a certain calculus subjects without having to go into calculus in depth?
+James N How does the numerator become sin^4 y cos y, with direct substitution it shld be sin^4 y ?! Help !!"
"And so if some of them are very, very large, you sort of just zero out the connections to everything that's not being attended to, that has low probability distribution and then they don't get gradients.
And so here's the self attention operation we've seen, OK.
I've taken, this is the multi-headed variant here, right, because we've got the other indices on output, I've got the indices on Q, K and V.
And all I'm going to do is I'm going to say, well, the things that I'm about to dot together are vectors of dimensionality d over h, because of the multi-headed attention again.","Good lecture, but I think there is some confusing usage of the K, Q, and V matrices. You introduce them by saying: (1) K*x_i = k_i, Q*x_i = q_i, V*x_i = v_i In this way you're saying that X, Q, and V operate on x_i column vectors from the left to yield another column vector. But then later, when you stack the transposes of the x_i to produce the X matrix for the tensor attention calculation, you multiple from the right with K: (2) softmax(XK(XQ)^T) = Attention weights I think both K and Q should be transposed in (2) as you're using them in order to be consistent with the way you define the matrix in (1).
Are matrices K, Q, V parameters to be learned?
Great lecture, very clear! Thanks for making it freely available! In multihead self-attention, each head is going to produce values vectors of dimension d/h whereas the single head self-attention produces value vectors of dimensions d. That allows the multihead self-attention to look at multiple places at the same time however reducing the dimension is going to discard some information. Could it be possible that single head self-attention works better in some cases than multihead self-attention because it doesn't reduce the dimensionality?"
"All right so that's the difference.
And the last thing I want to mention about this is_even function is how useful it can be.
So notice this is the function as in the slides, and once you write the function once, you can use it many, many times in your code.
So here I'm using the function is_even to print the numbers between 0 and 19, including and whether the number is even or odd.
So notice this piece of code here, once I've written this function is_even, looks really, really nice right? I have for all the numbers in this range if the number i is even, this is going to return a true or false for all the numbers 0, 1, 2, 3, 4.",why did it print with return and without return twice?
"With insertion and deletion in a 2-3 tree, it's actually not true, but for insertion only this is true.
So let's prove it.
A 2-3 tree, we have two types of nodes, 2 nodes and 3 nodes.
I'm counting the number of children, not the number of keys, is one smaller than the number of children.
Sorry, no vertical line there.
This is just sum key x, sum key and y.
So when I insert a key into a node, it momentarily becomes a 4 node, you might say, with has three keys, x, y, and z.
So 4 node, it has four children, hence the 4, and we split it into x and z.
There's the four children, same number, but now they're distributed between x and z.",It's possible the log(n) is referring to the rebalancing of the tree(which wouldn't happen if the element didn't exist) and not the searching for the element. Otherwise you're correct.
"But if you did that, again, you lose track of how much water is in the jug.
So these are the only safe moves.
And they're the only ones we're going to model.
All right.
So let's go back to Simon's challenge.
He wants to disarm the bomb by getting exactly four gallons of water in the jug and measure it on the scale, or things will blow up.
And how do you do it? Well, why don't you take a moment to think about it before I proceed to the next set of slides or before you let me proceed.
But just to understand the rules again, watch the work here's how.
We're going to start off with both jugs empty.","Because it requires more transitions? Yours requires 9, while the one explained in the video only requires 7. Yours B,L 1. 0,0 2. 0,3 3. 3,0 4. 3,3 5. 5,1 6. 0,1 7. 1,0 8. 1,3 9. 0,4 Lecture's B,L 1. 0,0 2. 5,0 3. 2,3 4. 2,0 5. 0,2 6. 5,2 7. 4,3"
"And then I want it to just alert XSS and then script.
So now that's interesting.
So it seems like something didn't quite work.
So I don't see any output.
I didn't see the alert either.
And if I actually look at the output for the web server-- and what I see is that here, the web server itself didn't actually get that trailing script tag.
So it seems like the browser itself has somehow detected something evil even though I tried to disable the XSS filter.
So that's interesting.
We're going to come to this defense mechanism a bit in the lecture.
But suffice it to say, it seems like the browser is trying to resist this cross-site scripting attack.",I did not catch how putting the sensitive info in the DOM keeps JavaScript eval() from accessing it.
"Um, what is interesting in graphs, is that, uh, sometimes we cannot guarantee that the test set will really be held out, meaning that there will be no information leakage from training and validation sets into the test set.
And this is why, uh, this becomes interesting.
So that's that's in terms of a fixed split.
Once we have created the fixed split, we could actually make it such that it is a random.
And what this would mean, is that we could randomly split our data into training, validation, and test set, and then we could kind of report- rather than on a single split, we could report average performance over different, uh, uh, let's call them, uh, random splits.",Thank you for this presentation! Very interesting. The question: When we do the lint prediction - the number of nodes should be constant in all dataset graphs? So they can not be like parts of each oter
"So a, d has a flow of 2, correct? And so d, a has a flow of minus 2, right? And d, a is part of what I have here, because d is part of capital S, and A is part of capital T.
You guys see that? So this is not trivial, so pay attention.
So this would be, for example, the minus 2 would correspond to d, a.
That's what I need here.
And I could also have-- what do I have here? I have something is going into d.
So a c, d is 1.
So d, c is minus 1, right? Make sense? d, c is minus 1.
What about the plus 1? Where do I get a plus 1 from? STUDENT: d, b.","What did that student mean at https://youtu.be/VYZGlgzr_As?t=3377?? We did consider c at f(d,c) which is -1 . Not sure what did he mean with his question."
"OK? But you'll notice that my CPU is only built to operate on a constant amount of information at once-- generally, two words in memory.
An operation produces a third one, and I spit it out.
It takes a constant amount of time to operate on a constant amount of memory.
If I want to operate on a linear amount of memory-- n things-- how long is that going to take? If I just want to read everything in that thing, it's going to take me linear time, because I have to read every part of that thing.
OK, so in general, what we're going to do for the first half of this class mostly-- first eight lectures, anyway-- is talk about data structures.","Can you please if following this playlist will tech algorithm and data structure? Or is not enough?
Is this course related to data structures...?
Can I take this course without discrete math?
Can anybody tell me what's the difference between this course and the 2011 course mit released ? I'm confuse by the numbers. This course has 32 videos and that one has 47 which makes me think that the other course might have more material. Can anyone help ?"
"So, notice now that the policy parameters only appear in terms of the distribution of trajectories that we might encounter under this policy.
And this is, again, a little bit similar to what we talked about for imitation learning before or where in imitation learning, we talked a lot about distributions of states and distributions of states and actions, and trying to find a policy that would match the same state action distribution, as what was demonstrated by an expert.
Um, today, we're not gonna talk as much about sort of state action distributions but we are talking about sort of distributions of trajectories that we could encounter under our particular policy.
So, what's the gradient of this? Um, so, we wanna take the gradient of this function with respect to Theta.
So, we're gonna go for this as follows.
We are gonna rewrite what is the probability of a trajectory under Theta.
So, sum over Tau.
I wanna do probability of Tau [NOISE] times.
All right, first actually I'll whip it in here.
And then what we're gonna do is, make sure I get the notation the same.
Okay.
So, then what we're gonna do is, we're gonna do something simple where we just multiply and divide by the same thing.","I'm a bit confused by the notation - is pi(a|s) equal to pi(s, a)?
Why does rewriting the gradient with respect to theta of V reduce the variance? I thought the two were the same equation?"
"Okay.
So, so that is just that sum that we have there, right? V pi of end is 0, so let me just put that 0 there.
I'm going to put 0 there.
I only have one state here too, right? So, so th- I just have this other function of this one, stay, in.
So having an equation, I can find the closed form solution of V pi of in.
I'm just going to move things around a little bit.
And then I will find out that V pi of in is just equal to 12.
So, so that's how you get that 12 that I've been talking about.
So, so you just found out that if you tell me the policy to follow is stay, if that is the policy, then the value of that policy from state in is equal to 12.
Is it you always choose the same or- so you always choosing to state.
Yeah.
So, so the policy is a function of state.
I only have this one state that's interesting here, right? That, that one state is in.
So I need to- when, when I defined my policy, I need to kind of choose the same policy for, for that state, right? My policy says, in in you've got to either stay or you've got either quick- quit.","Can in the Dice Game If choose to stay for the step 1 and then quit in the second stage: will I get 10 dollars if I choose to quit in the stage 2? Because If I am lucky enough to go to second stage i.e the dice doesn't roll 1,2 then I am in the ""In"" state and by the diagram I have option to quit which might give me 10 dollar but for that I should have success in stage 1. Then the best strategy might change. Let know what are your comments?"
"Can you choose a sum of the integers so that-- I'll write it the sum of S.
But what this means is the sum over the ai's that are in S of the value ai.
I want that to equal t.
So this is the definition.
This is the constraint.
So I give you a bunch of numbers.
Do any subset of them add up to t? That's all this is asking.
This problem is NP-hard.
It's NP-complete, in fact, when you can guess which integers should go in the subset, and then add them up to see if you got it right.
It is NP-hard, but it's something special we call weakly NP-hard.
And why don't I come back to the definition of that in a moment? Let me first show you the proof.","I can't fully understand that the professor stated that ""X is NP-hard if every problem y  NP reduces to X"". Doesn't it mean that a problem(NP) would then reduce to a harder problem(NP-hard)? How should I interpret it? Thanks!"
"Any, any questions on this? Yes, question? [inaudible] Yeah, so the question is-is it really practical to remember all-all um, our training examples into test-time? And the answer is, in general, yes, you need to do that, and um, that makes kernel methods not very scalable as you get lots and lots of data.
Which is why, you know, in-in practice, you see when you have lots and lots of data, you see methods like neural networks and deep learning take over because they don't have this limitation that you need to remember all your examples, all your training examples.
But at the same time, there are algorithms that we're going to see, in fact later today uh, called the Support Vector Machine, where the support vector machine will result in beta vectors that are very sparse, which means most of them are zeros.
So you just need to remember a few of your examples, and both are called support vectors.
We will-will be going to support vectors, but in general, your beta vectors can be dense, which means you need to remember all your training examples all the way to test time, and in fact, that is a big obstacle for scaling your algorithms to big data sets.","If I understand correctly, we are no longer estimating parameters like theta and instead are storing the values of Beta and all the training examples. Since in this example we would have a tendency to overfit, how would we regularize it? Can we apply Ridge/Lasso on Beta instead of Theta?"
"So these are called the styles.
And you can control the marker, if you have a marker.
You can control the line.
You can control the color.
Also you can control the size.
You can give the sizes of all these things.
What you'll see when you look at the code is I don't like the default styles things, because when they show up on the screen, they're too small.
So there's something called rcParams.
Those of you who are Unix hackers can maybe guess where that name came from.
And I've just said a bunch of things, like that my default line width will be four points.
The size for the titles will be 20.
You can put titles on the graphs.
Various kinds of things.
Again, once and for all trying to set some of these parameters so they get used over and over again.",can u explain what you mean by predicts? Look at trim sort once
"This is poor notation.
This is the weight of the edge from the vi minus 1 to i.
And we've got-- it indexes from 1-- that's the first edge-- to k, which is the last edge.
So that's the weight of my path.
The weight of my transformed path-- I'm going to do it down here.
It's a little iffy.
The weight of my transformed path I'm going to say is the weight in this new weighted graph G prime.
This weight of that same path-- it's the same path-- is just going to be the sum of all of the reweighted edges.
So i equals 1 to k of my original weight of my edge, so from 0, i minus 1 to vi.
But what did I do? This edge is outgoing from vi minus 1.
So it's outgoing, so I add that weight-- that potential, sorry.
But that edge is also incoming into vi.
So when I reweighted the thing, I got a subtraction of h, vi.
Now, what happens here in the sum, this term, if I just took the sum over this term, that's exactly my original pathway.
So that's good.
But you'll notice that this sum has k terms, and this sum has the subtraction of k other terms.
But most of these terms are equal.
Along the path, all the incoming and outgoing edges cancel out.
So we're left with only adding the potential at the starting vertex and subtracting the potential at the final vertex.
So we've got, add h, v0 minus h, vk.
And why is that good? Well, that's good because every path from v0 to vk starts at v0 and ends at vk.","doesn't connecting all vertices in V to a supernode s with 0 weight edges makes all delta(s, v) to 0, (and makes all h(v) = 0)? Or is the s in the Johnson's algorithm another s different from the supernode"
"We want to retain the variance in the data but lose just the dimensions.
That's the idea with PCA.
Right? So- [NOISE] so suppose we are- we are, um, suppose we represent the low dimensional subspace with unit vectors u.
So if- u, you can think of u in R_d to be unit length.
Right? So if- if u is- represents, you know, um, a basis of the subspace of unit length onto which we want to project our data, you might remember from one of our audio lectures, um, if u is- is a basis vector, the projection of x onto the space spanned by u, will be the projection matrix of u- [NOISE] projection matrix of u times x.","While performing PCA, will the projected points (on a lower dimension) also have mean = 0? If not, how is maximizing norm equal maximizing variance?
Hello Professor, is there an intuitive reasoning for us to consider eigen vectors of the sample covariance matrix to maximize variance, whilst reducing dimensions?"
"But they're in the same ballpark.
And the same is true of the two standard deviations.
Well, that raises the question, did we get lucky or is something we should expect? If we draw 100 random examples, should we expect them to correspond to the population as a whole? And the answer is sometimes yeah and sometimes no.
And that's one of the issues I want to explore today.
So one way to see whether it's a happy accident is to try it 1,000 times.
We can draw 1,000 samples of size 100 and plot the results.
Again, I'm not going to go over the code.
There's something in that code, as well, that we haven't seen before.
And that's the ax.vline plotting command.
V for vertical.
It just, in this case, will draw a red line-- because I've said the color is r-- at population mean on the x-axis.
So just a vertical line.
So that'll just show us where the mean is.
If we wanted to draw a horizontal line, we'd use ax.hline.
Just showing you a couple of useful functions.
When we try it 1,000 times, here's what it looks like.
So here we see what we had originally, same picture I showed you before.
And here's what we get when we look at the means of 100 samples.
So this plot on the left looks a lot more like it's a normal distribution than the one on the right.
Should that surprise us, or is there a reason we should have expected that to happen? Well, what's the answer? Someone tell me why we should have expected it.
It's because of the central limit theorem, right? That's exactly what the central limit theorem promised us would happen.","I am a bit surprised he didn't comment on how the standard deviation of the sample has a bias as an estimator for the standard deviation of the population, and how you should divide by (n-1) instead of n (n being the sample size) when doing this estimation..."
"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
ERIC DEMAINE: All right, today we do NP completeness, an entire field in one lecture.
Should be fun.
I actually taught an entire class about this topic last semester, but now we're going to do it in 80 minutes.
And we're going to look at lots of different problems, from Super Mario Brothers to jigsaw puzzles, and show that they're NP -complete.
This is a fun area.
As Srini mentioned last class, it's all about reductions.
It's all about converting one problem into another, which is a fun kind of puzzle in itself.","can someone explain the mario and clause reduction to me, or provide a link for theory.
can someone tell me at which point he proves that 3dm is np complete... and what is the proof exactly?
Reduction ... Time point https://youtu.be/eHZifpgyH_4?t=22m28s . Chalkboard writing .. then A element of P ... seems to disagree with the notes from the MIT site ... which is written A element of NP ... . Are the notes incorrect?
Question. you say ""framing of these problems requires us to peel away the contextual embedding of the problems for which they are supposed to clarify."" - but isn't that the goal of all mathematics? We don't care about context we care about abstract patterns. So do you extend your concerns to mathematics broadly, or, to this specific problem?
Not being rude, but can someone explain the applications of such mathematics? What does this course teach and why is it useful? Thanks!"
"You may notice I'm not using the letter i.
We will get to why in a moment.
V jk.
Row j, column k.
That's going to be x sub j to the power k.
That's the Vandermonde matrix.
We can compute it in quadratic time.
It has quadratic entries.
We can use the trick we suggested earlier-- compute each term from the previous one by multiplying by xj-- and then we want to compute this matrix-vector product, and you can clearly do it in quadratic time-- I'm just computing each thing correspondingly-- and that's sort of the best you can do without any further assumptions.
So this takes-- if I want to compute this product, that's the coefficients to samples problem.
This is the same thing as computing V times the A vector, so this is a matrix-vector multiplication, which takes n squared time.
OK? On the other hand-- so, that's a problem because we're trying to beat quadratic multiplication, so if we spend quadratic time to convert over here it doesn't matter if this is linear time.
There are two problems.
One is that conversion costs too much.
The other is we don't yet know how to convert backwards.
But this matrix field gives us also the reverse transformation.
If we want to convert samples to coefficients, this is-- the best notation I know is from MATLAB.
How many people know MATLAB? A bunch.
So for you, it's V backslash A, but usually in linear algebra like 18.06 you see you have some matrix V times some unknown vector-- usually it's called x, here it's called a-- and you know the right-hand side.
You want to solve for this.
How do you do it? AUDIENCE: [INAUDIBLE] ERIK DEMAINE: Sorry? AUDIENCE: Multiply by the inverse? ERIK DEMAINE: Multiply by the inverse.
Yeah.
How do you do it in computer science? AUDIENCE: Gaussian elimination.
ERIK DEMAINE: Gaussian elimination.","is there a mistake ? we know V.A = Y ( V - vandermond matrix, A - coefficien matrix, Y - samples matrix) (multiplying by V inverse i.e. V^(-1) both sides) => V^(-1).V.A = V^(-1).Y => A = V^(-1).Y So to go from samples matrix to coefficient matrix we need to do V^(-1).Y right ??"
"So if you want to work with numbers, you have to explicitly tell it, I'm going to work with a number.
So even if you give it the number 5, it's going to think it's the string 5.
Yeah.
That's just how input works.
The next thing we're going to look at is ways that you can start adding tests in your code.
And before you can start adding tests in your code, you need to be able to do the actual tests.
So this is where comparison operators come in.
So here, let's assume that i and j are variables.
The following comparisons are going to give you a Boolean.","Why isn't ""Type a number..."" included in the output, as it is included within the quotation marks?
I dont know if this is entirely true, if i do ""ab"" == ""ba"". the result is false. (She said something about lexicographical)
When comparing string how will it compare a and 1
Why isn't she explaining the comparison of strings via ASCII code?
Could someone please explain what's the difference between the ""+="" and ""=+"" operators?
the nested if prints the quotient of x and y. however, you can't do this if y=0 as you will get a zero division error (if x=0 and y=0, 0/0 can't be evaluated)
on the 40th minute why i has the values 5 7 9 not 5 6 7 8 9 ????? please explain !!!"
"And one problem is that the model becomes too big, um, to train, and then the other problem is that the model has too many parameters, um, and overfitting can quickly become an issue.
So what I wanna discuss next is, er, two approaches, how to reduce the number of parameters of this RGCN-style model, um, using two techniques.
One technique will be to use, uh, block diagonal matrices, uh, and the other one will be to use basis or, uh, dictionary learning as it is called.
So let me first talk about, uh, use of block diagonal, er, matrices.
So the key insight is that we wanna make these Ws- W matrices, the transformation matrices, we wanna make them sparse.","if this is the case, then this approach will require more computation and isnt better than the RGCN approach."
"Notice this is an expression here-- i%2 == 0 is an expression that's going to evaluate to some value.
And as long as this part is something that evaluates some value, it can be anything you want.
And this line here return something tells Python, OK after you have finished executing everything inside the function, what value should I return? And whoever called the function is going to get back that value, and the function call itself will be replaced by that value.
OK so let's look at an example.
I'm going to introduce the idea of scope now.
And scope just means-- is another word for environment.
So if I told you that you could think of functions as little mini-programs, the scope of a function is going to be a completely separate environment than the environment of the main program.","im 12 and i dont understand what is func, help...
so, x = 3 def f(y): print(x) works but def g(y): x =+ 1 does not work. because it's not in the functions scope. so are only operations with a variable out of scope not possible? because in either case it has to know the value of x, whether you print it or whether you do something with it. so why does the second one not work?
when func_a( ) is mapped to z ... then can we write return z instead of return z( ) ?
Is it bad style to use a variable from outside the scope of a function?
when you print(function_name) it just prints out what the function returns?
if suppose the function h() in the last example is called twice, does the ""scope"" remain or is it formed twice? pls help
because it never was used has a print(x) inside the function h(), and doesnt has a return. So it returns None"
"That's great.
It's always going to be the same cost.
Order log n says the cost grows logarithmically with the size of the input.
It's a slow growth, and I'm going to remind you of that in a second.
We saw lots of examples of linear running time-- we're going to see a few more today-- what we call log-linear, polynomial, and exponential.
And the thing I want to remind you is that, ideally-- whoops, sorry-- we'd like our algorithms to be as close to the top of this categorization as we can.
This is actually described in increasing order of complexity.
Something that takes the same amount of time no matter how big the input is, unless that amount of time is a couple of centuries, seems like a really good algorithm to have.","Could you not argue that the intToStr() example is linear with respect to the number of digits in the input number? How do we decide what the input space is when it's ambiguous?
it says ""turns out that the total cost to copy is O(n) and this dominates the log(n) cost due to the recursive calls"" I agree that the cost will be O(n) but not with the part that says ""and this dominates the log(n) cost due to recursive calls"". The following is implied: O(log(n)) + O(n) = O(n), but such addition can't be the case. Log(n) is the amount of recursive calls, which should be used as a multiplicative factor with the amount of steps inside one function call. As was mentioned correctly, the amount of steps in each consecutive function call decreases by factor of 2. Suppose that n = 32; then the amount of function calls is s = log(n) = log(32) = 5. The total steps in all function calls combined is (1 - 1/(2^log(n))) * n = (1 - 1/2)n = (1/2 + 1/4 + 1/8 + ... + 1/2)n = n/2 + n/4 + n/8 + n/16 + n/32 = 32/2 + 32/4 + 32/8 + 32/16 + 32/32 = 0.96875n = 31  n (for large n) --> O(n). I don't see how the addition of log(n) with n is logical in the proof for the code being O(n).
Let's say the time required by a program for input of size n is n+100. If you now go from 10 to 20, for 10, it takes 110 secs. But for 20, it takes 120 secs. For 30, it takes 130 secs. But difference between times for 10 and 20 and between 20 and 30 is same (10). Observe that from 10 to 20, time taken doesn't double. But the difference remains same"
"All objects in Python are going to have a type.
And the type is going to tell Python the kinds of operations that you can do on these objects.
If an object is the number five, for example, you can add the number to another number, subtract the number, take it to the power of something, and so on.
As a more general example, for example, I am a human.
So that's my type.
And I can walk, speak English, et cetera.
Chewbacca is going to be a type Wookie.
He can walk, do that sound that I can't do.
He can do that, but I can't.","will string of length one also be a scalar object in python
Is it only in Python everything is an object?"
"So no one had solved it yet, but it will happen someday I'm sure.
So this gives you a clear sense of what the shapes look like.
So I want to prove these puzzles are NP-hard also by reduction from jigsaw puzzles.
So let's just bring up the picture.
So we just did this reduction.
It actually works in both ways.
They're really the same puzzle.
This does not work so easily in both ways.
But I'm going to take a jigsaw piece.
I'm going to turn it into roughly a square made up of little squares.
I'm not going to work on the half triangles, although I think it wouldn't be any harder.
It's a lot easier to think about square puzzles more like Tetris.","if you solve one problem proven to be NP- Complete, does that also solve PvsNP by default?"
"Now, why would I want to look at the fraction within approximately 200 of the mean? What is that going to correspond to in this case? Well, if I divide 200 by 2 I get 100.
Which happens to be the standard deviation.
So in this case, what I'm going to be looking at is what fraction of the values fall within two standard deviations of the mean? Kind of a check on the empirical rule, right? All right, when I run the code I get this.
So it is a discrete approximation to the probability density function.
You'll notice, unlike the previous picture I showed you which was nice and smooth, this is jaggedy.
You would expect it to be.
And again, you can see it's very nice that the peak is what we said the mean should be, 0.
And then it falls off.
And indeed, slightly more than 95% fall within two standard deviations of the mean.
I'm not even surprised that it's a little bit more than 95% because, remember the magic number is 1.96, not 2.
But since this is only a finite sample, I only want it to be around 95.
I'm not going to worry too much whether it's bigger or smaller.
All right? So random.gauss does a nice job of giving us Gaussian values.
We plotted them and now you can see that I've got the relative frequency.","total area of 40 bins is what i have concluded but why is that the ""fraction within ~200 of mean""??
I did not get the weight parameter in the formula shown at the beginning. It says [1/numSamples]*len(dist). However, numSamples is 1000000 and dist has always a length of 1000000 as well, so the weight will end up as 1. Am I missing something?"
"So this approach use both the feature information about the nodes, as well as, labels of the neighbors, but still, kind of, would depend on homophily-type principle.
And then we talked about loopy belief propagation.
That- that, um, included this label- uh, label-label potential matrix, uh, and, uh, thought about this as collecting messages, transforming messages, and sending a message to the upstream neighbor, as well.
This process is exact and, uh, well-defined on, uh, chain graphs and on trees.
But on graphs with cycles, um, it creates, uh, problems.
However, as I said in practice, cycles tend to be, uh, few or tend to have weak connections, so that in practice, cycles don't, uh, cause, uh, too much problem, and loopy belief propagation is a very strong allegory or a very strong approach for semi-supervised, uh, labeling of nodes, uh, in the graph.
So, um, with this, uh, we have finished the lecture for today.
","Summarising the lecture contents to myself, the majority of thoughts are concerned with potential functions. I suppose, definition of Phi and Psi is the part engineering work that depend on the specific task. It's obviously the most non-trivial step in the belief propagation approach. I'd fancy a couple of examples of how can they be defined in specific tasks."
"So that's the magical transformation we're going to cover, and it is called the fast Fourier transfer.
Fast Fourier transform is the algorithm.
Discrete Fourier transform is that transformation mathematically.
Cool.
So the whole name of the game is converting from coefficient representation to samples, or vice versa.
Turns out they're almost the same, though that won't be obvious for a long time-- till the end of the class.
Any questions before we proceed? Yeah.
AUDIENCE: [INAUDIBLE] multiply repetitions, why not we evaluate a and b first then multiply? ERIK DEMAINE: Ah.","How does one perform FFT on a larger domain consisting of multiple cosets of a multiplicative subgroup of the field? I've heard it can be done but couldn't find any sources that explained how.
So what is the math doing in practical terms? If I understand correctly, it's using the behavior of a signal over time to determine specific properties of that signal at specific moments. Is that correct?
Does anyone have intuition as to why Fourier transforms pop up here?
Okay, we've figured out how to convert between different representations of polynomials, but how do we go from there to the familiar application of the FFT - converting between the time domain and frequency domain? Given a bunch of samples, we want a weighted sum of sinusoids, but what we get here is the coefficients of a polynomial.
is there a mistake ? we know V.A = Y ( V - vandermond matrix, A - coefficien matrix, Y - samples matrix) (multiplying by V inverse i.e. V^(-1) both sides) => V^(-1).V.A = V^(-1).Y => A = V^(-1).Y So to go from samples matrix to coefficient matrix we need to do V^(-1).Y right ??"
"So, um, let me talk about some of these concepts, uh, in a bit more, uh, detail.
So first, I wanna talk about the notion of batch normalization and the goal of batch normalization is to stabilize training of graph neural networks.
And the idea is that given a batch of inputs, given a batch of data points, in our case, a batch of node embeddings, we wanna re-center node embeddings to zero mean and scale them to have unit variance.
So to- to be very precise what we mean by this, I'm given a set of inputs, in our case this would be vectors of, uh, node embeddings.",From the lecture it is not clear how Batch Norm works for the mini-batch.
"And well what is the expected utility from this point on? We are in a chance node, so many things that can happen because I have like nature is going to play and roll its die, and anything can happen.
And they're going to have in transition, S, a, S-prime and with that transition probability, I'm going to end up in a new state.
And I'm going to call it S-prime, and the value of that state- again, expected utility of that state is V pi of S-prime, okay.
All right.
So, okay.
So what are these actually equal to? So I've just defined value as expected utility, Q value as expected utility from a chance node, what, what are they actually equal to? Okay.
So I'm going to write a recurrence that we are going to use for the rest of the class.
So pay attention for five seconds.
There is a question there.
I understand how semantically how pi and v pi are different, in like actual numbers, like expected value- how are they different? So they're- both of them are expected value.
Yeah.
So it's just- one is just a function of state the other one you've committed to one action.
And the reason I'm defining both of them, is to just writing my recurrence is going to be a little bit easier, because I have this state action nodes, and I can talk about them.
And I can talk about how like I get branching from these state action nodes, okay? All right.
So I'm going to write a recurrence.
It's not hard, but it's kind of the basis of the next like N lectures, so pay attention.
So alright.
So V pi of S, what is that equal to? Well, that is going to be equal to 0, if I'm in an end state.
So if IsEnd of s is equal to true, then there is no expected utility that's equal to 0.
That's a easy case.
Otherwise- well, I took policy pi S.
Someone told me, take policy pi S.
So value is just equal to Q, right? So, so in this case, V pi of S, if someone comes and gives me policy pi, it's just equal to Q pi of S, a.","I think the given definition for value-action function (Q(s, action)) is not correct. In fact value function is the summation of value-action functions over all actions."
"There are two kinds.
, Discrete and these when the values are drawn from a finite set of values.
So when I flip these coins, there are only two possible values, head or tails.
And so if we look at the distribution of heads and tails, it's pretty simple.
We just list the probability of heads.
We list the probability of tails.
We know that those two probabilities must add up to 1, and that fully describes our distribution.
Continuous random variables are a bit trickier.
They're drawn from a set of reals between two numbers.
For the sake of argument, let's say those two numbers are 0 and 1.
Well, we can't just enumerate the probability for each number.","Nice. Can I just argue something, there is always an example with coins but I don't think I have ever heard someone just adding a disclaimer that tossing a coin is not a random process. In theory, if you could start the tossing under the same initial conditions you would get the same outcome. So it is possible to get an infinite number of heads if you just manage to toss the coin the same way an infinite number of times (i.e. with the same initial conditions). I don't think that would be too difficult to achieve either. An example of a true random process is nuclear decay of radioactive atoms.
i have one question, for simulation we used different softwares, which method (discrete, continuous or Monte Carlo) is used in CST ( computer simulation technology)
but you didnt define combinatorics for us lol
the next toss is independent of the previous toss ;but there is a different question that can be asked :what is the probability of of x tail(heads) in a row=1/2^x .Two completely different betting strategies
Thank you Professor John guttag. You're a great teacher and reading your book --""introduction to computation and programming with python"" has been a great experience thus far. Please, if you don't mind could you clarify me on this: When is an event said to be truly random? Or better still do we have truly random events? Randomness implies causal nondeterminism and from my little knowledge that's almost nonexistent. Events that yield uncertain outcomes are better delineated as predictively nondeterministic which doesn't imply that they are random but in stead reveals the limitations of our probing instruments and/ or statistical technique in unsheathing the nature of such events and correctly predicting their future states. No event to me qualifies as random, the outcome of coin flips, die throws could sufficiently be predicted to very high degrees of accuracy if only we could be patient enough to understand the physics of the processes - the coin/ die initial position, throwing/ flipping force, air drag, et cetera. So what processes are random?"
"But if they were, then you instantly get a stable set of marriages.
There's another stable set that's not quite so obvious.
And you can check that all of these pairs have no instability.
There's no rogue couples in here when I marry 5 to A and 1 to E.
This is a so-called ""boy optimal"" set of marriages.
It turns out that in this set of marriages, every boy gets the best possible spouse that he could possibly get in any set of stable marriages.
There's no set of stable marriages in which boy 5 gets a more desirable girl than A.
There's no set of stable marriages in which boy 1 gets a girl that's more desirable to him than girl E.
The sad news is that it's simultaneously pessimal for the girls.
That is, each girl is getting their worst possible spouse among all sets of stable marriages.
We'll examine that further in a minute.
But let me just point out that this is more than a puzzle.
I mean, it's fun, and it's a nice puzzle, but it's more than a puzzle.
Because the original case where it was studied or published first was in a paper by Gale and Shapley in 1962.","What if you only want to now if 2 like each other? Not who they prefer but who they would take and if there is a match between two, so that two would take each other? The could also be multiple matches"
"So you had to be the best.
Now if you ended up being tied, you ended up having to play an elimination game.
But the whole purpose of this analysis is deciding whether you still have a chance to make the playoffs or not.
So the goal here is you want an algorithm that is going to look at the standings and decide if your team is alive or not.
Is there a chance of God is great, whatever, everything goes your way, angels in the outfield, right? Are you going to make the playoffs? And so you can be very optimistic.
And you might think that this is a straightforward, I'm going to add up a couple numbers and figure this out.","The last example ,from what I understood allows team NY to win only one game w.r.t the games it plays with the other 5 teams ,but can there be a case where the team wins a game against a team we have not considered and hence not allowed team 5 to still make it ? Please correct me if I am wrong"
"So that edge in there, in G had some capacity c u v which was greater than zero.
But this edge does not exist in G of f, correct? So what can I say? Go ahead.
AUDIENCE: [INAUDIBLE] PROFESSOR: Exactly.
In the original graph, we have saturated this edge-- that was my best throw of the term.
I like that.
And you guys didn't notice, but maybe it'll be on video.
Actually, it won't be on video unfortunately.
But so I got this edge here in the original graph and I didn't get this dotted edge here because the residual capacity was zero.
The original capacity was nonzero.
The residual capacity is zero.
The only reason that happens is because C f u v is zero since if C f u v greater than zero then v would belong to S, not v belonging to T as assumed.
So that's essentially what we've determined here.
So this means that f of u v equals c of u v simply because c f u v equals c u v minus f u v which is 0.
All right, so that's the statement I can make.
Now, I did not have any constraint on u and v.
I just said u belongs to s and v belongs to t.
And for each of those things, I know that I can't have edges in the residual network between u and v which means that any edge that exists-- maybe there's no edge in the original network, but if there was an edge in that original network it's saturated according to this argument.
So every edge from S to T in the original network is saturated, and that essentially means that I've gotten to the capacity of my cut.
That's essentially what that means.
If I've saturated every edge, I've reached that capacity.
OK, so that's it.
All you do once you recognize that you had an arbitrary choice of u and v, you just say summing over all u belonging to s and v belong to t yields f of S T equals c of S T.","max flow |f| = some c(S,T) cut..... not any I think. some (S,T) cut may have a larger capacity which is okay since all we need id f < any (S,T) cut. here , he just showed that there is always some (S,T) cut out there such that |f| = capacity of that (S,T) cut. did I make sense?"
"So we'll start at node S.
What are our choices at node S? Well, I'm going to force you guys to help.
So it won't be quite as fast, but it'll be fun.
So what are our choices at node S? You.
AUDIENCE: Me? PROFESSOR: Yes.
AUDIENCE: A or B? PROFESSOR: Not quite, and this is something I like about this problem.
You got most of them.
Do you see that there might be another choice? Everyone? Yeah, C.
This is a big problem that has happened on a few different quiz problems where there's a sort of a grid that looks like a tree, or a graph that looks like a tree, where people aren't as willing to go up as they are to go down.","Hi, Is there anyone who knows and can explain how he came up with the choice tree using the table? Thanks in advance..."
"But, um, I don't know.
If, if you've taken a calculus class a while back, you may remember that the derivative of a function is, you know, defines the direction of steepest descent.
So it defines the direction that allows you to go downhill as steeply as possible, uh, on the, on the hill like that.
There's a question.
How do you determine the learning rate? How do you determine the learning rate? Ah, let me get back to that.
It's a good question.
Uh, for now, um, uh, you know, there's a theory and there's a practice.
Uh, in practice, you set to 0.01.
[LAUGHTER].
[LAUGHTER] Let me say a bit more about that later.
[NOISE].
Uh, if- if you actually- if- if you scale all the features between 0 and 1, you know, minus 1 and plus 1 or something like that, then, then, yeah.","why in cost function he did 1/2 and not 1/2*m ?
I don't really understand what you mean by 1/2m. However, from my understanding, the 1/2 is just for simplicity when taking the derivative of the cost ftn the power 2 will be multiplied to the equation and cancellyby the half."
"So this is equal to the summation over j of the expectations of these individual ones.
One of these j's is the same as i.
j loops over all of the things from 0 to u minus 1.
One of them is i, so when xhi is hj, what is the expected value that they collide? 1-- so I'm going to refactor this as being this, where j does not equal i, plus 1.
Are people OK with that? Because if i equals-- if j and i are equal, they definitely collide.
They're the same key.
So I'm expected to have one guy there, which was the original key, xi.
But otherwise, we can use this universal property that says, if they're not equal and they collide-- which is exactly this case-- the probability that that happens is 1/m.
And since it's an indicator random variable, the expectation is there are outcomes times their probabilities-- so 1 times that probability plus 0 times 1 minus that probability, which is just 1/m.
So now we get the summation of 1/m for j not equal to i plus 1.
Oh, and this-- sorry.
I did this wrong.","Can someone explain how it's possible for the universal hashing function to make the probability of two keys hashing to the same value be less than or equal to 1/m, for ALL keys? Wouldn't the perfect case be that after hashing, the keys are uniformly distributed, in which case the probability should be equal to 1/m? I think I'm missing something here but for the probability of a pair, to be hashed to the same value, to be less than 1/m, wouldn't that imply another pair is necessarily greater than 1/m?"
"The problem was that Russell came along and looked at Frege's set theory, and came up with the following paradox.
He defined W to be the collection of s in sets such that s is not a member of s.
Frege would certainly have said that's a well defined set, and he will acknowledge the W is a set.
And let's look at what this means.
This is a diagonal argument.
So let's remember, by this definition of W, what we have is that a set s is in W if and only if s is not a member of s.
OK, that's fine.
Then just let s be W.
And we immediately get a contradiction that W is in W if and only if W is not in W.","Why does he call it ""self-application,"" exactly? I'd call it self-reference, where the self-reference defines the entity itself. I'm referring to the vicious circle principle, and I hope someone comments on this, because it is a genuine question. I'm not understanding this, or saying it exactly correctly, I think."
"You do a merge.
So what do you have? So here you have B minus 2, and here you have B minus 1.
And you get 2B minus 3.
Well, you've got another element.
You also take the parent.
So how do you do the merge.
I just want to show you the merge first.
So the way you do it is you move the parent down, and you merge these two.
Seems OK? So you move the parent node down and merge these two.
And, well, now this comes together, and this points into the new node.
Sort of clear what's going on? Questions? Yes? AUDIENCE: So now the parent is underfull? PROFESSOR: Well, so you have-- yeah, exactly.
So you have decreased the size of the parent, so it might be underfull.
So you propagate.
Anything else? AUDIENCE: So are these all different techniques for doing that? PROFESSOR: So there are two cases.
So either you have a sibling which has extra nodes to donate to you or you don't.
If you don't, then you have to do this.
AUDIENCE: But what about that case? Or is that just like-- PROFESSOR: No, that is moving it down to the leaf.
Once you move the deletion down to the leaf, so here we have something now.
And now you move it all the way back up.","isnt his definition wrong though? in CLRS 3rd edition page 489 the definition for internal nodes of degree t is: t <= num children <= 2t
so how can it adjust 19 children in it there"
"Of course, I'm not going to just go with one.
I want to put a whole bunch of stuff in there.
So I'll just run a bunch more simulations.
No [? dice.
?] I don't even have an entry at all yet for T F here.
That's because I haven't run enough data.
So let me clear it instead of doing it one at a time.
Let me run 100 simulations.
See, it's still not too good.
Because it says this T T probability true.
This just because I'm feeding it data, right? And I'm keeping track of what the data elements tell me about how frequently a particular combination appears.","Can you use the same data both for estimation of the probabilities, and for model checking?"
"So for a matrix-vector operations, first we're gonna consider operations where you have a matrix, let's call it a and a vector, call it x, and this, let's call it M by N, and this is, uh, n.
So A belongs to R_m by n, and x belongs to R_n.
Right? So this is- um and- and so A_x is now of dimension.
[inaudible] A + 1, mm-hm.
So how do you think about, um, this operation? So first you can, um, um, think of, uh, the matrix as a set of rows, right? So each row has n elements.
Right? And the vector you're multiplying it is a column vector.
And all you're doing is the inner product, right? Take the first row, get the inner product with the matrix, and you get a scalar, right? Take the second row do the inner product, you get another scalar.
And you're gonna get as many number of scalars as the number of rows you have in the matrix, right? So- so Ax R_m.
You can write this as m by one but, you know, um, we assume vectors are column vectors.
So if I write just, you know, R_m, you know, just assume it's- it's um, m by 1 if you want to think of it that way.
Another interpretation of, um, matrix-vector, um, multiplication is- it's the same matrix, except you're gonna think of them as columns.
Okay? m by n.
This is n, so this is A and this is x.
So this has- I'm just gonna use different symbols.","Is there a specific relationship between column space of a matrix and number of eigen vectors? Can the latter be greater than the former? Is there a relationship between rank, columnspace and eigenspace?
Rank is the number of independent vectors. Or the number of vectors generating the span. Or simply the d of the system. If rank of the system or matrix is 3, it mean we have 3 independent vectors which had created the system. Column space and Eigen space has no such relationship. Atleast I am unaware. If you know, please let me know."
"And so, lifting Gamma up, maximizing Gamma has effective maximizing the worst-case examples geometric margin, which is, which is, which is how we define this optimization problem, okay? Um, and then the last one step to turn this problem into this one on the left, is this interesting observation that, um, you might remember when we talked about the functional margin, which is the numerator here, that, you know, the functional margin you can scale w and b by any number and the decision boundary stays the same, right? And so, you know, if- if your classifier is y, so this is g of w transpose x plus b, right? So if- let's see the example I want to use, uh, 2, 1.
If w was the vector 2, 1- [NOISE] Let's say that's the classifier, right? Then you can take W and B, and multiply it by any number you want.","Question for the kind hearted people who understood: in the geometric margin, y is either 1 or -1 right? But wasnt it mentioned before that y is either 1 or 0? I got a little messed up on that."
"AUDIENCE: 128 raised to 2/3 maybe? SRINI DEVADAS: Yeah, yeah.
Two thirds.
Exactly right.
So what you want to do is, you want to start-- it makes sense that when you have lots of balls, in terms of being able to use them, that you start with larger intervals and you shrink them.
That's exactly what happened with two balls.
You started with k in the case where we had two balls.
And then we are down to one when we have one ball.
So that's really what you're doing.
When you have one ball, all you can do is go one floor at a time.
When you have two balls, you go k, where k happened to be square root of n.
And then we had three balls, you kind of want to start with like n raised to 2/3 in terms of the interval.
And then if I had four balls, the argument would be that I would start with n raised to 3/4.
And then I would go to square root of n.",How do u generalize if we have 4 (or more) balls... penta-numbers in 4D space? ;-)
"And if at any point, you're fine, you look at the parent node and go, OK, that's fine.
That's in the range.
But every time it overflows, you can keep going.
And how many times can you do this? You can do this all the way up to the root.
And when you reach the root, either it's fine or the root is too big.
It's reached 2B minus 1.
And then you split the root, and you get one single [INAUDIBLE] up there.
So that, in answer to your question, that is why you need that property in some sense.
Not a very convincing argument, but sort of.
So let's actually do an insertion in this tree we have here.
So we are going to insert 16.
So 16 comes in here.","In the Cormen textbook, section 18.1, it is written that if a node has 2*B-1 keys then it is called ""full node"" and once the # keys become 2*B then split occurs. But in this lecture, split occurs at 2*B-1 itself as here max # keys should be strictly less than 2*B-1. So practically which one to prefer is the confusion here 
can someone explain why in the last bit we chose to rotate 17 on the left over 30 on the right? thxxxxx in advance!
I think the reason to use these trees is balancing! Near the beginning, he asks ""Why use B-trees over Binary Search Trees"". He then explains something about avoiding reading memory from disk. This is not the reason I was taught in my class. The reason I was taught, is that these trees will stay balanced. With a Binary Search Tree, the algorithm of insert could result in the tree being extremely deep on one branch and very shallow on others. This means you will not actually get log(n) performance, since most of the data is in longer branches.
B is defined as the branching factor. By definition, this establishes a bound on the number of children: [B,2B) and hence a bound on the number of keys: [B-1, 2B-1]. You can what B is by taking the floor of lg(# of children in any branch)....(correct me if im wrong here...)
Any one please explain how its no of children B<= No children < 2B . When B=10 it can have macimum of 11 children . then ???
isnt his definition wrong though? in CLRS 3rd edition page 489 the definition for internal nodes of degree t is: t <= num children <= 2t
so how can it adjust 19 children in it there
Why log n? shouldn't it be log n base 3"
"Okay? So I'm going to say, I get a 4 but then after that point I'm actually just going to substitute this 11 in.
Okay? This is kind of weird, right, because normally I would just see, okay, what would happen? But what happens is kind of random.
On average it's going to be right but, you know, on any given case, I'm gonna get, like, you know, 24 or something.
And the, the hope here is that by using my current estimate which isn't going to be right because if I were, if it were right I would be done but hopefully it's kind of somewhat right and that will, you know, be, you know, better than using the, the kind of the raw, rollout value.
Yeah, question.
You, you would update your current estimate at the end of each episode, correct? Uh, yeah.
So the question is, would you update the current estimate, um, after each episode? Yeah.
So all of these algorithms, I haven't been explicit about it, is that you've seen an episode, you update, uh, after you see it and then you get a new episode and so on.
Yeah.
Sometimes you would even update before you're done with the episode, uh.
[NOISE] Okay.
So, uh, let me show this, uh, what, um, this algorithm.","I think it should be (4+8+16)/3, as I believe their last run has four 4 values."
"Um, I want to mention- before we wrap up, I want to mention one, one unusual fun fact about LQR and this is very specific to LQR.
Uh, and, and, and it's convenient, uh, but, but, er, let me say what the fact is and just be careful that this doesn't give you the wrong intuition because it doesn't apply to anything other than LQR, which is that if you look at where, um, so first, if you look at the formula for L, ah, let me see.
Move this around.
[NOISE] All right.
If you look at the formula for L_t, you need to compute, I mean the, you know, the goal of doing all this work is to find the optimal policy.
Right? So you want to find L_t so that you can compute the optimal policy.
You notice that L_t, um, just depends on Phi but not Psi.
Right? Um, so, you know, and, and maybe it's gonna make sense.
You're going to- when you take an action, you get to some new state and your future payoffs is a quadratic function plus a constant.
It doesn't matter what that constant is.
Right? And so in order to compute the optimal action, in order to compute L_t, you need to, you need to know Phi or actually Phi T plus 1 but you don't need to know what is Psi T plus 1.","max is applied for both the terms and to understand what expectation of quadratic function turns out be it is considered separately leaving out the first term. But it is true that he did not mentioned the 1st term when solved for a_t , but ig the result is when it is included"
"This times 1 is going to produce something that's non-zero, and then all of the other ones are going to produce 0.
So I'm just adding a bunch of 0's to this non-zero multiplied by one.
So I'm going to get something that's non-zero.
Right? All make sense? So I'm going to see something here, which is the jth entry that's not equal 0.
And so that implies that Dv is not equal to 0.
And in particular, what I'm saying is Dv of j-- so if I just look at that entry that is identically Dij, which is not equal to 0.
Because I'm multiplying it by 1 and I'm adding a bunch of 0's to it.
That's it.
OK.
Yeah.
A question.",Wasn't the professor correct in making the jth value 1 instead of the ith value as pointed out by one of the students?
"Well, height over the width.
And so we know that the derivative of del- f prime, that's the derivative of f at the point theta 0, that's equal to the height, that's f of theta, divided by the horizontal.
Right? So the derivative, meaning the slope of the red line is by definition the derivative is this ratio between this height over this width.
Um, and so delta is equal to f of theta 0 over f prime of theta 0.
And if you plug that in, then you find that a single iteration of Newton's method is the following rule of theta t plus 1 gets updated as theta t minus f of theta t over f prime of theta t.",can anyone explain me where that x came from in the final equation of gradient ascent
"And you can see that the first two or three lines of code are the trivial case of two elements that needed to be sorted.
So let's go back and forget that.
Let's look at this picture and think about how you'd break this up.
How would I bring this up? So think two-dimensionally.
And merge sort was a single dimension.
OK, think two dimensional.
This is two dimensional.
This is a two-dimensional problem.
It's in that sense, closer to N-queens.
But merge sort was simply a single dimension.
All right, I see Kevin had-- Fadi had his hand up.
Kevin has a hand up.",the merge sort code shown only works on arrays are a length of a power of 2. Adding another condition for len(L) == 1 in mergeSort that returns a single element array and a 2 element array will take care of the general case. Otherwise you'll get stuck in an endless recursion loop since it can't handle the base case of a 3 element array.
"Um, model free approaches have an explicit value function and a policy function and no model.
Yeah.
Going back with [NOISE] the- the earlier slide, I'm confusing when the value function is evaluated ice with the- with well the setting yes.
So, why is it not [NOISE] S_6 that has value of 10 because if you try right at S_6 you get to S_7.
You were saying well how do I- when do we think of the rewards happening.
Um, we'll talk more about that next time.
When really, uh, there's many different ways people think of where the rewards happening.
Some people think of it as the reward happening for the current state you're in.
Some people think of it as it's the reward you're in [NOISE] and the action you take.
And some people- some- another common definition is r- SAS prime, meaning that you don't see what reward you get until you transition.
And this particular definition that I'm using here we're assuming that rewards happened in one year in that state.","Does anyone have the feeling that the lecturer has a poor clarification on the notation she used.
Can you provide some examples ? I think it was very clear."
"ROFESSOR: So it's time to examine uncountable sets.
And that's what we're going to do in this segment.
So Cantor's question was, are all sets the same size? And he gives a definitive answer of no.
Cantor's theorem, which we're about the present, will show that, in fact, there isn't any biggest infinity.
For any given infinity, you can find a bigger one in a very simple way.
But let's begin by coming up with the simplest form of Cantor's diagonal argument, of how do you prove that a set is not countable? Remember, a set is countable if you can list it, possibly with repeats.
So A is countable if there is a sequence, a0, a1, a2, such that every element in the set A shows up at some time or other in the list, possibly more than once.
And the only things in the list are elements of A.
And we saw as an example that the finite bit strings, the finite strings of zeroes and ones, with the finite binary words, are an example of a countable set.
And we claimed last time, and now we're about to prove that the difference is that if you look at the infinite bit strings-- One-way infinite.","The constructed sequence is different from every member of the list because it is the last member of the list, which doesnt exist just as the last member of 1,2,3,... doesnt exist because it is an infinite, endless, sequence. And how do you construct an infinite (endless) list in the first place?
Is Cantor theorem using Axiom of Choice?
The axiom of separation says the set exists, it doesnt say you can construct it. And the axiom of infinity only guarantees the existence of a set containing the natural numbers.
Fistro Man I don't understand your examples. How do you say that one set is bigger than the other using your examples?"
"And this is probably seen through a simple, uh, visualization, which- let me have a quick look at.
[NOISE] Any questions on this so far? Yes, question.
[inaudible].
So- so the question is, uh, what happens, uh, can we use, uh, an unsupervised learning setting to learn the different cluster centers and use that as a classification algorithm.
[inaudible].
Yeah, it might or it might not have the same as a supervised learning algorithm.
Yeah, so supposing this is, you know, uh, think of this as a collection of points that are given to us where each green point is, uh, you know, is- is, uh, a data point in, uh, x_i in R_d and the way, uh, the algorithm, uh, works like this.
We- here we assume k equals 2 and the red X and the blue X, you can think of them as mu_1 and mu _2, which are randomly initialized.",Is the topic of Tree Ensembles not taught in this version of course ?
"For the same mathematical answer there can be multiple algorithmic answers.
We saw that with kernels as well.
Right.
For the same- for the same inner product, we could either calculate the explicit feature representation.
We take the inner- the, the dot-product between them, or we can directly calculate the- the kernel function.
Right.
They're- they're mathematically equivalent but algorithmically different.
Similarly, the backpropagation mathematically is just the chain rule, right.
But algorithmically, we calculate it in a way such that it is memory efficient and we reuse a lot of the computation while- when, uh, um, when- when, when- we use a lot of the intermediate computations when calculating, right.
So that's- that's backpropagation.
Uh, and let's- let's see how we go about doing it.
Any- any questions so far? [NOISE] Before we- we go into that propagation, there was a- a question asked earlier.
Right? Why- why can we not- what would happen- or rather the question was, wouldn't all the neurons in say the first layer, learn the same thing, right? Why- why aren't we just learning multiple copies of the same logistic regression? And the answer of that is, if we perform an initialization where the entire network is initialized to 0s, all the weights and biases are initialized to 0s, then that's exactly what will happen.","Hello, In this lecture, you mentioned that a Neural network for classification with a sigmoid activation can be considered as some combination of logistic regressions (one for each neuron starting from the 1st hidden layer). Knowing that logistic regression's loss function is convex and NN's are not convex, can we minimize each logistic regression individually and use the optimal parameters obtained from each model to reach a global minimum for the Neural Network?"
"We'll be a little bit pedantic there.
But that doesn't mean you shouldn't ask questions, means if you don't understand something, you should, and I haven't done my job.
So just fire off a question in any form you like.
Then we're going to talk about linear regression, which as I said, is fitting a line, except where we fitting high dimensional lines eventually.
So we're going to want to abstract that away.
We'll talk about batch and stochastic gradient descent, which are two algorithms in machine learning as Tengyu talked about.
We're not great with terminology.
This algorithm was called incremental gradient descent in the '60s.
It's been around forever.
Our incremental gradient methods actually wasn't-- even it's not even a descent method formally, doesn't matter.
The point is these are old things that people have been using for a long time.
And weirdly enough, it's what we use every day.",Apologize in advance if I sound rude but I would like to know what sets out this course different from Andrew NG's Machine Learning course on Coursera?
"So it's going to take logarithm time.
And now I can store, with that node, my pointer to this data structure.
Does that makes sense? And in the other case, I kind of do the same thing.
If it's bigger than the smallest thing here, I pop that smaller thing out, stick it in there, and I stick my new guy in here, cross-linking each of those pointers along the way.
Does that make sense, hopefully? Kind of? Kind of? OK.
And for update, very similar.
If I want to update a certain bidder, I look in this data structure, find the bidder, traverse that pointer to wherever it is in one of these AVL trees, right? If it's in this one, I just remove it from the tree, or I remove it from the tree and then I re-insert with whatever the new bid is.","what is the item in AVL tree  and  in problem4-3 ? i dont understand that what is good point of cross-linking.
In 4-3; assume everyone has the same bid and we update someone's bid, in the worst case, we need to iterate through O(n) items to update the bidder set AVL tree because all bids are in the same list. If we always change the bid of one person back and forth, in the worst case we have an amoritized cost of O(n/2) = O(n). Doesn't the introduction of multisets change the worst case running time? Probably a dictionary keyed on the bidder id would resolve the issue but does introduce expected running times..."
"This may have millions of values in this vector.
And we can then refer to the function represented by this neural network as f, which will give us a distribution over y given the input x parameterized by theta.
So this should follow fairly standard notation that you may have seen before.
Now in single-task supervised learning, we will be given some form of data set, which has input output pairs.
So a number of examples of images like the tiger and labels like the options on the right.
And then we will define a loss function that tells us how good is that model at performing that task.
And our goal will be to find the parameters that minimize that loss function.
So try to find the parameter setting of one of these neural networks such that we do well on a classification problem for example.
And so a typical form of this loss function might be something like negative log likelihood.
This would look something like this, where we're measuring the likelihood that f assigns to a given label given x.
And then negating that because typically loss functions are things that we minimize.
And then trying to minimize the negative probability of the label given the input.",Is there possibility of building a model which can do object detection and image classification
"If I have that solution, then I can construct the solution to the bigger problem really easily.
Wow.
Well, all of the things that were in that solution to the smaller problem have to be part of the solution to the bigger problem.
They're all subsets of 1 to n, because they're all subsets of 1 to n minus 1.
So I'm going to add all those in.
And then I'm going to say, let's take each one of those and add n to each of those subsets.
Because that gives me all the rest of the solutions.
I've got all the ways to find solutions without n.
I get all the ways to find solutions with n.
That may sound like a lot of gobbledygook, but let me show you the example.
There is the power set of the empty set.
It's just the empty set.
Get the power set of 1, I include that, and I include a version of everything there with 1 added to it.","For the power set, wouldn't it be better to put the last element (""extra"") in a list of itself before adding it to ""small""? This way the function will work for all variables that can be indexed (tuple, list, string) instead of working only with lists. def genSubsets(L): if len(L) == 0: return [[]] else: smaller = genSubsets(L[:-1]) extra = [L[len(L)-1]] # put the last elem in its own list new = [] for small in smaller: new.append(small + extra) return smaller + new
Why is res = [] needed in the power set problem code? I don't see any reason why it needed to be there."
"And that one's dead-on at 0.01.
So if I run enough of these simulations, I get a pretty good idea what the probabilities ought to be given that I've got a correct model.
OK, so that takes care of that one.
And of course, I didn't draw the other things in here.
But by extension, you can see how those would work.
Oh.
But you know what? I think I will put a little probability of raccoon table in here.
Because the next thing I want to do is I want to go the other way.
This is recoding tallies from some process so I can develop a model.
But once I've got these probabilities, of course, then I can start to simulate what the model would do.","Can you use the same data both for estimation of the probabilities, and for model checking?"
"And finally, if I ask you what was the smallest number of US coins that could make $1.17, again, we don't have to worry about existence because the well ordering principle knocks that off immediately.
Now for the remainder of this talk, I'm going to be talking about the nonnegative integers always, unless I explicitly say otherwise.
So I'm just going to is the word number to mean nonnegative integer.
There's a standard mathematical symbol that we use to denote the nonnegative integers.
It's that letter N at the top of the slide with a with a diagonal double bar.
These are sometimes called the natural numbers.
But I've never been able to understand or figure out whether 0 is natural or not.
So we don't use that phrase.","I am quite disappointed that when introducing the Well Ordering Principle, he does not at least mention the Axiom of Choice (AC). I understand AC to be the foundational axiom leading to the Well Ordering principle, and the subject of a fascinating historical and current debate.
can you please help me to make 3 analogies for the well ordering principles (WOP) and well ordering axiom or WOA
What is an example of a non-empty set of nonnegative rationals that does NOT have a least element??? can't wrap my head around that
But a set of negative integers also has the smallest number in it! Like that: -1; -8; -12, the least number here should be -12. What am I missing?
How can there be an empty set of non-negative integers? What's the difference between ""non-empty set of non-negative integers"" and ""a set of non-negative integers?""
It is interesting to know that zero is considered nonnegtive Would assume it is negative due to the fact zero = 0 the lack of any real number? Is there any proof on this?
he did not finish his proof of squared root of 2 is irrational.can anyone explain the remaining part? just dont understand how we can prove by wop
I don't see where the square root of 2 comes into play here. Explanation seems to drift away to the least element theroem without ever coming back... ? Am I missing something ? Thanks in advance
Consider a nonempty subset of nonnegative integers such as S = {0}. Does S have a least element? If it had couldn't we also say that it has a greatest one too? So, the least element would be identical to the greatest element which somehow sounds nonsensical! Dear Professor Meyer, please, could you share your thoughts on my given example?
I reviewing the fundamental knowlodge for computer science, but it's the second time that square of 2 is not clear to me how was proved that is rational, I know what is rational, but I didn't undestand this explanation of prove. I will look for anothers proves or people explaning in another way.
Consider the following set: {x: x  (0, ), x is rational} Can you tell me which is the least element in this set; 1/100 or 1/1000? Whichever element you decide is the least, you can always come up with a less element.
I think it was suggested that with N integers you always know that the smallest number is 0 but the smallest number with Z can go to -
A finite set would, but the set of all negative integers is a set of negative integers and has no least element.
There is in This set but not in ""EVERY"" set. e.g. {1, 0, -1, -2, ...} this dotdotdot implies there's no least element.
but this does not prove irrationality unlike proof by contradiction. Please elaborate if i am wrong"
"And by that, I mean basic material on that data structures, classical algorithms like sorting, algorithms for dynamic programming, or algorithms that use dynamic programming I should say, algorithms for shortest paths, et cetera.
6046 itself, we're going to run this course pretty much off the Stellar website in the sense that that'll be our one-stop shop for getting everything including lecture handouts, problem sets-- turning in your problem sets, et cetera.
And I should mention that this course is being taped for OpenCourseWare, and while it'll take a little bit of time for the videos to be put online, we hope to do that perhaps in clumps before the quizzes that you will have as we have to have in our class.","what is the prerequisite to this course? I don't understand most of it
I was looking at 6.006 and 6.046j....can anyone please tell in which course do they teach about asymptotic notaions, how to calculate them?? Even in 6.006 they assume students know how to find it.
I don't get it, Srinivas sir says that this course is a follow up of 6.006 intro to algorithm I was unable to follow that because that needed knowledge about Data Structures, I didn't have any... please help me, dude."
"Now, if these colors are not presented so nicely, you need to transform them with the function f so that it kind of approximates this intuitive one-hot encoding, right? So this means that, uh, uh, GIN, uh, uh, aggregation GIN type of convolution is composed of two M- uh, uh, two MLPs, one operated on the colors of neighbors, one, um, and then the aggregation is a summation plus some, uh, final MLP that, again, kind of provides the next level one-hot encoding so that when we, again, sum up information from the children at the next level, no information, uh, gets lost.
So you can think of these f's and, uh, f's and phi as some kind of transformation- transform- transformations that kind of softly do uh, one-hot, uh, encoding.
So let's now summarize and provide the entire, uh, GIN model.
Uh, GIN, uh, node-embedding update goes as follows.","Thanks for the lecture, it's really amazing how GIN is related to the previous WL kernel. I have a question about the MLP_phi in the GIN layer (P.62). Since the function could be injective relying on MLP_f and is determined by the summation of MLP_f(x), and the final transformation MLP_phi should not change or affect the injectivity, so are MLP_phi necessary?"
"A family of Gaussian distribution such that for any value of the parameter, you get a member of the Gaussian family.
All right.
And this is mostly, uh, to show that, uh, a distribution is in the exponential family.
Um, the most straightforward way to do it is to write out the PDF of the distribution in a form that you know, and just do some algebraic massaging to bring it into this form, right? And then you do a pattern match to, to and, and, you know, conclude that it's a member of the exponential family.
So let's do it for a couple of examples.
So, uh, we have [NOISE].
So, uh, a Bernoulli distribution is one you use to, uh, model binary data.
Right.
And it has a parameter, uh, let's call it Phi, which is, you know, the probability of the event happening or not.","Bernoulli distribution is for discrete random variable, PMF is defined for Discrete random variable"
"So maybe I ought to modify this gold star idea before I get too far downstream.
And we're not going to treat everybody in a crowd equally.
We're going to wait some of the opinions more than others.
And by the way, they're all going to make errors in different parts of the space.
So maybe it's not the wisdom of even a weighted crowd, but a crowd of experts.
Each of which is good at different parts of the space.
So anyhow, we've got this formula, and there are a few things that one can say turn out.
But first, let's write down the an algorithm for what this ought to look like.
Before I run out of space, I think I'll exploit the right hand board here, and put the overall algorithm right here.
So we're going to start out by letting of all the weights at time 1 be equal to 1 over n.
That's just saying that they're all equal in the beginning, and they're equal to 1 over n.
And n is the number of samples.
And then, when I've got that, I want to compute alpha, somehow.
Let's see.
No, I don't want to do that.
I want to I want to pick a classifier the minimizes the error rate.
And then m, i, zes, error at time t.
And that's going to be at time t.
And we're going to come back in here.
That's why we put a step index in there.
So once we've picked a classifier that produces an error rate, then we can use the error rate to determine the alpha.","it looks like the volume around correct classified points could be computed and that volume takes vast amount of the total volume. Hence the algorithm not overfitting. How to compute volume arount error classified points when all points are classified correctly?
Excellent in every way. Just one question: I tried to implement this simple version but what I find strange is that some of my alphas are negative because that happens when the error is greater or equal to 0.5 but if that happens we dont have a weak learner right? So whats the deal with this case? I noticed that in the demo some of the alphas were negative too. How can I deal with this case? I would appreciate answers. Thanks for the great lecture and making that amazing knowledge available to the world!"
"ENDRA PUGH: Hi.
Today, I'd like to talk to you about circuits.
Last time, we finished up the LTIs, and signals, and systems, where we learned how to both model existing systems and predict their long-term behavior.
But we haven't forayed into how to actually create systems in the physical world.
We've created some amount of systems in software and made some brains for our robots.
But if we want to make something in the physical world, then we probably have to come up with ways to model physical systems or use physical components.
That starts our new model on circuits.
Circuits are going to be our first foray into designing systems in the physical world, also designing systems using physical components.
It's worth mentioning now that the information that you learn about circuits is good for more things than even circuits.
You can use basic circuit diagrams and properties of circuits to model all sorts of kinds of systems, especially ones in the human body-- circulatory system, neurological system, different kinds of fluid flow, that kind of thing.
In the next few videos, we'll go over how to represent circuits, and also cover some of the basic methods by which people solve circuits.","if the output of voltage of RL circuit is taken to be the voltage across the inductor, the circuit is said to be what
How would i2 relate to it being correlated to R3 being over the sum and for i3 visa versa? Why can you not use i2 with R2 over the sum and so on?
What would you use, Maxwell equation within time or frequency domain?"
"As you can read is r to the n plus 1 minus 1 is the numerator and r minus 1 is the denominator.
And the claim is that this identity holds for all non-negative integers n and for all real numbers r that aren't 1 because I don't want the denominator to be 0.
So how are we going to prove this? Well, I'm going to prove it by using the well ordering principle, and let's suppose that this identity didn't hold for some non-negative integer n.
So we'll apply the well ordering principle and we'll let m be the smallest number n where this equality fails-- it becomes an inequality.",Isn't the right hand side of the geometric sum formula = (1- r^n+1)/(1-r)? Not (r^n+1-1)/(r-1)?
"So let me just say a couple more things about logistics, and then we get started with technical content.
As I mentioned, we're going to be running this course off Stellar.
Please sign up for recitations section by going to the stellar website and choosing a section that works for your schedule.
Sections go from 10:00 AM all the way to 3:00 I think, and we've placed a limit on the number of students per section.
We wanted the sections to be manageable in size, but there's plenty of room for everybody, and the schedule flexibility should allows you to choose a section pretty easily.
We have a course information document and an objectives document on the website.
That has a lot of details on the grading policy, the collaboration policy, et cetera.
Please read it very carefully from the first page all the way to the end.
And I will mention one thing that you should be careful about, which is that while problem sets are only 30% of the grade, we do require you to attempt the problems.
And there's actually a penalty associated with not attempting problems and not tuning problem sets in that is way more than 30%, so keep that in mind, and please read the collaboration policy as well as the grading policy, carefully.
And feel free to ask us questions.
You can ask us questions anonymously through Piazza, or you can certainly send us email.
All the information is on Stellar.
So that's all I really had to say about course logistics.
Let me tell you a little bit about how the content of this course is structured.",what is the prerequisite to this course? I don't understand most of it
"And this is the output.
And I get d-bits out.
And I've got x1, x2, to xn, OK? Now I've given this h-- I've been given this h which is one-way and TCR.
It satisfies those properties that you have up there.
In the case of one-way, I give you an arbitrary d-bit string.
You can't go backwards and find a bunch of the xi's that produce exactly that d-bit string, all right? So it's going to be hard to get here.
But you're allowed now to give me an example.
So this is some hash function that you can create, which may use h as well.
And h is kind of nice because it has this one-way property.
So let's say that we want to discover something where one-way does not imply TCR.
So I want to cook up a hash function h prime such that h prime is one-way, but it's not TCR, OK? The way you want to think about this is you want to add to h.
And you want to add something to h such that it's still hard-- if you add h it's still hard to go from here to there.","While the sets of OW and TCR are not necessarily contained in each other, I'm not sure about the intersection of R and OW not being contained in TCR. It looks to me that given an h that is both random and non inversible we shouldn't be able to produce an x' to a given x, such that h(x') = h(x). Being able to find such an x' = f(x) would mean that either there were some structure on X that was passed on to h, or h induced such a structure.
Consider simpler examples to get the idea: If h(x) =x^2 (square of x) Clearly h(x) is not OW... x=sqrt( h(x) ) However, there is no collisions CR & TCR is satisfied . -A reverse example, let H(x) = XOR(xis) {I mean some kind of XORing of different bits of x in some manner You see here the reason that makes H(x) OW (hard to retrieve x after the mangling) is almost the same reason it is NOT CR & NOT TCR (It's not that we can't find an x given h(x), it is there r so many valid Xs) . and conceptually, there r applications that require OW not TCR (like storing pswds hashes to not reveal them), and other applications that cares more about TCR not OW (like file signatures where the file is not a secret, u care only that it has not been modified by an opponent or a virus for example by storing its hash... here ur worry is that they can modify F to F' where h(F) =h(F')...note that even if F' doesn't make sense and u won't be deceived by it the original F will be lost)"
"They will all work.
Or you could use Jason's trolling answer of size plus a constant, like 5.
Why is this bad? Yeah? AUDIENCE: [INAUDIBLE] ERIK DEMAINE: You'll have to do it again.
You'll have to resize frequently.
When? Five steps later.
In the original static array, we were reallocating every single time.
That's like n plus 1.
If we do n plus 5, that really doesn't change things if we ignore constant factors.
Now, we'll have to spend linear time every five steps instead of linear time every one step.
That's still linear time per operation, just, we're changing the constant factor.","I expect to hear how to address more RAM but he said ""you can buy more RAM"". Thanks for the entire trash talk. Instead of constantly typing on the chalkboard you could maybe tell us ""why"" is an array problematic, and wtaf is a word-bit, your invention? What real life scenario is a linked-list good for?
And no build() is linear time not constant"
"We also saw that the GIN is closely related to the WL kernel and that both GIN and WL kernel can distinguish, uh, most of the real-world, uh, graph structures.
To summarize, the- the- the important point of the lecture is that if you say about mean and max pooling, for example, mean and max pooling are not able to distinguish these types of neighborhood structures where you have two or three neighbors, all the same features.
Here is where maximum pooling fails because the number of distinct- k- kind of the distinct colors are the same.
So whatever is the maximum is the same in both cases and this is, again, the case where both mean and max pooling fail because we have, uh, um, green and red and they are in the same proportion.
So if you rank, uh, different pooling operators by- by discriminative power, um, sum pooling is the best, is most expressive, is more expressive than mean pooling, is more expressive than, uh, maximum pooling.
So in general, sum pooling, uh, is the most expressive, uh, to use in graph neural networks.
And last thing I want to mention is that you can further improve the expressive power of graph neural networks.
So the important characteristic of what we talked today was that node features are indistinguishable, meaning that all nodes have the same node feature information.
So by adding rich features, nodes may become, um, distinguishable.
The other important thing that we talked about today is that because graph neural networks only aggregate features and they use no reference point in the network.","Thanks for the lecture, it's really amazing how GIN is related to the previous WL kernel. I have a question about the MLP_phi in the GIN layer (P.62). Since the function could be injective relying on MLP_f and is determined by the summation of MLP_f(x), and the final transformation MLP_phi should not change or affect the injectivity, so are MLP_phi necessary?"
"OK, so this gives me a sense.
The next thing I'll get is some statistics.
So we know the mean is 16.3 and the standard deviation is approximately 9.4 degrees.
So if you look at it, you can believe that.
Well, here's a histogram of one random sample of size 100.
Looks pretty different, as you might expect.
Its standard deviation is 10.4, its mean 17.7.
So even though the figures look a little different, in fact, the means and standard deviations are pretty similar.
If we look at the population mean and the sample mean-- and I'll try and be careful to use those terms-- they're not the same.","I am a bit surprised he didn't comment on how the standard deviation of the sample has a bias as an estimator for the standard deviation of the population, and how you should divide by (n-1) instead of n (n being the sample size) when doing this estimation...
It would be useful to carefully explain the difference between sample (a random draw of size n from the population), individual sample elements ( each member of the sample), and replications (number of samples drawn). Otherwise it could be easy to confuse which one your talking about."
"And I'm going to stop there because this is an admissible heuristic and that's not good enough unless it's a map.
It's not good enough for this particular case because this is not geometric.
This cannot be done as a map on a plane.
So that's a situation where what I've talked to you about, so far, works with branch-and-bound.
Works with branch-and -bound plus an extended list.
But doesn't work when we added an admissible heuristic.
So if we're going to do this in general, we need something stronger than admissibility, which works only on maps.
And so the flourish that I'll tell you about here in the last few seconds of today's lecture is to add a refinement as follows.
So far, we've got admissibility.
And if we want to write this down in a kind of mathematical notation, we could say that it's admissible if the estimated distance between any node X and the goal is less than or equal to the actual distance between X and the goal.","I think this consistency matter is there whenever we don't extend the same node twice -Sure it is true what he said, but I think he should have mentioned/clarified that whenever it is not a map (the distance eq AB+BC>=AC is not necessarily true) We should check not just whether we've extended this node before or not, but with what cost value???? I mean even if we are not using heuristics this could miss a shorter path (i. e. for example it could be yes we have extended C before but with a longer path, if SC=11 while SA->AC=5+2=7) . Or am I missing something here????
So when we use admissible heuristic algorithm, how do we know, that the path is actually the shortest? As was said in the previous lecture, heuristics usually work, but not necessarily always. We can think of a simple case when stepping away from the goal would be a better solution, even though the heuristic tells us otherwise (in that case path weights won't correspond to geometrical rules, but that's usually the case in real life problems). But if we stop when potential distances are higher when the one we found, it wouldn't be the optimal path. So this basically means that there are limitations on the heuristic we use, as it should guarantee that the ""airplane distance"" that we get is definitely lower than an actual distance
The last example is not a ""Map"" but if I change the ""100"" value to ""2"" that is ""Admisible and Consistent"", but it is not a ""Map"" still?
Well, if you use the definition of consistent heuristic found on wikipedia, changing 100 by 2 will not make it either way. I thought these two definitions of consistency were equivalent but it seems they are not. Any ideas why or am I overlooking something?
At the end he shows the example where A* doesnt work. Wouldn't you just make it so that if you find a shorter path to a node that already been covered you swap it in?
Yea, I was wondering the same. But, does this new rule guarantee that you can always find the optimal path with only admissibility being assumed?
when he is talking about consistency he says if the distance to the goal from A is 2 instead of 100 it will be consistent but I doubt it since according to formula 2-0 < 1 is not correct what are your opinions?
Can anyone explain to me what real world example could be tied to his last in class example? I'm trying to think when he would ever have to use the A* algorithm on something thats not a map lol
I believe the impossibility is in setting H(C)=0, as this means that the direct distance from C to G is 0 which contradicts the fact that the actual distance is 100, as there is a direct road connecting these nodes.
what specifically does Chomsky say about this?
What is the name of that kind of distribution"
"OK so far? There's a bunch of lemmas.
Now we actually get to do algorithms using these lemmas.
We'll start with maybe the less obvious algorithm, but it's nice because it's very much like Dijkstra.
It follows very closely to the Dijkstra model.
And then we'll get to the one that we've all been thinking about, which was choose a minimum weight edge, contract, and repeat.
That doesn't-- well, that does work, but the obvious way is, maybe, slow.
We want to do it in near linear time.
Let's start with the Dijkstra-like algorithm.
This is Prim's algorithm.
Maybe I'll start by writing down the algorithm.
It's a little long.
In general, the idea-- we want to apply this greedy choice property.
To apply the greedy choice property, you need to choose a cut.","Thank You. Sir, you are an amazing teacher. Thank you for your videos, they are so clear and easy to understand. However, I'm a little confused still about the running time of Prim's. Would anyone be able to explain how you can decrease a key in a Fibonacci heap in constant time? If you were implementing prim's with a regular min-heap, would the running time change from O(Vlog(V) + E) to O(Vlog(V) + Elog(V)) to reflect the slower decrease key operation? (normal heaps, as I understand, take log(n) time to decrease the value of a key, correct?)
What an excellent teacher. However, I'm a little confused still about the running time of Prim's. Would anyone be able to explain how you can decrease a key in a fibonacci heap in constant time? If you were implementing prim's with a regular min-heap, would the running time change from O(Vlog(V) + E) to O(Vlog(V) + Elog(V)) to reflect the slower decrease key operation? (normal heaps, as I understand, take log(n) time to decrease the value of a key, correct?)
Inorder to understand why/how Prim's works you need to see the before part of this video as well."
"And that's x sub i.
Those are the quantities I need in order to do it.
So that means that if I have a function, let's call it k of x sub i and x sub j, that's equal to phi of x sub i dotted with phi of x sub j.
Then, I'm done.
This is what I need.
I don't actually need this.
All I need is that function, k, which happens to be called a kernel function, which provides me with the dot product of those two vectors in another space.
I don't have to know the transformation into the other space.
And that's the reason that this stuff is a miracle.
So what are some of the kernels that are popular? One is the linear kernel that says that u dotted with v plus 1 to the n-th is such a kernel, because it's got u in it and v in it, the two vectors.","is syscall an intrerrupt or does it invoke the shell
How did they come up with the kernels?
I am still confused about how to determine a support vectors. Could anyone recommend me some links about it?
Still hard to understand the L part and the kernel part any one can help to give some more good resource for this? Thanks
Very good introduction to the SVM (I've honestly been assuming it was esentially a slightly modified LDA classifier for months now!!), but I'm still not sure how to optimise it or select a kernel. Any ideas where to look?
Great lecture but why does the kernel trick work? Is there a guarantee that it would work?
Maybe I am missing some thing but it seems to me that the utility of this technique requires that you have a transformation that pushes non-linear data onto another dimension. How are these transformations found, is this by trial and error. If so then isn't this still a hill climbing exercise
First of all you don't seek transformations, you directly seek a function that gives you the dot product of 2 transformed points. So you can easily have an insane amount of dimensions which increases the likelyhood that the data is linearily seperable. as far as I understand you can also have infinitely many dimensions. If your transformation has infinitely many dimensions (fore example f(x) = [x, x, x...] the dot product of f(x) and f(y) could be a converging infinite sum! So this kernel allows you to have a lot of dimensions which seem to practically guarantee linear seperability
Question.. w = sum over i ( y_i * a_i * x_i ) => weights are linear combination of the samples. When prediction is perfomed, using linear svm, thanks to the linearity of the dot products, we do not need to perform the inner product among x_test with all the x_i on the ""gutt"" of the wider street, in fact sum_over_i (y_i * a_i * x_i * x_test) = sum_over_i (y_i * a_i * x_i) * x_test. We need just to precompute the first term. However, what if non linear kernel functions K(x_i,x_test) is involved, for instance gaussian kernel exp(-(|| x - y ||^2/sigma)? For prediction, I need to do product scalar with all the samples in the training set that are on the gutt of the wider street?"
"When computing the kernel value, many colors, uh, appeared in two graphs need to be tracked.
So the number of colors will be at most the number of nodes, uh, in the network.
So this again won't be too- too large.
And counting the colors again takes linear time because it's just a sweep over the nodes.
So the- the total complexity, uh, of computing the Weisfeiler-Lehman graph kernel between a pair of, uh, graphs is simply linear in the number of edges in the two graphs.","positive semi-definite means the eigenvalue is nonnegative, not just positive
where there are 7 g_2 or 6 g_3? If we follow this numbers for nodes: 2 4 1 3 5 Then g2 are: 245, 354, 235, 324, 124, 135 and g3 are: 125, 134, 145 So, the answer I see is (1,6,3,0)"
"So what this shows is that, um, if you say how many graphlets are there on five nodes, uh, there is, uh, 73 of them, right? Labeled from one, uh, all the way to 73 because it's different graphs as well as, uh, positions in these graphs.
So now that we know what the graphlets are, we can define what is called Graphlet Degree Vector, which is basically a graphlet-base, uh, based features, uh, for nodes.
And graphlet degree counts, um, the number of times a given graphlet appears, uh, uh, rooted at that given node.
So the way you can think of this is, degree counts, the number of edges that the node touches, uh, clustering coefficient counts the number of triangles that are node touches or participates in and, uh, graphlet degree vector counts the number of graphlets that- that a node, uh, participates in.
So to give you an example, um, uh, a Graphlet Degree Vector is then simply a count vector of graphlets rooted at that given node.","In the previous video, while describing Graph Neural Net, mentioned there is no feature enginering, here you are telling about featues. It is confusing.
I have a doubt about GDV at Could you provide more clarification about when you are explaning about counting connected nodes, Why is every single node to node connection considered as a graphlet instance associated with (a) and in particular this question arises for me because when it comes to counting graphlet instance of (d) you are counting the instances even when there are links connecting to other nodes are present. Why does this difference occur ?"
"There is no y.
And the goal with autoencoders is to learn a way in which we introduce something called as a bottleneck and reconstruct the original data.
What I mean by that is we start with the original-- with the input layer being x, right, and this used to be d-dimensional, right? And this will still be d-dimensional.
So this is the input layer.
And then we have a few fully connected layers, right, and we bring it down to some k-dimensional hidden layer.
Let's call it z, right? And from this k-dimensional hidden layer, we start where typically k is smaller than d.","Hi sir, could you please give more details on the relation between Factor Analysis and Auto-encoder? When learning Factor Analysis, I would thought that both of them are relatively the same. In Factor Analysis, if we are given a set of examples x, then we can construct the continuous latent z. In order to reconstruct back to x, we can simply apply formula: x_hat (\approx) mu + sigma * z + error. But I think maybe the difference(L2-norm) between (x and x_hat) is not smaller than when using Auto-encoder instead."
"Just 'b0' in fact.
What that says is plot blue circles.
I could have written-- in fact I did write 'k+' and that says black plus signs.
And I don't actually remember what I did to get the triangles, but it's in the code.
And so that's very flexible what you do.
And as you can see here, we get the insight I'd communicated earlier that if you look at east and west, not much difference between this ball and that ball.
They seem to be moving about the same spread, the same outliers.
But this ball is displaced north.
And not surprisingly, after 10,000 steps, you would not expect any of these points to be below zero, where you'd expect roughly half of these points to be below zero.
And indeed that's about true.
And we see here what's going on that if we look at the mean absolute difference in x and y, we see that not a huge difference between the usual drunk and the masochistic drunk.
There happens to be a distance.
But a huge difference-- sorry, x and x.
Comparing the two y values, there's a big difference, as you see here.","This is a very misleading class in my humild opinion, because he is computing the average of distances and not the expected value of the random walk. The random walk should be a bell curve with its peak at distance zero, so the expected value of the walk is always zero for the Usual Drunk. What happens is that when the number of steps increases the bell curve becomes wider and you have small probabilities of finding bigger distances, hence the 'mean' distance increases a little bit. Hower the expected distance to the origin is still zero.
If someone could help me, in the textbook there's another exemple of drunk: the EW Drunk, moving only in the horizontal axe (-1, 0) and (1, 0). However this drunk is also getting farther away from the origin. But why? If after n number of steps he has equal chance to step etiher W or E, wasn't he supposed to be back to the origin according to the law of big numbers? Isn't it the same as to flip n coins and count number of head and tails?
but why is the expected distance to the origin zero? for a point that is 1 step away from the origin, there is 3/4 chance for the second step to be even further away from the origin. So the distance will eventually get bigger and bigger.
I did not have run the simmulation because the result is presented in the textbook. The professor is computing the average of distances and not the expected value of the random walk. The random walk should be a bell curve with its peak at distance zero, so the expected value of the walk is always zero for the EW. What happens is that when the number of steps increases the bell curve becomes wider and you have small probabilities of finding bigger distances, hence the 'mean' distance increases a little bit. Hower the expected distance to the origin is still zero. Kind of misleading IMO but it is correct.
When talking about distances the sign is not relevant. Say you have two trials with one step each and lets only allow movements in x. Asumme the first trial ends at -1 and the secon one at +1. The mean of covered distance is then one while average distance from origin is zero. Its just tow ways to look at the probem: The expected value of distance from origin is 0 while the average distnace covered is not."
"Right? If the derivative says it's negative, then just go this way.
And now you're on a new point, you compute the derivative again, you descend, and now you compute it again.
And then maybe you compute the derivative and it says keep on going this way and maybe you overshoot, and then you come back.
And then, you know, hopefully you'll end up at the minimum.
Okay.
So let's try to see what this looks like in code.
So gradient descent is one of the simplest algorithms, but it really underlies essentially all the algorithms that you people use in machine learning.
So let's do points.
We have two points here.
Um, and I'm going to define, um, some functions.
Okay, so f of w, so what is this function? So I'm going to sum over all the different, um, you know, and basically at this point it's converting math into Python.","We use cache after we do all computing ( after ""result = min(subCost, delCost, insCost)"" ) so how does it benefit to us?
You check the cache FIRST before running all the computation ""if (m,n) in cache => return cache(m,n)"" lines at the top before everything else. So basically if the result is already in the cache then there is no need to run 3 computations again, just return the result"
"And the way its aggregation function looks like it says, let's take messages from the children, let's transform them with a multi-layer perceptron, let's sum them up and apply another, uh, multi-layer, uh, perceptron.
And, you know, uh, given everything I explained, this is, uh, injective multi-set aggregation function, so it means it has no failure cases.
It doesn't have any collisions, and this is the most expressive graph neural network in the class of this message passing, uh, graph neural networks.
So, uh, it is super cool that we were basically able to define the most powerful graph neural network, um, out of, uh, an entire class, uh, of graph neural networks.
And we now theoretically understand that it is really all about the aggregation function, and that the summation aggregation function is better than the average, is better than the maximum.
So, uh, let me, uh, summarize a bit, right? We have described neighborhood aggregation fun- uh, uh, of, uh, function of a GIN, um, and, uh, we see that basically the aggregation is a summation.
We take messages, we transform them through an MLP and then sum them up.
And that has the injective property, which means it will be able to capture the structure of the entire computation graph.
Now, uh, that we have seen what GIN is, we are going to describe the full, uh, model of the Graph Isomorphism Network, and we are actually going to relate it back to the, uh, Weisfeiler-Lehman graph kernel, the WL graph kernel that we talked about, I think in lecture number 2.","Thanks for the lecture, it's really amazing how GIN is related to the previous WL kernel. I have a question about the MLP_phi in the GIN layer (P.62). Since the function could be injective relying on MLP_f and is determined by the summation of MLP_f(x), and the final transformation MLP_phi should not change or affect the injectivity, so are MLP_phi necessary?"
"But for the other parameters, where the expectation depends on phi itself, we make use of this reparameterization trick, where, with the reparameterization trick, we can rewrite the expectation from in terms of z to in terms of epsilon that has no parameters.
Yes, question? You sample epsilon, or-- Yes, I'm going to come to that, right? So, now, we've replaced the expectation with respect to z with expectation with respect to epsilon, right? So the question is now, piecing this all together-- over here, what we can see is we still have, even though we were able to swap the gradient and the expectation, we are still left with the expectations, right? So the gradients still have an expectation from left integer which, in general, can be a problem.
But what is done in practice is that this expectation is approximated with Monte Carlo, which means we take a sample zi from Qi and construct and calculate the-- and use that as the input for this encoder neural network to perform backpropagation.
And backpropagation is performed with respect to theta.
And the output of the neural network is trained to be xi's, right? So you can think of this as the-- sorry, this should be the decoder.
I'm sorry about that.
This is the P of z given x, so this should be the decoder.
And then the decoder, we are fed the input zi, and the output should be xi.
And the xi over here-- instead of g, let's give it a different name.
So what does the notes use for g? So, all right, so the note uses g for decoder.","Professor, can I use gradient decent to solve GMM or factor analysis? Thanks!"
"So I'm going to show you code that implements exactly this algorithm.
And while you see the code, I want you to think about this harder question.
This code is going to do exactly what we describe.
It's going to go compute all of these things, and as you can imagine, he's going to make a pass through this entire Python list, this group of people, this queue of people, and is going to do this computation.
Then it's going to say, 4 is greater than 3.
And then it's going to go through and start calling out commands for each of the backward intervals.
So in some sense, it's going to make two passes over the list.
So the first pass is to get all of the intervals together.
Then there's a check.
And then the second pass is to take those backward intervals, in this case, and call out those commands.
So that's what I mean by two passes.
Yeah, please.
AUDIENCE: You don't necessarily need to count them, the 4 and 3 because whichever orientation interval comes second is always going-- SRINI DEVADAS: Brilliant.
Brilliant.
So I was going to say-- if you didn't understand what Ganatra said, wonderful because then you can think about the question I'm going to ask and ignore what he said.
And then we'll get to, I guess, the more efficient code.
But what I was going to ask was, is there a way that I'm looking at this, and I'm just going-- and I'm going to just start calling out by-- the moment I see that there's an interval, the moment I see-- this is an interval.
And I'm going to make a call with respect to whether I'm going to call out a command or not.
And I'm going to do this in one pass through this array.","do we actually need two if checks?
What about the case where there is FBBBBBB Would it still be efficient to not tell the first person to flip their cap?
Very poor explanation on the last optimal algorithm, ABABABA - here skipping first intervals of first A-label gives us the best result. ABABAB - here also skipping first intervals of first A-label gives us the best result. The above 2 configurations are the whole possibilities
Can you give a brief example of what you mean? How the array is populated and the size don't matter. Here are break downs of the simplest cases that illustrate all the possibilities: 1) F..F or B..B, i.e. they are all the same so no commands are needed 2) F..FB..B or B..BF..F, i.e. the pattern ends with the opposite of the state so the same number of flip commands are need for either case 3)F..FB..BF..F or B..BF..FB..B, i.e. the pattern ends with the same orientation as the start. In this case there will be one fewer commands needed to flip the second orientation. In any problem where they are not already all in the same orientation it will either be case 2 or 3 so flipping the second orientation will always require the least number of commands. Starting, you don't need to know the number of commands that will be needed since you know by this procedure you will take the minimum."
"That's a primitive data item.
That's an integer.
This person wants me to understand an integer.
And so it will echo 2, indicating that it thinks you want it to understand a simple integer.
Similarly, if you type 5.7, it says, oh, I got that.
That's a float.
The person wants me to remember a floating point number.
And it will similarly echo the float.
Now, of course, there's no exact representation for floats, right? There's too many of them, right? There's a lot of them.
There's even more floats than there are ints, right? So it has an approximation.
So it will print its approximation to the float that it thinks you are interested in.","At ~44;00 when he is talking about lists and [-1] being the last, how does he arrive at 8 instead of 7? Isn't 7 the first item produced by y[-1][1]? Thanks!
I might be late but the last example... y[-1][1], and it gives 8. Shouldnt it be 7? Because if -1 represents the last list within that list, and youre asking for the 1st element (or number) within THAT list, you would see 7. So what? Also, why is -1 used to represent that element. Shouldnt it be 3 since its the 3rd list within that list
why does y[-1][1] get 8?"
"We'll see that, if the examples are well separated, this is easy to do, and it's great.
But in some cases, it's going to be more complicated because some of the examples may be very close to one another.
And that's going to raise a problem that you saw last lecture.
I want to avoid overfitting.
I don't want to create a really complicated surface to separate things.
And so we may have to tolerate a few incorrectly labeled things, if we can't pull it out.
And as you already figured out, in this case, with the labeled data, there's the best fitting line right there.
Anybody over 280 pounds is going to be a great lineman.
Anybody under 280 pounds is more likely to be a receiver.",Th concept is understandable but how to make the right algorithm and set the data . Im new to this . Anyone cares enough to guide me through this ?
"[NOISE] I wanna present a second set of intuitions and this one will be easier if you're good at visualizing high dimensional spaces I guess.
But uh, let me just give intuition number two which is um let's see.
So um, so first of all let's take our example just now right? Let's say that the classifier uses this, 2, 1 [NOISE] X minus 2, right? So this is W and this is B.
Then it turns out that the decision boundary is this where this is 1 and this is uh, 2 and it turns out that the vector W is always at 90 degrees to the decision boundary right? This is a factor of I guess geometry or something or linear algebra, right? Where as the vector W 2, 1.","Question for the kind hearted people who understood: in the geometric margin, y is either 1 or -1 right? But wasnt it mentioned before that y is either 1 or 0? I got a little messed up on that.
Is it really hard to prove that w is a linear combination of the training samples? The training samples represent some set of vectors in a vector space. They might not span all of the space, and in fact they can't span more than N where N is the number of training samples (and they might span less, if there's any linear dependence among them). But they span some subspace of dimension M. The vector w defines a hyperplane that divides that space into classes. Well, the training samples span that subspace, and therefore any vector in that subspace is a linear combination of them. That leaves the non-spanned dimensions to consider, but your training sample conveys no information whatsoever about those, so there's nothing to do about them. I hope linear algebra was a prerequisite for this class."
"Uh, fourth step is marginalize out anything that's [NOISE] disconnected.
Uh, nothing's disconnected.
So I can't do anything.
And last, I have to do actual work.
Okay.
So what does actual work mean here? I'm interested in the probability of B.
So I need a marginalize out E.
Now, I have to do this kind of a hard way, um, based on last time, uh, last lecture.
So, um, what I'm gonna do here is, you know, what happens when I marginalize out E? I create a new factor.
Let, let me actually replicate this down here, so it doesn't get too confusing.
Um, so I'll create a new factor, and this new factor that's called f of b, um, which is the Markov [NOISE] negative E, there's only one other variable B.
And this is going to be the product of all the factors here that touched this variable that I'm marginalizing out.
And the only difference between this and what we are doing last time is before we had a max, because we're doing maximum weight assignments, [NOISE] and here, I'm going to have a sum because we're doing probabilities in marginalizing.",Please can someone tell me the calculation of getting P(B|A)=0.51? What tool is prof. using here?
"Unfortunately, all of these are solid blocks, so Mario can't actually get up to here to get the star.
But as long as Mario can visit this question mark or this question mark or this question mark, then there will be at least one star up here.
So the idea is that each of these represents one of the literals that's in the clause.
And if we choose-- so let's look at this first clause, x1 or x3 or x6 bar.
So if we choose x1 to be true, then we'll follow the path and we'll be able to hit the star.
Or if we choose x3 to be true, then we'll come in here and hit this star.","can someone explain the mario and clause reduction to me, or provide a link for theory."
"And so for each of the categories of knowledge, there's a way it gets represented.
How is it used? Straightforward, transformations are used to make the problem simpler.
The table is used to trim off and to serve as the bottom of the tree.
Those are the ways in which the knowledge is used.
And then there's the question of course of, how much knowledge is required.
Something that's useful to know if it's late at night, you have 2 finals the next day, and you're not sure which course you should study.
So how much knowledge might you suppose was actually in this program? I've shown you a glimpse of the kind of knowledge that's involved in the program.
I've answered a little bit of question 5, what exactly.
But how much knowledge was involved.
You might be surprised by the answer.
First of all, the table of integrals.
I've listed only 3 things there.
There are lots of other things you can think of, like integral of e to the x is e to the x.","I studied computer science but I never was a math guy. Can someone please explain me why this lectures of him matters for understanding ai?
Out of curiosity: What level of mathematics should I have before watching these videos. I have never taken a calculus course, so I don't know why those transformations work. Do I need calculus, or can I get by without it? Or can I learn a certain calculus subjects without having to go into calculus in depth?
how is average depth of tree = 3"
"So, first of all let's define what a horizon is.
A horizon is just the number of time steps in an episode.
So, it's sort of like how long the agent is acting for or how long it, how long this process is going on for it and it could be infinite.
So, if it's not infinite, then we call it a finite Markov Decision Process.
We talked about those briefly last time.
Um, but it often we think about the case where, um, an agent might be acting forever or this process might be going on forever.
There's no termination of it.
The stock market is up today.
It'll be up tomorrow.
We expect it to be up for a long time.
We're not necessarily tried to think about evaluating it over a short time period.
One might wanna think about evaluating it over a very long time period.
So, we've done this.
The definition of a return is just the discounted sum of rewards you get from the current time step to a horizon and that horizon could be infinite.
So, a return just says, if I start off in time step T, what is the immediate reward I get and then I transition maybe to a new state and then I weigh that return reward by Gamma.
And then I transitioned again and I weigh that one by Gamma squared, et cetera.
And then the definition of a value function is just the expected return.",How return function is different from value function ? How come return will be different from value function when process is not stochastic .( both having sum of reward )
"When I start out I have n coefficients, I have n different positions I want to evaluate them at because that's what I want to do to do this conversion from coefficients to samples.
So at the root of the recursion tree I'm just going to write n.
Order n work to get started and to do the recursions.
There are two recursive calls.
One has som-- both have size n over 2 in terms of a, and they have the same x-- which is also known as n-- in those two recursions.
So in fact, the linear work here will be n and n, and then we'll get n and n.
x never goes down, so x always remains n-- the original value of n.","should not that be O(n^3) as we have total work = n + 2n + 4n + ...+ n*n = n ( 1 + 2 ...+n) = n*n*(n+1)/2 = O(n^3), he says it is O(n^2)
is there a mistake ? we know V.A = Y ( V - vandermond matrix, A - coefficien matrix, Y - samples matrix) (multiplying by V inverse i.e. V^(-1) both sides) => V^(-1).V.A = V^(-1).Y => A = V^(-1).Y So to go from samples matrix to coefficient matrix we need to do V^(-1).Y right ??"
"I think today is a wonderful time to jump into machine learning, uh, and, and, and the number of- and the opportunities for you to do unique things that no one has- no one else is doing, right? The opportunity for you to go to a logistics company and find that exciting way to apply machine learning, uh, will be very high because chances are that logistic company has no one else even working on this.
Because, you know, they probably can't- they, they may not be able to hire a fantastic Stanford student that's a graduate of CS229, right? Because there just aren't a lot of CS229 graduates around.",is this for beginners who are starting to learn machine learning?
"If I did get_at at 3 down here, I would get the value that used to be X 2.
That's maybe hard to track.
But this is a conceptually very useful thing to do, especially when you're inserting or deleting at the ends.
So we're going to define, in particular, insert and delete first and last.
These are sometimes given-- if you have an insert, it has an x.
If you do a delete, it has no argument.
This means insert_at the beginning of the array, which would be like adding it here.
And insert_last means adding it on here.
insert_last doesn't change the indices of any of the old items.","Insert last in static array? Why O(n)?
Why is the time complexity for linked list for insert_last(x) and delete_last(x) linear? Shouldn't it be constant since we are able to access the tail?
So great lecture in really giving you why and how, not just a bunch of hows. One question: delete_last() of array seems to me to only take O(1) constant time if choose to do so. I understand insert_last(x) would take O(n) time as a new array has to be created and copy all old elements (and inserted x) to the new array. But deleting the last one would still maintain the old array untouched for the first n-1 elements, and what only needs to be done is to update the len(static array) now to be n-1. Do I miss anything?"
"So at system initialization time or whatever, TaintDroid looks at all these sources of potentially tainted information, and essentially assigns a flag to each one of these things.
So things like your GPS, your camera, and so on and so forth.
As the program executes, it's going to pull out sensitive information from these sensitive sources, and then as that kind of thing happens, the interpreter is going to look at all these types of op codes here and basically follow those policy rules in the table on the paper, and figure out how to propagate taint through the system.",I'm not sure if the purpose of TaintDroid is to protect users from malicious apps or help the developer to protect his users.
"Right.
And so the- the key steps are, you know, one, make an assumption about P of Y given X, P of Y given X parameters theta, and then second is figure out maximum likelihood estimation.
So I'd like to take this framework and apply it to a different type of problem, where the value of Y is now either 0 or 1.
So is a classification problem.
Okay? So, um, let's see.
So the classification problem.
In our first classification problem, we're going to start with binary classification.
So the value of Y is either 0 or 1.
And sometimes we call this binary classification because there are two clauses.
Classification.
Right.
Um, and so right- so that's a data set where I guess this is X and this is Y.
Um, so something that's not a good idea is to apply linear regression to this data set.
Some- sometimes you will do it and maybe you'll get away with it but I wouldn't do it and here's.
Which is, um, is- is tempting to just fit a straight line to this data and then take the straight line and threshold it at 0.5, and then say, oh, if it's above 0.5 round off to 1, if it's below 0.5 round it off to 0.
But it turns out that this, um, is not a good idea, uh, for classification problems.
And- and here's why? Which is- for this data set it's really obvious what the- what the pattern is.",Since when these these basic statistics techniques become machine learning??
"So to, uh, to give an example, consider this particular graph here, and we are interested in the node v.
Then here is a set of graphlets on two and three nodes.
This is our universe of graphlets.
We are only look- going to look at the graphlets all the way up to size of three nodes and node v there.
Um, and then, uh, you know, what are the- what are now the graphlets instances, for example, the- the graphlets of type a, these node v participates in two of them, right? It's- one is here and the other one is there, right? So this means this one and that one.
Then the graphlet of type b node v participates in one of them, right? This is, um, this is the graphlet here, right, uh, b.
And then you say, how about how many graphlets of type d does, uh, node, um, sorry, of type c, does node, uh, v participate in.
And it doesn't participate in any because, uh, here it is, but these two nodes are also connected.","I have a doubt about GDV at Could you provide more clarification about when you are explaning about counting connected nodes, Why is every single node to node connection considered as a graphlet instance associated with (a) and in particular this question arises for me because when it comes to counting graphlet instance of (d) you are counting the instances even when there are links connecting to other nodes are present. Why does this difference occur ?
Yep, need more clarification regarding this matter. In my opinion, instance of (c) should be included, so the GDV will be [2, 1, 1, 4]
I believe that is because, you consider a graphlet in isolation to other nodes in the graph. So in (a), only two nodes are involved, and the links with any other nodes is disregarded. But in (c)/(d), the graphlet is only valid if the two edge nodes are not connected."
"We're going to prove a bunch of other things along the way but crucially what is the i implies j that I want if I want to claim that the Ford-Fulkerson algorithm terminates with the max flow.
Yeah, you over there.
AUDIENCE: 3 implies 2.
PROFESSOR: 3 implies 2.
That's right, 3 implies 2, exactly right.
3 implies 2.
So we need to do 3 implies 2.
Now of course, the theorem says that, 1 implies 2, 2 implies 3, 3 implies 1, et cetera.
There's a lot of implications here.
What we are going to do is we're going to show that 1 implies 2, 2 implies 3, and 3 implies 1.
And that pretty much takes care of everything.
And it turns out the reason we do this is simply because it makes for the simplest proofs.
1 implies 2 and 2 implies 3 are one- liners, and that 3 implies 1 is a little more interesting and involved, but it's a little bit easier than directly doing 3 implies 2.
So that's the way we're going to do this.
You could certainly play around and do other things.
So any questions so far? OK, so let's go ahead and do this.
All right, we should be able to knock these two, 1 implies 2 and 2 implies 3, in just about a minute each.
So I want to show 1 implies 2, and essentially what I want to say here is, if I've saturated a particular cuts capacity then I have a max flow.
And really, I mean this comes from the definitions, since I'm going to have f less than or equal to the c S, T, and this is simply because of edge capacity constraints.","max flow |f| = some c(S,T) cut..... not any I think. some (S,T) cut may have a larger capacity which is okay since all we need id f < any (S,T) cut. here , he just showed that there is always some (S,T) cut out there such that |f| = capacity of that (S,T) cut. did I make sense?"
"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation, or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
JOHN GUTTAG: I'm a little reluctant to say good afternoon, given the weather, but I'll say it anyway.
I guess now we all do know that we live in Boston.
And I should say, I hope none of you were affected too much by the fire yesterday in Cambridge, but that seems to have been a pretty disastrous event for some.
Anyway, here's the reading.
This is a chapter in the book on clustering, a topic that Professor Grimson introduced last week.
And I'm going to try and finish up with respect to this course today, though not with respect to everything there is to know about clustering.
Quickly just reviewing where we were.
We're in the unit of a course on machine learning, and we always follow the same paradigm.
We observe some set of examples, which we call the training data.
We try and infer something about the process that created those examples.
And then we use inference techniques, different kinds of techniques, to make predictions about previously unseen data.
We call that the test data.
As Professor Grimson said, you can think of two broad classes.
Supervised, where we have a set of examples and some label associated with the example-- Democrat, Republican, smart, dumb, whatever you want to associate with them-- and then we try and infer the labels.","Thanks for an amazing lecture! it tries to cluster data into two groups and see if it correctly differentiated people who dies of heart attack and those that didn't. To me this is using clustering for classification task, if yes, when would someone use clustering rather than classification?
When we are clustering the airports, the professor only stopped to think about linkage when he arrived at Denver. Shouldn't we have thought about it since the beginning of the clustering? If so, we could have gotten (BOS, SF) instead of (BOS, NY) for the first iteration using complete linkage.
Can anyone link machine learning to digital signal processing for me?"
"But because it's hard, in practice, to change all the software out there, many people try to devise techniques that make it more difficult to exploit these bugs.
For example, making the stack non-executable, so you can't inject the shell code onto the stack, and you have to do something slightly more elaborate.
And next couple of lectures, next two lectures, actually, we'll look at these defense techniques.
They're not all perfect.
But they do, in practice, make it much more difficult for that hacker to exploit things.
Question? AUDIENCE: I just have a general administrative question.
PROFESSOR: Yeah? AUDIENCE: I was wondering if there was a final? And also if there are quizzes, and what dates-- PROFESSOR: Oh yeah.
Yeah, I think if you go to the schedule page, there's two quizzes.
And there's no final during the final week, but there's a quiz right before it.
So you're free for the final week, but there's still something at the end of the class.
Yeah.
All right.
OK.
So I think that's probably it for buffer overflows.
I guess the one question is, so what do you do about mechanism problems? And the general answer is to probably have fewer mechanisms.",Is this course useful for someone who has not taken any sort of computer science course and has little/no experience in coding? Or is there another lecture series I should start with?
"And we check to see which direction seems to be doing the best job of getting us moving upward.
And that's our Hill Climbing approach, right? We have explored four directions we can go and pick the best one.
And from there, we pick four, try all those, pick the best one, and away we go.
We've got ourselves a Hill Climbing algorithm.
What's wrong with it? Or what can be wrong with it? Sometimes it works just fine.
Yes.
SPEAKER 1: You might get stuck in a local maximum.
PATRICK WINSTON: We might get stuck in a local maximum.
So, problem letter a is that if this is your space, it may look like that.
And you may get stuck on a local maximum.
Is there any other kind of problem that can come up? Well, it all depends on what the space is like.
Here's a problem where the space has local maxima.
Now, a lot of people have been killed on Mt.
Washington when the fog comes up.
And they do freeze to death, why? The reason they freeze to death is the Hill Climbing fails them, and they can't get to the top to the ranger station.","Still i can't understand why in Hill Climbing, when he did the representational tree, why he took B instead of A and why he assume than the larger path, begining by B, is the optimal. I searched for a lot about hill climbing and yet i do not catch that search, please there are another video which i can understand this concept with very detail?
So the hill climbing approach is just similar to the greedy algorithm, right?
How could Hill climbing backtracking??
I don't really get the last problem (the contour problem) of Hill Climbing. Can anyone explain it for me? Thanks in advance.
Hmmm, no because then you would actually have to be on a maximum, just not the biggest one. In that situation you would go lower in any direction you go, because you really are on a top point (just not the highest one). In the contour problem you can actually go higher, the directions in which one must go to go higher is just very limited. If you don't try out all directions you might miss the direction that leads higher, which is the problem. So in the contour problem you could actually go higher (although it might lead to a local maximum), instead of the local maximum problem where you think you reached the real maximum (but you didn't). And no problem! :)
If the hill climbing algorithm can be outfitted with a backtracking feature why then could it ever get stuck at a local maximum, can't it just backup and look for the global maximum?
Why at the Hill Climbing explanation, the professor says that the closest node to the goal is B, in wich he says it has size 6 (why 6?) and node A he says it has size 7+? Isn't A closer to S with 3, and B with 5?"
"Okay.
Last one or two in the layers of M2.
So do we train, what do we fine tune? There's a lot about layers actually.
Size of added layers, not sure.
[LAUGHTER] Okay, let- let's, let's go over it together because it seems that there's a lot of different answers here.
Um, [NOISE] I'm trying to write it down here.
So let's say we have, we have the model M2.
[NOISE] Is it big enough for the back? We have the model M2, and so we give it an input image.
[NOISE] Okay input.
[NOISE] And the model M2 gives us a probability distribution, softmax.
So we have a softmax here.",Hi. I think you are missing the non-linearity added in each layer( like RELU or tanh ) which affects the derivatives.
"How shall we prove it just from the definitions? Well, the way we're going to do it is by showing that the two sets on the left-hand side and the right-hand side have the same set of elements.
Namely, if I have an element x that appears in the set described on the left-hand side, then that point is in the right-hand side.
And it's an if and only if.
So that says that the left-hand side and the right-hand side expressions defines sets with the same set of points.
This holds for all x.
And it turns out that the proof is going to follow by analogy to a propositional formula that we're going to make use of in the proof.
That was a propositional equivalence that we proved in an earlier talk, namely that OR distributes over AND.","how about rewriting (A U B)  (A U C) as (AA) U (B  C) ? I mean that what it LHS implies... right? I mean that's what gonna happen if u calculate that... rest will fall out. 
When proving that the set identity A union (B intersect C) = (A union B) intersect (A union C) is true, is it sufficient to ""go from the LHS to the RHS"" instead of ""going from the LHS to the RHS and back to the LHS"" as shown in the video?"
"Excellent.
Do we have a matching assertion, everyone? No, we don't.
That would be the world's easiest quiz problem.
We do not have a matching assertion.
Great.
So, since we don't have a matching assertion, what now? AUDIENCE: We start to look at the rules.
MARK SEIFTER: That's right.
And do you see any rule that could prove that Millicent becomes Hermione's friend? AUDIENCE: P4.
MARK SEIFTER: That's right.
You can see in P4, x, which can be anybody.
Anyone is capable of being Hermione's friend.
Great.
So P4 is our rule of the hour.
So we're going to use P4.
And when we use P4 to prove that Millicent becomes Hermione's friend, we're going to have to add something or another to the goal tree.","Would adding this assertion answer quesion 1 ""Milicent is S's friend""?"
"It's just rearranging the terms in this sum after I've multiplied through by probability of omega.
Well, of course, this is equal by definition to a times the expectation of A.
Notice this is the expectation of A, and that's the expectation of B times b.
And the proof is done.
Not inspiring, but routine if you use the alternative definition of expectation in terms of summing over the outcomes.
It's a messier proof if you have to use the definition of the expectation, being the value times the probability that the variable takes that value.
And you wind up having to convert that formula into this formula in order to carry through the proof nicely.","Hi, is the expectation function linear even for two variable with different density function (for example weibull for variable A and gamma for variable B)?, thanks in advance for your help."
"But nevertheless, let me give you a couple of examples of where this has found actual, bona fide practical application.
So when you look for practical application, you might say, well, in what kind of problem does a good front piece combine with a good back piece to produce a good thing overall? And the answer is, when you're making a plan.
So you might have a problem in planning that requires you to take a series of steps.
And you might have two plans, each of which is a series of steps.
And you might combine these to produce something new that's the front half of one and the back half of another.",outstanding teacher. Thanks a lot. But can anybody explain it's real life application.
"Uh, here, in this case, right, we have a links to b, and then b has a self-loop.
So if you run this, um, er, power iteration of what an adjacency matrix describing this graph, what will happen is that in the end, a will have, um, importance zero, and b will have importance 1.
And if you think of this, why is this happening is because wherever the random walker starts, uh, you know, it will traverse this edge and get into b, and then it is going to- to- to be stuck here in b forever, so really, you know, after some number of time, the random walker is- is in node b with probability 1, and can never go back to a.
So this is called a spider trap because the random walker gets trapped, and at the end, you know, this may be, uh, er, all the importance will be, uh, kept here in b.
And you can imagine these that, you know, even if- if you have a super huge graph here, eventually, the random walker is going traverse over this edge and then be stuck forever in this, uh, self-loop.
So that's the problem of spider traps.
Um, and then here is the problem of dead ends.
The problem of dead ends is now that node b has no outlink.","I have a question. Before adding teleport, the original matrix is super sparse, which makes computation cost affordable. However, after adding the teleport part, the matrix becomes dense. How do we solve the issue?
If M has more than 1 linearly independent eigenvectors with eigenvalue 1, the power iteration may not converge; there is more than one solution to the flow equation. One can assume it's not the case; it's interesting why."
"Um, uh, we actually kept the enrollments to CS229a at a relatively low number at 100 students.
So I actually don't want to encourage too many of you to sign up because uh, I think we might be hitting the enrollment cap already so, so please don't all sign up for CS229a because um, we- CS229a, does not have the capacity this quarter but since CS229a is uh, um, much less mathematical and much more applied, uh, uh, a relatively more applied version of machine learning and uh, so I, I guess I'm teaching CS229a and CS230 and CS229, this quarter.
Of the three, CS229, is the most mathematical.
Um, it is a little bit less applied than CS229a which is more applied machine learning and CS230 which is deep learning.","Hey question, what math should I exactly understand as a background before I start learning this course?"
"And the last piece here is, if, in fact, the low point and the middle point are the same, I've got a list of size 1.
There's nothing left to do.
I'm done.
OK, I know it's a lot of code.
I would invite you just to walk through it.
But I want to take you back again to just, simply, this point and say, here's what we're doing.
We're starting off with pointers at the beginning and end of the list.","Why is store in list of order 1? Dont you have to store all n elements so it should be n?
I dont get this part either"
"Suppose that I wanted to prove that the cube root of 1,332 was less than or equal to 11.
Or more precisely, suppose I didn't know and I'm asking this question, is the cube root of 1,332 less than or equal to 11? Well, one way to do it would be to simply compute the cube root of 1,332, which is a small bother, but manageable.
But there's a simpler way than figuring out how to compute a cube root of a four-digit number.
Let's just suppose that this inequality was true-- that is, that the cube root of 1,332 was less than or equal to 11.
Well, if that was true, then what I could do is cube both sides.
And I'll conclude that 1,332 is less than or equal to 11 cubed.
Now, 11 cubed is a lot easier to compute than the cube root of 1,332.
As a matter of fact, 11 cubed is 1,331.
Wait a minute, I've just concluded that 1,332 is less than 1,331.
That's obviously not true, which means that my assumption that this inequality held doesn't make sense.
It leads to this immediate contradiction, which means that in fact, the inequality doesn't hold.
And I have now precisely and unambiguously-- I hope clearly-- proved that the cube root of 1,332 is greater than 11, even though we never actually computed the cube root of 1,332.","if n and d both even ,this contradicts no common factor . how?? they must have 2 as common factor
Why does 3|n^2 imply 3|n please?
n and d have 2 as common factor so it contradict our assumption that 2 is rational."
"And we can say that H of x depends on H1, H2, and H3.
But now, if that that's a good idea, and that gives a better answer than any of the individual tests, maybe we can make this idea a little bit recursive, and say, well, maybe H1 is actually not an atomic test.
But maybe it's the vote of three other tests.
So you can make a tree structure that looks like this.
So this is H11, H12, H13, and then 3 here.
And then this will be H31, H32, H33.
And so that's a sort of get out the vote idea.
We're trying to get a whole bunch of individual tests into the act.
So I guess the reason this wasn't discovered until about '10 years ago was because you've got to get so many of these desks all lined up before the idea gets through that long filter of ideas.
So that's the only idea number two of quite a few.
Well, next thing we might think is, well, we keep talking about these classifiers.
What kind of classifiers are we talking about? I've got-- oh, shoot, I've spent my last nickel.
I don't have a coin to flip.
But that's one classifier, right? The trouble with that classifier is it's a weak classifier, because it gives me a 50/50 chance of being right.","Great lecture. I would LOVE to see an updated version of it (without having to go to Cambridge...), as much has changed over the past 10 years. For one thing, I imagine the focus would now be on gradient boosting... Anyway, I'm curious to hear people's thoughts on the implied quiz question around 8m15s. I thought about it for a few minutes, and my feeling is that as long as all of the individual models have the same classification accuracy (i.e., the sizes of the small circles are the same), ensembling can never hurt. Yes/no?
So, why several weak learners combine can become a strong learner? Can we prove it in probability?
Hi I didnot understand this concept,can you share some references where I can better understand the concept?"
"So that will tell us the number of nodes at each level.
So if there are n items, the number of nodes in the tree is going to be the sum from 0 to n of 2 to the i because we have that many levels.
And if you've studied a little math, you know that's exactly 2 to the n plus 1.
Or if you do what I do, you look it up in Wikipedia and you know it's 2 to the n plus 1.
Now, there's an obvious optimization.
We don't need to explore the whole tree.
If we get to a point where the backpack is overstuffed, there's no point in saying, should we take this next item? Because we know we can't.","Can someone please explain the recursive procedure in maxval? Especially in the withVal and withtoTake and the opposites of the two? How is it actually exploring the tree? When does it check between withVal and withoutVal
Min 9.17 shouldn't the number of nodes be when there are n items: 2^(n+1) - 1 ?"
"Linked lists are very bad at get and set at a given index.
AUDIENCE: Is that the-- the bottom idea, is that a linked list? ERIK DEMAINE: This is not a linked list.
This is just storing a single number as integer in your data structure that says, what is the smallest key in my data structure? That's all it this.
It's a counter.
AUDIENCE: Ah.
ERIK DEMAINE: OK, so data structure keeps track of its length.
And it keeps track of the minimum key.
And so it will always consist-- the invariant is, you will always have keys from first up to first plus length minus 1.
And that's what we're exploiting here.
We have no idea where first will be.
It depends how many operations you've done, how many inserts at the beginning, and so on.
But the keys-- keys will always be first to first plus length minus 1.
This is what we call an invariant.
Useful to write these things down so you can understand what the heck-- why is your data structure correct? Because of invariants like this, which you can prove by induction, by showing, each time you do an operation, this is maintained, even when I'm changing first in order to maintain this invariant.",How come a iterator on a set(with hashing) gives ordered sequence? I can't able to think of a way.. can someone help me?
"And we're just gonna talk about the probabilistic analog of these, as opposed to finding the maximum weight assignment.
Okay.
All right.
So now let's try to motivate why we need, uh, Bayesian networks with this following example.
So, um, here's a setting.
So earthqua- earthquakes and burglaries are things in the world, they are bad things.
Um, but suppose that they're independent, right? That kinda makes sense.
Um, but in your house you've ins- installed an alarm system, which is going to detect either, uh, both earthquakes and alarms.","No idea. I feel like it should just be 0.5. Both mathametically and intuitively, if there is an alarm and we have no other information, and the probability of an earthquake = probability of burgalary, then it should be equally as likely for it to be an earthquake that triggered the alarm as it is a burgalary to trigger the alarm."
"And from that point on, your guess is actually all 0.
You have no idea what the other bits are.
So these guys are going to try to get this guess g into this place in the pipeline.
Because this is where there are two tiny effects: this choice of Karatsuba versus normal multiplication.
And this choice of, or this a different number of extra reductions depending on the value c0 prime.
Sp they're going to actually try to get two different guess values into that place in the pipeline.
One that looks like this, and one that they call g high, which is all the same high bits, q2 qj.
And for the next bit, which they don't know, [? you ?] guess g is going to have 0, g high is going to have a bit 1 here and all zeros later on.
So how does it help these guys figure out what's going on? So there are really two ways you can think of it.","What about q1,q2...qj, how do you know them? What about solutions to that attack?"
"1 plus b, that's getting or insurance policy down here at this first level.
And we're going to add b squared all the way down to b to d minus 1.
That's how much we're going to spend getting an insurance policy at every level.
I wished that some of that high school algebra, right? Let's just do it for fun.
Oh, unfortunate choice of variable names.
bs is equal to-- oh, we're going to multiply all those by b.
Now, we'll subtract the first one from the second one, which tells us that the amount of calculation needed for our insurance policy is equal to b to the d minus 1 over b minus 1.",Why is bS an unfortunate choice of variables?
"OK, so let's think of it in terms of recurrences, in case that's not clear.
Here we have t of u is 2 times t of square root of u.
Right, to solve a problem of size u, I solve two problems of size square root of u plus constant.
Because high of x and low of x, I'm assuming, take constant time to do.
It's just, I have an integer.
I divide it in half.
Those are cheap.
What does this solve to? It's probably easier to think of it in terms of log u.
Then we could apply the master method.
Right, this is the same thing as t prime of log u is 2 times t of log u divided by 2 plus order 1.",Why is T(u) = 2*T(u) + O(1) <=> T'(lg u) = T'(lg (u^1/2) + O(1)? Like why can we replace u with lg(u) and obtain the time complexity for T(u)? Thanks!
"It becomes vanishingly small as our sequence length becomes long.
Well, what can we mean by small? Well, a matrix is small if its eigenvalues are all less than 1.
So we can rewrite what's happening with this success of multiplication, using eigenvalues and eigenvectors.
And I should say that all eigenvalues less than 1 is sufficient, but not necessary conditioned for what I'm about to say, right? So we can rewrite things using the eigenvectors as a basis.
And if we do that, we end up getting the eigenvalues being raised to the lth power.
And so if all of our eigenvalues are less than 1, if we're taking a number less than 1, then raising it to the lth power, that's going to approach 0 as the sequence length grows.
And so the gradient vanishes.
OK, now the reality is more complex than that.
Because actually, we always use a nonlinear activation sigma.
But in principle, it's sort of the same thing apart from, we have to consider in the effect of the nonlinear activation.
OK, so why is this a problem that the gradients disappear? Well, suppose we're wanting to look at the influence of time steps well in the future, on the representations we want to have early in the sentence.
Well, what's happening late in the sentence just isn't going to be giving much information about, what we should be storing in the h at time 1 vector.
Whereas on the other hand, the loss at timestep 2 is going to be giving a lot of information at what should be stored in the hidden vector at timestep 1.","Regarding vanishing gradients: h are not parameters - W are. Right? And for W, those gradients sum up, i.e., no vanishing. It seems unclear why dJ/dh should be important. Aren't we only interesting in updating W?"
"What is 2 to the eighth? Well, if I say 2 asterisk 8, I get 256.
One issue that comes up when we're starting to do this integer arithmetic is something called operator precedent.
If I have 5 plus 3 times 5, this could be interpreted as 5 plus and then 3 times 5, which would give me 20; or this could be interpreted as 5 plus 3 times 5, which gives me 40.
And so it turns out that Python is going to use the same rules that you might remember from algebra, and it's going to take the 3 times 5 as more important and having higher precedence than the 5 plus 3.
So 5 plus 3 times 5 with no parentheses is going to give me 5 plus 3 times 5, so that's going to give me 20.
And these are actually the same rules that you might remember from algebra.
So multiplication and division will take precedence over addition and subtraction.
Another area where it's ambiguous on what happens is the order in which operations will take place if we've got the same operation.
So 12 minus 6 minus 6 could be 12 minus 6 minus 6, which would give us 12; or it could be 12 minus 6 minus 6, which would give us zero.",Why cant 12-6-6 be 12+(-6-6) or (12-6)-6? Is separating the - from the -6 allowed?
"So at each step, we predict the next action using some discriminative classifier.
So starting off he was using things like support vector machines, but it can be anything at all like softmax classifier that's closer to our neural networks.
And there are either for what I presented three classes if you're just thinking of the two reducers in the shift, or if you're thinking of you're also assigning a relation and you have a set of R relations, like 20 relations, then there'd be sort of 41 moves that you could decide on at each point.
And the features are effectively the configurations I was showing before.
What's the top of the stack word? What part of speech is that? What's the first word in the buffer? What's that word's part of speech? Et cetera.
And so on the simplest way of doing this, you're now doing no search at all.
You are just sort of taking each configuration and turn, decide the most likely next move and you make it.
And that's a greedy dependency parser, which is widely used.
You can do better if you want to do a lot more work.
So you can do what's called a beam search, where you maintain a number of fairly good parsed prefixes at each step.
And you can extend them out further and then you can evaluate later on which of those seems to be the best.
And so beam search is one technique to improve dependency parsing by doing a lot of work.
And it turns out that although these greedy transition based parsars are a fraction worse than the best possible ways known to parse sentences that they actually work very accurately.
Almost as well and they have this wonderful advantage that they give you linear time parsing in terms of the length of your sentences and text.",Does this play list covers the coding aspect even a little?
"But the point is that within this gadget, I only have three allowed triples.
And these points only appear in this gadget, which means they have to be covered in this gadget.
They can be covered by this triple or this triple or this triple.
But once you choose one, you can't choose the others.
What this means is if I set x1 to be true, it leaves behind these points marked true.
If I choose the red things, then it's the blue points that are left behind.
Leaving points behind in this case is going to be good, because this clause, in order to satisfy this clause, in order to choose one of these three triples, at least one of these must be left behind by the wheel.
If all of these are covered by their wheels, then there's no way.
I can't choose any of these guys.
But if at least one of these is left behind by the wheel, then I can choose the corresponding triple and cover these points.
So I'll be able to cover these points if and only if at least one of these is true.
And that's a clause.
That's what a clause is supposed to do in 3SAT.
If at least one of these is true, then the clause is satisfied.","why is there no x_5 or x_6 in the 3SAT example?
i got lost once he started drawing the wheel. can anyone explain?"
"And now going back to what I had initially with respect to the claim here where the r here was a randomly chosen r, I'm saying, thanks to this little argument-- this line up top-- I'm going to be able to say this is greater than or equal to one half.
OK? Cool.
Any questions? Yeah.
AUDIENCE: I think the Dr squared times column equal column on the board.
SRINIVAS DEVADAS: Yeah.
AUDIENCE: On the last column, it should be i, not j.
SRINIVAS DEVADAS: This should be i? AUDIENCE: Yes.
SRINIVAS DEVADAS: People agree with that? Majority vote.
All right.
I'm good.
Let's make that an i.
AUDIENCE: The iteration of that as well, Dv sub j.
SRINIVAS DEVADAS: Oh, yeah.
Of course.
Yeah.
Once you do that, you have to have an i there.
Good.
So you're looking at a particular entry-- it makes a difference whether you used a column or a row.","What does the T mean?
What is the collection of r vectors? Doesn't the 1-1 mapping argument require that your set of vectors (the support of the randomization) is close under addition by v. But without knowing v ahead of time, this might require an infinite set of vectors, in which case a bijection is not enough to guarantee equal measure between sets.
Wasn't the professor correct in making the jth value 1 instead of the ith value as pointed out by one of the students?
the professor was incorrect?"
"It's a funny usage of the word, but I needed something short that would fit on the slide.
So registers is basically turning the arrows backwards of is registered for.
And now I can apply the definition of image to R inverse in a useful way.
But just to be crisp about what we're doing here is formally our inverse is gotten by flipping the role of the domain and the codomain.
So we have that dRj if and only if jR inverse d.
So let's look at R inverse of 6.012.
What that's going to mean is all the students that are taking 6.012.
So we start off at 6.012, and we go back to all the students that are registered for it.
It's Jason and Yihui again.
And so our inverse of 6.012 is Jason and Yihui.
Our inverse of 6.012 and 6.003? Well, it's same deal.
Let's look at 6.003 and 6.012 and look at all the students that are registered for either one of them.
Now its Jason, Joan, and Yihui shown by those red arrows-- all the arrows coming out of those two courses, 003 and 012.
And so our inverse of 003 and 012 is that set of three students-- Jason, Joan, and Yihui.
And in general, when you start off with a bunch of subjects-- a bunch of elements-- of the codomain and you apply R inverse to it, it's called the inverse image of the Y under R.
Well, let's look at the set J of all the subjects and think about what is R inverse of J.
What does it mean? Well, R inverse of J is all the students that are registered for some subject at all, which is a good thing to have.","""D  inverseR(J),"" states __the set of all students__ (denoted by D) happens to be a subset of __the set of subjects that have registered students__ (denoted by inverseR(J)). It's NOT true because the set of all students (that is, D) includes/contains the element Adam but the set of subjects that have registered students (that is, inverseR(J)) does not include/contain some subject that has registered the student Adam -- D  inverseR(J) is {Jason, Joan, Yihui, Adam}  {Jason, Joan, Adam} and is False.
Why is the label for that group called stuDent when no course registers him?"
"We haven't created a new object in memory.
We're just modifying the same object in memory.
And you're going to see why this is important as we look at a few side effects that can happen when you have this.
So I've said this a couple of times before, but it'll make your life a lot easier if you try to think of-- when you want to iterate through a list if you try to think about iterating through the elements directly.
It's a lot more Pythonic.
I've used that word before.
So this is sort of a common pattern that you're going to see where you're iterating over the list elements directly.
We've done it over tuples.
We've done it over strings.
So these are identical codes.
They do the exact same thing, except on the left, you're going from-- you're going through 0, 1, 2, 3, and so on.
And then you're indexing into each one of these numbers to get the element value.
Whereas on the right, this loop variable i is going to have the element value itself.
So this code on the right is a lot cleaner.
OK.
So now let's look at some operations that we can do on lists.
So there's a lot more operations that we can do on lists, because of their mutability aspect than we can do on tuples or strings, for example.","I'll make a comment on it, since for some reason this really confused me: why .sort() returns none. (If I get something wrong, feel free to correct me and I will edit this comment, I am new to coding.) The reason that list.sort() and list.reverse() return None, while sorted(list) returns a list's values, is basically that the methods are completed by different means. Code that returns None is completed ""in-place,"" which means that the original variable itself is not duplicated, only changed. Python does not want you to think that a new variable has been/could be created after running the method .sort() or .reverse() (or any other in-place method), and so it returns None. It is basically telling you that any method that is done in-place CANNOT generate a new variable because it has not duplicated the old variable. Meanwhile, sorted(list) is creating a new list entirely, and so you can assign it to a new variable name. ( sortedList = sorted(originalList) ) So basically, don't assign in-place methods to any new variables.
I have a confusion, If tuples are immutable how does nums = nums + (t[0],) work?
For the last L1, L2 question, I've tried it with the first code but got L1 = [2, 3]. Is that because Python has been updated to fix that issue??
it's concatenating an empty tuple to a singleton tuple (and on and on as the for loop runs). Consider; nums = 'walk' nums = nums + 'ing' print(nums) will output walking. 'Walk' and 'ing' are immutable as strings, but they can be concatenated together. In the same way a tuple can be concatenated to a tuple. nums = (1,2) nums = nums + (3,4) print(nums) will output (1, 2, 3, 4) What it's not doing is changing an element within a tuple, as tuples are immutable.
In that last example, couldn't one iterate through the list backwards?
I am curious as to the advantage of using a tuple over a list. It seems as though lists have the exact same functionality as tuples but with the added advantage of you being able to modify them(mutability) if you so choose to. Why not just alway use lists?"
"I'd write it on the board.
But I am a slow writer and already low on time.
And so essentially, what did I implement? Well, I found the biggest element between index 0 and index i minus 1.
So let's say that I have an array-- I forget the sequence of numbers-- 8, 3, 5, 7, 9.
That'll do it.
And so like I give a pointer here, which is i.
And the very first thing that I do is I compute the biggest number all the way to the left of this stuff.
In this case, that is? AUDIENCE: 8.
JUSTIN: 8.
There we go.
Now, I look at the very last element of my array, which is-- 9.
You're killing me today, guys.
And then what do I return? Well, I want the biggest one between 0 and index i.
So in this case, I return the 9.
Does that make sense? So I know Jerry Cain at Stanford likes to talk about the recursive leap of faith that happens.",Anyone else get confused by the mathematical part?
"Second one I'm going to use is called the law of multiplication.
And this says when I have nested statements or nested loops, I need to reason about those.
And in that case, what I want to argue-- or not argue-- state is that the order of growth here is a multiplication.
That is, when I have nested things, I figure out what's the order of growth of the inner part, what's the order growth of the outer part, and I'm going to multiply-- bleh, try again-- I'm going to multiply together those orders of growth, get the overall order of growth.
If you think about it, it makes sense.
Look at my little example here.
It's a trivial little example.",Oh he's also counting the check again for the if condition? I don't get the second nested loop
"Let's apply it.
And I'm going to look at an example where we're looking at digit permutations, and I'm going to look at permutations of the 10 digits, 0 through 9 inclusive.
There's a standard one where they're listed in order, and there is just a random seeming permutation of the digits 0 through 9.
Notice that the 1 and 3-- the 2 is sort of out of order.
The rest are in order.
Now, what I'm going to be interested in is those permutations where certain patterns appear.
So first of all, let's note it that the number of permutations we know how to count-- it's 10 factorial.
I'm interested in how many permutations have a consecutive 6 and 0, a consecutive 0 and 4, or a consecutive 4 and 2.",how the heck is he getting 193 from 3*9!-3*8!???
"And this is really a good list.
The first thing was I just read the psets as soon as they came out, made sure that the terminology just sunk in.
And then, during lectures, if the lecturer was talking about something that suddenly I remembered, oh, I saw that word in the pset and I didn't know what it was.
Well, hey, now I know what it is.
Right? So just give it a read.
You don't need to start it.
If you're new to programming, I think the key word is practice.
It's like math or reading.
The more you practice, the better you get at it.
You're not going to absorb programming by watching me write programs because I already know how to program.
You guys need to practice.
Download the code before lecture.
Follow along.
Whatever I type, you guys can type.
And I think, also, one of the big things is if you're new to programming, you're kind of afraid that you're going to break your computer.
And you can't really do that just by running Anaconda and typing in some commands.
So don't be afraid to just type some stuff in and see what it does.
Worst case, you just restart the computer.
Yeah.
That's probably the big thing right there.
I should have probably highlighted it, but don't be afraid.
Great.
So this is pretty much a roadmap of all of 6.0001 or 600 as I've just explained it.
There's three big things we want to get out of this course.
The first thing is the knowledge of concepts, which is pretty much true of any class that you'll take.
The class will teach you something through lectures.
Exams will test how much you know.
This is a class in programming.
The other thing we want you to get out of it is programming skills.
And the last thing, and I think this is what makes this class really great, is we teach you how to solve problems.
And we do that through the psets.
That's really how I feel the roadmap of this course looks like.","What are PSETS? Are they like assignments that students of the lecture received?
Video never explained: ""What is computation?""...
Hello MIT, I know this was taught in Fall of 2016, so are the methods and things learned in this course now obsolete or can what we learn from this class still be used today?"
"Let's say this is the potential value of Q.
Okay.
So this is the value of A1 and this is the value of A2 and this is our uncertainty.
Well, if you're being optimistic, you're always gonna select this, because it's got a higher value.
But if you wanna be really confident that A1 is better, you should select A2 because likely when you do that, you're gonna update your confidence intervals and now you're gonna be totally sure that A1 is best.
But this approach won't do that.
Okay, because it's, it's, um, it's like, no I'm gonna suffer the cost in my head of taking the wrong action so I'm gonna take A1.
But if ultimately, you just need to know what the right action is to do, then sometimes in terms of computation you should take A2 because now, your confidence intervals will likely separate.",what is the difference between UCB and UCT to evaluate?
"We're looking for a test that produces homogeneous groups.
So just for the sake of illustration, I'm going to suppose that we're going to judge the quality of the test by how many sample individuals it put into a homogeneous set.
So ideally, we'd like a test that will put all the vampires in one group and all the ordinary people in another group right off the bat.
But there are no such tests.
But we can add up the number of sample individuals who are put in to at least homogeneous sets.
So when we do that, this guy has 3 in a homogeneous set here.
A fourth.
But these are not a homogeneous set.
So the overall score for this guy will be 4.
This one, well, not quite as good.
It only puts 3 individuals in a homogeneous set.
This one here, 2 individuals into a homogeneous set.
Everybody else is all mixed up with some other kind of person.
And over here, how many samples are in a homogeneous set? 0.
So on the basis of this analysis, you would conclude that the ordering of the test with respect to their quality is left to right.
So the best test must be the shadow test.
So let's pick the shadow test first, see what we can do with that.
If we pick the shadow test first, then we have this arrangement.
We have question mark, and we have Yes, casts a shadow, and No, doesn't.
We have 3 minuses here.
We have a plus here.
And unfortunately, over here, we have plus, plus, minus, minus.","Sorry, I still don't understand how you arrived at 4 in the shadow group. Is it because there were 4 question marks? I fully understand the other homogenous sets. Thanks."
"So in the update rule- um, so this is just, um, so kij represents the- the, um, um, ij element of the matrix, and that should be a scalar.
And the kernel always computes into a scalar.
The kernel always computes into a scalar, right? And for beta- so beta is a vector, so beta of t is an Rn, right? So beta t plus 1 are the- the jth beta at step t plus 1 is equal to the old beta plus alpha, again scalar.
So scalar, scalar times something, yi is a scalar and kij are scalars, and you're summing over some scalar times some scalar, so this whole thing is a scalar, right? So-.
[inaudible] I'm sorry? [inaudible] So j is just the index you, you are summing over.
[inaudible] Yeah, so beta- beta j is a scalar, so beta t is an Rn.
So beta t subscript b is, is a scalar, all right? So this whole thing is a scalar, scalar minus a scalar times a scalar, plus a scalar equals a scalar, right? And similarly over here, this is a vector in Rn, Rn.
Y is in Rn.
K is n by n, beta t is Rn, so n by n times Rn will give you n.
So there's an n vector, n vector, difference is an n vector, times a scalar is an n vector plus an n vector equals n vector.","thanks but im talking about k(x, z) are you talking the same ? isnt it like x is the rows of matrix and z is column ?"
"Let's look at one more example of using the invariant to verify a little algorithm that actually will be quite important as the course progresses, and that is fast exponentiation.
So in this set up, a is a real number and b is a non-negative integer.
I want to compute the b power of a.
Let's say b was 128, and I want to compute the 128th power of some real number a.
Well, I can multiply a by itself 127 times, that would work fine.
But you think about it, suppose I square a and then I square it again, and I square it again, then in about eight squarings, instead of 99 multiplications, I'm going to get to 8 of the 128th.","Good day! Fast exponentiation is a good trick. However, how do I know what is preserved in state machines. Is it though a trial-and-error, or is there a good way to come up with what is preserved? Thanks in advance"
"OK? Sorry for the mess here.
But this last line there-- so this is simply a specification of the problem.
I'm going to be given x, and I'm going to be given a large collection of subsets, such that the union of all of those subsets are going to cover x.
And now I'm saying, I want to look inside and I want to select all of them.
I want to select a bunch of these things.
You know, C, which is some subset of these indices, so 1 may not be in it.
2 may be in it.
4 may be in it, et cetera.
And such that those subsets that are in this capital C set add up to x.
OK? So pictorially, may make things clearer.
You have, let's say, a grid here corresponding to x.
So each of these dots that I'm drawing here are elements that need to be covered.
So that's my x.
And I might have S1 corresponding to that.
This is S3, right? And S2 is this thing in the middle.
That's S2.
S5 is this one here.
And I got a little S4 over here.
And finally, I got-- let's see-- S6, yup.
Which is kind of this funky thing that goes like that.
So this thing here is S6.
All right? OK.
What's the optimum? You got 30 seconds.
What's the optimum cover? Yeah, go ahead.
You had your hand up.
AUDIENCE: [INAUDIBLE] PROFESSOR: Yep.
That's exactly right.
So the optimum S3, S4, S5.
All right.
AUDIENCE: [INAUDIBLE] PROFESSOR: Oh.
S3.
Yeah, S4 is this one here.
S6, S5, OK good.
S3.
And then-- oh! You know what? You're right.
Let's make you right.
Don't erase that.
Here you go.
So that's 3, right? So C would be-- cardinality of C would be 3.","The input elements are sorted in decreasing order, so you can imagine that only 'small' values were left which don't change the sums that much so were added to only one of the sets and thus the bigger set(aka A) stays the same after the initial first stage. The second stage always executes(if m < n), so if the lector said that, then he made a mistake."
"That's the- that's the idea, and, uh, as I said, in terms of dropout, what it does is it helps with, um, preventing over-fitting of, uh, neural networks.
The next component of our graph neural network layer is in terms of non-linear activation function.
Right, and the idea is that we apply this, uh, activation to each dimension of the, uh, of the embedding X.
What we can do is apply a rectified linear unit, which is defined simply as the maximum of x and 0, so the way you can think of it based on the inputs- input x, the red line gives you the output.","Coefficients e_AB can be negative in the way they are defined because the last step is a matrix multiplication. Perhaps, one can use ReLU and then what you've described. It might be an interesting thing to try in GraphGym."
"I have no idea how big that is, but it's really big, Then we take that universe, that population, and we sample it by drawing a proper subset.
Proper means not the whole thing.
Usually more than one sample to be useful.
Certainly more than 0.
And then we make an inference about the population based upon some set of statistics we do on the sample.
So the population is typically a very large set of examples, and the sample is a smaller set of examples.
And the key fact that makes them work is that if we choose the sample at random, the sample will tend to exhibit the same properties as the population from which it is drawn.
And that's exactly what we did with the random walk, right? There were a very large number of different random walks you could take of say, 10,000 steps.
We didn't look at all possible random walks of 10,000 steps.
We drew a small sample of, say 100 such walks, computed the mean of those 100, and said, we think that's probably a good expectation of what the mean would be of all the possible walks of 10,000 steps.
So we were depending upon this principle.
And of course the key fact here is that the sample has to be random.
If you start drawing the sample and it's not random, then there's no reason to expect it to have the same properties as that of the population.","What's the main difference between the law of large numbers and the monte Carlo experiment? Monte Carlo is using the law of large numbers to run the experiment? Why do we need the monte Carlo experiment if we know the characteristic of the law of large numbers? Can anyone tell?
What is a ""randomly generated non-representativel sample"" , and what proportion of conceivable sample results are likely to result from all random samplings ?
Nice. Can I just argue something, there is always an example with coins but I don't think I have ever heard someone just adding a disclaimer that tossing a coin is not a random process. In theory, if you could start the tossing under the same initial conditions you would get the same outcome. So it is possible to get an infinite number of heads if you just manage to toss the coin the same way an infinite number of times (i.e. with the same initial conditions). I don't think that would be too difficult to achieve either. An example of a true random process is nuclear decay of radioactive atoms.
The computer program is no really random as the random samples are generated using psuedo-random generators.
Hello, thank you for your lecture on Monte Carlo. I have a question, I dont understand why it is better to run the simulation rather than just use the mean? For the roulette example, were providing the code the 1/36 probability and using a simulation to show that well win 1/36 times. But of course we know that already, and so why run the simulation? My question related to why Im researching Monte Carlo. I have a real world problem where I am trying to forecast unplanned shutdowns for machinery. I have a detailed history of unplanned shutdowns for the past 10 years. Say I have 1005 machines and I have on average 7 breakdowns per month. Can I use Monte Carlo to forecast how many machines will breakdown per month going forward? Or should I just assume Ill have 7 breakdowns per month? What would be the benefit of each? Im happy to self learn, Im hoping someone can point me in the right direction. Thank yoy"
"So this was now a message operation, message transformation, and message aggregation.
One important issue is, if you do it the way I defined it so far is that information from the node itself could get lost, right? Basically, that computation of message for node v for level l does not directly depend on what we have already computed for that node- same node v from the previous level, right? So for example, if I do it simply as I show here, we are simply aggregating information about neighbors, but we don't really say, okay, but who is this node v? What do we know about node v before? So an opportunity here is to actually, to include the previous level embedding of node v.
Then we are computing the embedding of v for the next level.
So usually basically a different message computation will be performed, right? What do I mean by this is, for example, the message transformation matrix W will be applied to the neighbors u.
While there will be a different message aggregation function B that will be applied to the embedding of node v itself.
So that's the first difference, right? So that the message from the node itself from previous layer will be multiplied by B, while messages from neighbors from previous layer are going to be multiplied by W.
And then the second difference is that after aggregating from neighbors, we can aggregate messages from v itself as well.","I assume ""dividing by degree"" should be a part of the aggregation instead of messaging. Because the children do not know the degree of the parent vector.
so the box interprets as a neural network which means ""applying transformation followed by a non-linearity to create a next level message"" (mentioned in a previous lecture). How is it then aggregation function? Am I missing anything, would be helpful if anyone clarifies
In a sense, GCN also considers previous layer embedding (via self loop) of the present node while doing aggregation. Isn't it?"
"Now we want to argue that it's efficient.
What does efficiency mean? Efficiency just means not only how fast does this algorithm run, but how fast does it compare to other possible ways of approaching this problem? So how could we measure how fast an algorithm runs? This is kind of a silly question.
Yeah? STUDENT: [INAUDIBLE] JASON KU: Yeah.
Well, just record the time it takes for a computer to do this thing.
Now, there's a problem with just coding up an algorithm, telling a computer what to do, and timing how long it takes.
Why? Yeah? STUDENT: [INAUDIBLE] JASON KU: It would depend on the size of your data set.
OK, we expect that, but there's a bigger problem there.","Can you please if following this playlist will tech algorithm and data structure? Or is not enough?
I would suggest you gave a definition of an algorithm first and only after that you go to a definition of a computational algorithm."
"If n is positive and n divided by 2's remainder is 0, return true.
So if n is even and positive return true.
The next one, if n is negative and divisible by 2 return true.
OK, so far.
And otherwise return false.
Question being, with that implementation is this test set n is 4, n is minus 4 path complete? And the answer is, yes.
Because 4 is a positive number and divisible by 2.
Minus 4 is a negative number and divisible by 2.
And 5 would hit upon the else.
So the answer is actually yes for that first question.
Perfect.
Second question, with that implementation, which value for n is incorrectly labeled by that program? Well, n is very large still works, and is very small still works.
And remember, I said you have to test boundary conditions.
In this case, boundary conditions for this program being when n is equal to zero.
So I think the orange is n is equal to 0.
Perfect.
",the path isn't complete cuz of the 0 cases haven't been covered since 0 is even and the method returns false for it!
"And so in general, in practice when putting these algorithms into practice, it's important to keep that in mind.
If you also want to add, I can also potentially try to point you to things that have specifically looked at unsupervised learning within balanced data.
Cool.
And so you apply this process iteratively to update your encoder.
And once you have your encoder, you can then either train a classifier on top of that representation, or fine tune the entire network.
One other design choice I'll mention, it's not kind of written in the equations.
But the SimCLR paper found it pretty helpful to actually use the representation right here as your pre-trained representation, rather than the one right here.
And that means that there's this kind of additional projection head that's taking the representation and projecting it into another space.
And then doing the compare and contrast in that other space.
And they found that this made performance somewhat better.
And you could imagine that it gives a little bit more flexibility to the network.
Although, it also introduces additional hyper-parameters in determining where should this representation be in the network.
Yeah.
On the loss function, I see a similar, the two samples.
And if [INAUDIBLE] come from a different class of group because, I'm thinking, for example, for example, an instance, suppose [INAUDIBLE] maybe in the first and second, both are for a similar cat, right.","Also, doesnt SimCLR use positive AND negative examples in denominator? + it uses temperature parameter in num/denominator tau. Why Stanford prof ignore this??"
"AUDIENCE: Was it heads or tails? JOHN GUTTAG: What was that? So when we think about nondeterminism in computation, we use the word stochastic process.
And that's any process that's ongoing in which the next state depends upon the previous states in some random element.
So typically up till now when we've written code, one line of code did depended only on what the previous lines of code did.
There was no randomness.
Here, we're going to have randomness.
And we can see the difference if we look at these two specifications of rolling a die.
The first one, returns an int between 1 and 6, is what I'll call underdetermined.
By that I mean you can't tell what it's going to return.",What were the coins though?
"I'm sure of that.
Let's see.
That it? So each of the places where a line is obscured has four possibilities, labeled E.
The arrow junctions are labeled A.
The forks-- there are five of them-- at the fork junction 5.
So let's just step through here and see what happens.
Boom.
I've got it.
I did do it right.
So let's try some more.
What do you think will happen with this one? Unique solution? It stopped.
Bug in my program? Unthinkable.
What's happened? It is genuinely ambiguous.
It can be something hanging down from the ceiling.
Or it could be something that we can think of as a step going up from left to right.
Let's look at something more complicated.
You think it'll work? Not enough constraint for us to figure that one out.
It's equally ambiguous, but a little bit larger example.
What about this one? Yeah, but the stuff is creeping up from the lower left up to the upper right.
Yeah, bingo.
It worked.
It's unambiguous.
It's variation on the same theme we had before.
But let me, just for fun, take these two lines out.",How is he deciding the direction of boundaries ? XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXOOOOOOOOOOOOOOOOOahhhhhhhhhhhh
"OK, that one's not so bad, I hope.
Let's look at the same example with the universal quantifier.
This time, we'll say that R of y means that for every x, x is less than y.
Well, R of 1 is false.
And the reason is that 5 is a counterexample.
5 is not less than 1.
And so it's not true that every x is less than 1.
R of 8 is false because 12 is not less than 8.
And therefore, not every x is less than 8.
R of a googol, 10 to the 100th, is false.","MrRandompersondude1 Well, now that I think about it. He did change the definition of 'biggest'. Is that what you're saying?
it is relative to all other negative integers, however, as he goes on to say ""any y is not going to be bigger than itself"", so it makes no difference how you define the domain! The crucial point here is the behaviour of the < operator, not the domain."
"Well, how many failures do we expect in one try? Well, by definition, it's the expectation of getting a head on the first flip-- it's p.
OK, now if you flip n times, you expect to get n times as many failures as you'd expect in one try.
So the expected number of fails in n tries is n times p.
That's an intuitive argument.
In fact, it's the rigorously correct argument.
Remember that if we flip n times, we're counting the number of heads and flips-- that's a binomial distribution we already figured out in a couple of ways-- that it's expectations is n p.
But never mind that.
I think it's intuitively clear that if you expect p heads in one try, and you do n tries that are all independent, you're going to expect to get n times p failures-- or heads.
Now, what's the expected number of tries between failures? Well if you think about that, I've done n tries, and I've got n p failures, so if I divide the number of tries by the number of failures, that, by definition, is the average time between the failures.
It's the expected time to a failure.
So I divide the number of tries by the number of fails-- which, by definition, is the average number of tries between failures, and it's equal to n over n p, which is equal to 1 over p.
And that's an argument that I hope you will remember.
",How did you get 1/p from p + (1+E(f))*q ? I keep getting 1/q
"OK.
And you proved this in 6006 but you assumed something called simple uniform hashing.
Simple uniform hashing is an assumption, I think invented for CLRS.
It makes the analysis very simple, but it's also basically cheating.
So today our goal is to not cheat.
It's nice as a warm up.
But we don't like cheating.
So you may recall the assumption is about the hash function.
You want a good hash function.
And good means this.
I want the probability of two distinct keys mapping to the same slot to be 1/m if there are m slots.
If everything was completely random, if h was basically choosing a random number for every key, then that's what we would expect to happen.
So this is like the idealized scenario.
Now, we can't have a hash function could choosing a random number for every key because it has to choose the same value if you give it the same key.
So it has to be some kind of deterministic strategy or at least repeatable strategy where if you plug in the same key you get the same thing.
So really what this assumption is saying is that the key's that you give are in some sense random.
If I give you random keys and I have not-too-crazy hash function then this will be true.
But I don't like assuming anything about the keys maybe.
I want my keys to be worst case maybe.
There are lots of examples in the real world where you apply some hash function and it turns out your data has some very particular structure.
And if you choose a bad hash function, then your hash table gets really, really slow.
Maybe everything hashes to the same slot.
Or say you take-- well yeah, there are lots of examples of that.
We want to avoid that.
After today you will know how to achieve constant expected time no matter what your keys are, for worst case keys.","If you haven't yet found your answer: When you build the hash table and generate the hash function(s) (by choosing a random 'a') you store and re-use the 'a'. 'a' is not chosen randomly for each k-particular. You choose one 'a' for each hash function you use. The main idea is randomly choosing a hash function (by selecting a random 'a') to more likely get a random distribution of the keys across the bins, instead of assuming keys are random and having a static hash function.
One key may be hashed multiple times when using a hash table to insert an item, search for the item and finally delete it. So, there is a problem that how can i guarantee the hash code stays the same each time the key is hashed using randomized hashing? For example, how can i make sure the dot product hash function uses the same vector a (a1, a2, ..., ar-1) for the same key (k1, k2, ..., kr-1) at different times?"
"So I'll go either left or right.
And let's say I go right, so I come to here.
Then I'll just make my way up to the top of the hill, making a locally optimal decision head up at each point, and I'll get here and I'll say, well, now any place I go takes me to a lower point.
So I don't want to do it, right, because the greedy algorithm says never go backwards.
So I'm here and I'm happy.
On the other hand, if I had gone here for my first step, then my next step up would take me up, up, up, I'd get to here, and I'd stop and say, OK, no way to go but down.
I don't want to go down.
I'm done.
And what I would find is I'm at a local maximum rather than a global maximum.
And that's the problem with greedy algorithms, that you can get stuck at a local optimal point and not get to the best one.
Now, we could ask the question, can I just say don't worry about a density will always get me the best answer? Well, I've tried a different experiment.
Let's say I'm feeling expansive and I'm going to allow myself 1,000 calories.
Well, here what we see is the winner will be greedy by value, happens to find a better answer, 424 instead of 413.
So there is no way to know in advance.
Sometimes this definition of best might work.
Sometimes that might work.
Sometimes no definition of best will work, and you can't get to a good solution-- you get to a good solution.
You can't get to an optimal solution with a greedy algorithm.
On Wednesday, we'll talk about how do you actually guarantee finding an optimal solution in a better way than brute force.","One more question: Why is density function returns self.getValue() / self.getCost()
Thank you! I was confused that he was describing a local optimum with those examples because the metrics he is using are qualitatively different, ie. it might be more desirable to me to have slightly less overall calories but me maximising on ""value"" (how much I like the food) rather than cost. What seems significant for determining the optimum is the order of the elements, and the metric (or the key function) determines the order. So then the global optimum is the solution with biggest total across all orderings."
"[NOISE] Um, er, one other reason why, um, you might want to use the L_1 norm soft margin SVM is the following, which is, um, [NOISE] let's say you have a data set that looks like this.
[NOISE] You know, seems like- it seems like that would be a pretty good decision boundary, right? But, um, if we add just, you know, measure a lot of examples, a lot of evidence.
But if you have just one outlier, say over here, then technically the data set is still linearly separable, right? [NOISE] If you really want to separate this data set, um, sorry, I seem to be killing these pens myself as well.","What is that that weird symbol he wrote during L1 soft margin svm? He called it c-i , i guess"
"Okay.
So single factor connects all the parents, one factor per variable, okay? Got it? All right, so, um, the joint distribution over all the variables, remember is the product of all the local conditional distributions and just for reference, this is what it is.
Um, and now you can go and answer questions about this.
So this is kind of the fun part and I'm not going to go through the details of how this is done but I'm just gonna show you kind of the interface, um, what you would expect.
So again, this is, um, the definition of an alarm network, um, using the same machinery as a factor graph because it is a factor graph, um, and first we're gonna ask what is the probability of B? So what is that? That says in the absence of any information, is there a burglary or not? Okay? So what do you think that should be?  And epsilon here is 0.05.","Please can someone tell me the calculation of getting P(B|A)=0.51? What tool is prof. using here?
No idea. I feel like it should just be 0.5. Both mathametically and intuitively, if there is an alarm and we have no other information, and the probability of an earthquake = probability of burgalary, then it should be equally as likely for it to be an earthquake that triggered the alarm as it is a burgalary to trigger the alarm."
"And then you go to n minus 1.
Do I include it or not? And you could think about writing a big loop that would generate all of these-- actually, a bunch of nested loops.
But there's a nice recursive solution.
And I want to encourage you to think that way.
So here's the way I'm going to do it.
What did we do when we said we want to think recursively? We say, let's assume we can solve a smaller size problem.
If I want to generate the power set of all the integers from 1 to n, I'm going to assume that I can generate the power set of integers from 1 to n minus 1.","For the power set, wouldn't it be better to put the last element (""extra"") in a list of itself before adding it to ""small""? This way the function will work for all variables that can be indexed (tuple, list, string) instead of working only with lists. def genSubsets(L): if len(L) == 0: return [[]] else: smaller = genSubsets(L[:-1]) extra = [L[len(L)-1]] # put the last elem in its own list new = [] for small in smaller: new.append(small + extra) return smaller + new
Why is res = [] needed in the power set problem code? I don't see any reason why it needed to be there."
"And we know that a bottleneck here would imply a bottleneck in the whole graph.
What we want to argue is that a bottleneck in the complement of S would imply a bottleneck in the complement of E of S.
So there's the complement of E of S.
And there's the complement of S.
Now, notice that some edges that come out of the complement of S may very well go into the E of S.
That is, this would be a point that's both in E of S and in E of S complement.
But we're not allowed to use that point, because we're trying to find a match only within S bar and E of S bar.
And what we're really trying to argue is that a bottleneck here would imply a bottleneck in the whole graph.
So let's see if we can argue this.
We want to claim that there's no bottleneck between S bar any E of S bar given that there's no bottleneck anywhere and when S and E of S are the same size.
So let's suppose that we had a set T over here that was a subset of S bar.
And let's look at its image over here in orange.","what is S?
What is a bottleneck?
|S| <some condition> for all sets S that are a subset of L(H). Each S is some subset of the left side of the graph (as racun correctly stated), and we are considering all such subsets. Basically we want to analyze every subset S of the left side of our graph and see if it to conforms to the condition that its size is is less than or equal to the number of nodes it is connected to on the other side; |S| <= |E(S)|.
A bottleneck is where |S| > |E(S)| for some subset S  G. This violates Hall's matching condition."
"Okay.
So, I don't have to sum all the way over the future ones.
So, we can take this expression, and now, we can sum over all time steps.
So, this says, what's the expected reward, uh, or the derivative with respect to the reward for time step t prime? Now, I'm gonna just sum that, and that's gonna be the same as my first expressions.
So, what I'm gonna do is I'm gonna say, [NOISE] V of Theta is equal to the derivative with respect to Theta of er, and I'm gonna sum up that internal expression.
So, I'm gonna sum over t prime is equal to zero to t minus 1 rt prime, and then insert that second expression.","Why does rewriting the gradient with respect to theta of V reduce the variance? I thought the two were the same equation?
Not sure, but I think they rearranged the sum terms such that in the later notation, only the actions in the trajectory which add to the current reward are considered in expectation calculation."
"Extended list up here.
SB.
All right.
B, as we saw, goes only to D.
What is the value that I should write here? AUDIENCE: [CHATTER] PROFESSOR: OK.
I am happy I heard all the things that I expected to hear.
I heard the correct answer, which is 57.
I also heard someone say 107.
So why is it 57, and not 107? Someone does it this way every time.
Do not add up all the heuristics along the way.
I will try to explain to you why you would never want to do that.
The heuristic value at any given node says, given that I'm here, how much work do I think I have left to get to the end? All right? It's sort of like, let's guess the last few nodes in the path that we haven't done out yet.","The A* problem is not consistent in how it is solved in the final part , he changes styles at the end, depending of the accumulate path you take you can get the correct answer or the bad answer he got. In the search and bound problem he uses search and bound with an extended list while this is one thing we can add to the algorithm to improve it,it is not part of it in the algorithm of s&b in the normal use. You get the same result but you have more steps to be made."
"So, it'd be a pretty good cloud.
It would have to harness together lots of universes.
So, the British Museum algorithm is not going to work.
No good.
So, what we're going to have to do is we're going to have to put some things together and hope for the best.
So, the fifth way is the way we're actually going to do it.
And what we're going to do is we're going to look ahead, not just one level, but as far as possible.
We consider, not only the situation that we've developed here, but we'll try to push that out as far as we can and look at these static values of the leaf nodes down here and somehow use that as a way of playing the game.","Thanks for your response. You didn't get my question. How are you going to decide if a certain move is a ""somewhat good move""? Only leaf nodes can tell you what is a good move and what is a bad move. Forget about the win move at (d-5) level, think about not choosing a lose move, at (d-5)level how can you decide that a certain move isn't going to lose you the game down the road?
+Daniel Jones So it is like breadth first search in a way, right ?"
"So these guys are actually going to go and try to guess what the value of q is by timing this pipeline.
All right.
So how do these guys actually do it? Well, they construct carefully chosen inputs, c, into this pipeline and-- I guess I keep saying they keep measuring the time for this guy.
But the particular, well, there's two parts of the attack, you have to bootstrap it a little bit to guess the first couple of bits.
And then once you have the first couple of bits, you can I guess the next bit.
So let me not say exactly how they guess the first couple of bits because it's actually much more interesting to see how they guess the next bit.","What about q1,q2...qj, how do you know them? What about solutions to that attack?"
"And I build each food by giving it its name, its value, and its caloric content.
Now I have a menu.
Now comes the fun part.
Here is an implementation of a greedy algorithm.
I called it a flexible greedy primarily because of this key function over here.
So you'll notice in red there's a parameter called keyfunction.
That's going to be-- map the elements of items to numbers.
So it will be used to sort the items.
So I want to sort them from best to worst, and this function will be used to tell me what I mean by best.","One more question: Why is density function returns self.getValue() / self.getCost()
Are the numbers inside the 'values' array randomly picked by the instructor or the does it act as a grading scale for each menu item?
Thank you! I was confused that he was describing a local optimum with those examples because the metrics he is using are qualitatively different, ie. it might be more desirable to me to have slightly less overall calories but me maximising on ""value"" (how much I like the food) rather than cost. What seems significant for determining the optimum is the order of the elements, and the metric (or the key function) determines the order. So then the global optimum is the solution with biggest total across all orderings.
Where did the I[i] come from? Shouldn't it be L[i]?"
"Okay.
Where all I did was take this definition of p of y given x parameterized by theta, uh, you know, from that, after we did that little exponentiation trick and wrote it in here.
Okay.
Um.
[NOISE] And then, uh, with maximum likelihood estimation we'll want to find the value of theta that maximizes the likelihood, maximizes the likelihood of the parameters.
And so, um, same as what we did for linear regression to make the algebra, you have to, to, to make the algebra a bit more simple, we're going to take the log of the likelihood and so compute the log likelihood.",Can you guys help me out ? I can't get my head around likelihood of theta thing ....why this is equal to product of probabilities of Y
"So, I put up a couple of examples here.
So, in something like hypertension control, you can imagine the state is just the current blood pressure, um and your action is whether to take medication or not.
So, current blood pressure meaning like you know, every second for example what is your blood pressure? So, do you think this sort of system is Markov? I see some people shaking their heads.
Almost definitely not.
Almost definitely there are other features that have to do with, you know, maybe whether or not you're exercising, whether or not you just ate a meal, whether it's hot outside.
What the- if you just got an the airplane.
All these other features probably affect whether or not your next blood pressure is going to be high or low and particularly in response to some medication.
Um, similarly in something like website shopping, um, you can imagine the state is just sort of what is the product you're looking at right now? So, like I open up A- Amazon, I'm looking at some um, you know, computer, and um that's up on my webpage right now, and the action is what other products to recommend.","To put it simply, Markov's Assumption is ""that the future actions are influenced only by the present*, not the *past states"". In other words, Markov's Assumption emphasizes only the independencies across the time domain, NOT the independencies across different features. Hence, explaining the concept of Markov's Assumption by saying one feature, such as ""blood pressure"", is dependent on other features, such as ""exercise"", is missing the point on time domain dependency. The professor's other example, such as ""hot-or-not-outside"", is simply wrong. Blood pressure can be dependent on the temperature outside, but still satisfies Markov's assumption, as long as knowing current temperature provides sufficient information about blood pressure, regardless of the temperature in the past. In fact, Markov's Assumption is so widely applicable exactly because most natural processes are continuous in time, i.e. knowing the current state is often sufficient to ignore the past states. The common examples of processes that violate Markov's Assumption are human discretion and randomness.
Can you provide some examples ? I think it was very clear."
"I want to do this in one pass through the array.
I want to look at this, and I say, I see the interval 0, 0.
Is that going to be something that I need to worry about in terms of that person having to flip his cap? And then I see B, B.
And I say, oh, the interval is 1 comma 2.
Is that something I'm going to have to deal with in terms of a command? And so someone else other than Ganatra, tell me if there's a one-pass algorithm.
And explain to me why you can do this in one pass through the array.
And just in terms of code, I'll preview this.","do we actually need two if checks?
What about the case where there is FBBBBBB Would it still be efficient to not tell the first person to flip their cap?
caps[i] != caps[0] doesn't make sense it make sense only when we know first element label is not the one that we wanna flip, right?
Why don't we compare each cap (2nd throgh the last) with the direction of the First cap and flip the caps if not the same as the first cap ?
Did this guy just populate the array randomly? I cant tell. I tried 2^(2n) -2n, -n, and -1 to check for indices and their evenness to determine where the Bs are being populated. Because if you know the loc of the Bs in the array, you know where the Fs are. In theory, iff you know the number of moves to flip the array to conform to one of the two, you know the other procedure. But, then you know based on which case you use for performance efficiency, the effectiveness of the algorithm . But for his array, the case of populating in B was like ceil(n/2) and F case was like floor(n/2) + c, c contains [1,2] for cases of 0...9 and 0...10. How it was populated is like the one thing I dont understand here. The best I can think of is using 2^i somewhere: Edit: parameters for c is [0,2], not [1,2]"
"So this is the general look of a for loop.
So we have for some loop variable-- again, can be named whatever you want-- in range some number.
Do a bunch of stuff.
And again, these are part of this for loop code block.
So you should indent them to tell Python that these are the things that you should do.
So when you're using range some number, you start out with variable getting the value 0.
With variable having value 0, you're going to execute all of these expressions.
After all the expressions in the code block are done, you're going to go on to the next value.
So 1.
You're going to execute all these expressions with the variable being value 1, and then so on and so on until you go to some num minus 1.
That-- so using range in that way is a little bit constraining, because you're always going to get values starting from 0 and ending at some num minus 1, whatever is in the parentheses in range.
Sometimes you might want to write programs that maybe start at a custom value.
Don't start at 0.
Maybe they start at 5.
Maybe they start at minus 10.
And sometimes you might want to write programs that don't go with-- don't expect the numbers by 1, but maybe skip every other number, go every two numbers, or every three numbers, and so on.","the number you mention in parenthesis in for loop doesn't printed on the screen
on the 40th minute why i has the values 5 7 9 not 5 6 7 8 9 ????? please explain !!!"
"We're going to minimize the error of that entire expression as we go along.
And what we discover when we do the appropriate differentiations and stuff-- you know, that's what we do in calculus-- what we discover is that you get minimum error for the whole thing if alpha is equal to 1 minus the error rate at time t, divided by the error rate at time t.
Now let's take the logarithm of that, and multiply it by half.
And that's what [INAUDIBLE] was struggling to find.
But we haven't quite got it right.
And so let me add this in separate chunks, so we don't get confused about this.
It's a bound on that expression up there.
It's a bound on the error rate produced by that expression.
So interestingly enough, this means that the error rate can actually go up as you add terms to this formula.","how is the volume of error classified point defined?
Can anyone please explain how the error rate is bounded by exp fn. I'm kindof getting the idea but still, there's just a small sense of doubt.
Excellent in every way. Just one question: I tried to implement this simple version but what I find strange is that some of my alphas are negative because that happens when the error is greater or equal to 0.5 but if that happens we dont have a weak learner right? So whats the deal with this case? I noticed that in the demo some of the alphas were negative too. How can I deal with this case? I would appreciate answers. Thanks for the great lecture and making that amazing knowledge available to the world!"
"The first is a simple if.
And given a program that just linearly has statements that get executed, whenever I reach an if statement, you're going to check the condition.
The condition is going to be something that's going to get evaluated to either true or false.
So I've reached the condition here.
And if the condition is true, then I'm going to additionally execute this extra set of expressions.
But if the condition is false, then I'm just going to keep going through the program and not execute that extra set of instructions.
How does Python know which instructions to execute? They're going to be inside this what we call code block.","When comparing string how will it compare a and 1
The 'if' nested in an 'if' would only be considered if the parent 'if' is true, the 'else' nested in that won't ever be considered, then why use it at all? Also if all the nested 'if's have same condition during nesting then if they are true, it'll execute them all at once, so what is the use of that? What we want is that it asks us the question and show us the forest screen until we enter a 'left', so what we want it to do is to repeat this 'if' condition again and again so that it asks us that question again and again X=input ("" left/right..?"") If x== ""right"": <Show forest screen> Else: <Show exit screen> How can we do that? I didn't understand the need for while here, because for it to ask us the same question again an again we will have to run this code again and again, and that can be done with an 'if-else' condition as well.
the nested if prints the quotient of x and y. however, you can't do this if y=0 as you will get a zero division error (if x=0 and y=0, 0/0 can't be evaluated)"
"Static semantic errors can also be caught by Python as long as, if your program has some decisions to make, as long as you've gone down the branch where the static semantic error happens.
And this is probably going to be the most frustrating one, especially as you're starting out.
The program might do something different than what you expected it to do.
And that's not because the program suddenly-- for example, you expected the program to give you an output of 0 for a certain test case, and the output that you got was 10.
Well, the program didn't suddenly decide to change its answer to 10.
It just executed the program that you wrote.
That's the case where the program gave you a different answer than expected.
Programs might crash, which means they stop running.
That's OK.
Just go back to your code and figure out what was wrong.
And another example of a different meaning than what you intended was maybe the program won't stop.
It's also OK.
There are ways to stop it besides restarting the computer.
So then Python programs are going to be sequences of definitions and commands.
We're going to have expressions that are going to be evaluated and commands that tell the interpreter to do something.
If you've done problem set 0, you'll see that you can type commands directly in the shell here, which is the part on the right where I did some really simple things, 2 plus 4.
Or you can type commands up in here, on the left-hand side, and then run your program.
Notice that, well, we'll talk about this-- I won't talk about this now.
But these are-- on the right-hand side, typically, you write very simple commands just if you're testing something out.
And on the left-hand side here in the editor, you write more lines and more complicated programs.
Now we're going to start talking about Python.
And in Python, we're going to come back to this, everything is an object.
And Python programs manipulate these data objects.","Is random truly random ?
What are PSETS? Are they like assignments that students of the lecture received?
will string of length one also be a scalar object in python
Is it only in Python everything is an object?"
"And indeed, n cubed is polynomial.
It's not a great polynomial, but this is an alternate way to analyze merge sort.
Obviously don't do this for merge sort.
But it illustrates the technique.
Good so far? Any questions? All right.
Let me remember where we are.
Cool.
So the next thing I'd like to do is show you one more algorithm that we've already seen in this class that fits very nicely into this structure-- arguably is a dynamic program-- and that is DAG shortest paths.
So just to close the loop here, when I say dynamic programming, I mean recursion with memoization.
I mean, we take-- we write a recursive piece of code, which is like def f of some args, some sub-problem specification.",Which topics of descrete is used in this course? Pov: I'm just overviewing the course rn
"A lot of the time we're going to raise a value error.
So if the number is less than 0, then raise a value error, something is wrong.
The key word, the name of the error, and then some sort of descriptive string.
So let's see an example of how we raise an exception.
I have this function here called get ratios.
It takes in two lists, L1 and L2.
And it's going to create a new list that's going to contain the ratio of each element in L1 divided by each element in L2.
So I have a for loop here.
For index in range length L1.
So I'm going through every single element in L1.
I'm going to try here.
I'm going to try to do this line.
So I think that this line might give me an error.","I am surprised that no one in the class asked why does it raise ValueError instead of TypeError, as the user provides the wrong Type of input i.e string Type whereas the expected input was integer Type. I had to google to find the answer. May be they already know the answer ?
what is need of raising the exception? we can simply use print statement and break ?
even though the user provides a string , n = int(input(""How old are you? "")) this line converts n to an integer, even though n = twenty , which doesnt sount like an integer but when you ask type() it will return the answer ""integer"" so the error isnt a type error, sorry for the messy explaination even if you already googled and learned the answer , it might help the people who hasnt :D"
"So to write them down actually requires theta n bits because they are some constant to the n power.
And so they're actually really big .
n is probably bigger than w.
Usually you think of problems that are much bigger than 64 or whatever your word size happens to be.
We do assume that w is at least log n.
But n is probably bigger than w.
It might be bigger or smaller.
We don't know.
And in general, to do an n bit addition-- these are n bit additions-- is going to take ceiling of n over w time.
So in the end, we will spend this times n, because we have to do that, many of them, which is n plus n squared over w time.
So a bit of a weird running time.
But it's polynomial, whereas this original recursive algorithm was exponential here.
Using this one simple idea of just remembering the work we've done, suddenly this exponential time algorithm becomes polynomial.
Why? Because we have few sub problems.
We had n sub problems.
And for each sub problem, we could write a recurrence relation that if we already knew the solutions to smaller sub problems, we could compute this bigger problem very efficiently.","What is n-bit addition? I did not understand that particular thing. Is it the representation of total digits?
Why does O(ceil(n/w)*n) = O(n + n^2/w)? Where does the addition with n come into play, shouldn't it just be O(n^2/w)? I guess it doesn't matter since n^2/w dominates anyways (assuming w is a const or grows slower than n)"
"You can do this with just plain lists.
But the idea here is actually a very compelling one.
It's something that you'll see in other algorithms.
Whenever you have something where you're doing repeated computation, there's always-- many a time, there's a way of removing redundancy and turning into incremental computation, right? And so the insight here is the following.
The only time that celebrity density, which is the number of celebrities that are in the room, changes is obviously when a celebrity enters or when a celebrity leaves, right? I mean, that's it.
I mean, we're all celebrities here.
And I guess we did have someone leave, so there you go.
And then maybe someone is going to come in, right? So I just need to monitor sort of input and output, or entry and exit.
And I also know what the endpoints are with respect to the 6:00, 6:00 PM versus 12:00, correct? So what I can do here is the following.","Do we need to sort? Can we do this in O(N) where N will be total celebrity. After finding the start point and end point. We can iterate the schedule and update as for every entry as +1 and exit as - 1. That is if a celebrity enters at 6 and exit at 7. We update arr[6] +=1 and arr[7] - =1 In the send just keep on iterate this array and adding the values, maximum sum at any point will be the answer
I keep thinking this can be done in better then O(n^2). Any ideas?
Will it be more efficient if we find the Mode of the range of time. Say celebrity A comes in at 7 and leaves at 10. So we expand it as 7,8,9,10 , but we will not consider the last element of the range (10 here) since they have already left. Then we find the number whose frequency is maximum across all celebrities (Mode), which will be the best time to party."
"That make sense? So I'm going to keep incrementing my guess by that small value.
Before I go on, I'm going to run the code.
And we're going to discover a small issue with it.
So with 27, we're going to run it.
Perfect, it took me 300 guesses.
But 2.99999 is close to the cube root of 27.
We can find the cube root of this guy here.
And it took me 20,000 guesses, but I figured out that 200.99999, so 201, is close to the cube root of that large number.
I should have done this.
This is going to be a giveaway, you guys.
Sorry.
Then we're going to have-- let's say I want to try cube of 10,000.
So 10,000 is not a perfect cube.
So we can run the code.
And with 8,120,601 I had already gotten an answer.
But with 10,000, I'm not getting an answer yet, right? So I'm thinking that there might be something wrong.
So I'm going to stop my code.
So I just hit Control C, because I feel like I've entered an infinite loop.
And, in fact, I have.
So what ended up happening is this problem here.
So I'm going to draw something.
According to the code, I'm going to start from 0, and I'm going to increment my guesses, like that.
With every little increment, I'm going to make a new guess.
I'm going to take that guess to the power of 3.
I'm going to subtract the cube, and I'm going to figure out if I'm less than epsilon.
This is the epsilon that I want to be in, this little bit here.
So with every new guess, I might be, maybe-- so this is where I want to be, within this little boundary here.
With every new guess, I might be here.
With the next guess, over here, I might be here.
When I make another guess, I might be here.
So I'm getting close to being within epsilon.
But maybe with my next guess, I'm going to hop over my epsilon and have made too big of a guess.
So just because of the way the numbers were chosen in this example, just to illustrate this, using an increment of 0.01, a with finding the cube of 10,000 and epsilon of 0.1, it turns out that, as I'm doing all these calculations, I'm going to skip over this perfect sort of epsilon difference.
So first, I'm going to be too small.
And then I'm going to be too large.
And once I've become too large or too far away from epsilon, the guesses I continue to make are just going to be even farther away from epsilon.
And I'm not going to get to my answer.
And that's why I've reached an infinite loop in this code.
All I'm doing in this code is checking whether my guess cube minus cube is less than epsilon.
The only thing I need to do here is sort of add this little clause, here, that says, oh, by the way, also check that I'm less than cube.","Why aren't we getting cube root of 27 and 8120601 in form of x.xx as we always increment guess by second decimal place.
Cube root simple guess. Why does that code keeps running?
Am I stupid? In Approximations code we set guess=0.0 and increment=0.01. How can our answer be different than increment times number of guesses??
Can anybody explain me in Assignment 1 part C of the downpayment problem(The one where we calculate ideal monthly savings rate) Why am I getting an Infinite loop. I followed all instructions. Help would be appreciated. annual_salary = float(input(""Please enter your yearly salary: "")) monthly = annual_salary/12 portion_saved = 0.0 cost = float(input(""Please enter the cost of your dream home: "")) down_payment = 0.25*cost semi = float(input(""Decimal rate of increase of semi annual salary: "")) low = 0.0 high = 1.0 x = (low+high)/2 r = float(input(""Rate of interest yearly for savings:"")) current_savings = 0.0 month = 0 while abs(current_savings - down_payment)>= 100.0 : portion_saved = x*monthly while month <= 36: if month%6 == 0 and month != 0: portion_saved = (1 + semi)*portion_saved current_savings += r*current_savings/12 + portion_saved if current_savings < down_payment: low = x else: high = x x = (low + high)/2 print(x) print(""Your saving rate should be: "", x)
and guess**3<=cube is better choice ?
What I get from the approximation algorithm is that it will only give you an answer if the difference between guess**3 and cube is equal to epsilon, if it manages to jump above epsilon, guess will increase in value until it is <= cube, and thus the message ""failed to obtain a value"" will appear. How can we make it so that if the difference does jump above epsilon that the value will get will be the value of guess at time - 1 of that moment.
For bisection I wanted to use this code to take direct input, but error is coming. #To find the cube root of a number using bisection method cube=float(input(""Put the number... "")) accuracy=float(input(""Give accuracy limit... "") low=0.0 high=cube guess=((high+low)*0.5) iteration=0 while abs(guess**3.0-cube)>accuracy: if (guess**3.0)<=cube: low=guess else: high=guess guess=((high+low)*0.5) iteration += 1 print (guess, ""is close to the cube root of"", cube) print (""Total iteration="", iteration) **************************** syntax error in low=0 why?
can anyone tell me what am i doing wrong!! code: cube = int(input('type the no....')) for guess in range(cube+1): if guess**3 == cube: print(""Cube root of"", cube, ""is"", guess) else: print(""cube root does'nt exist"") output:- type the no....1(#1 was the input) cube root does'nt exist Cube root of 1 is 1
I know she said it is a simple if statement but I can't figure it out. What did you do to make it work? My thought is: if cube < 0: guess = -guess
What is the answer of cube = 27 on the exercise. I rewrote the code on the IDE but the execution never ended, why? Help, please
what is the meaning of epsilon here ???
Any idea on the code for a negative integer? Tried a handful of if statements, but it kept turning my while loop into an infinite loop."
"Over here, we've just defined what our subproblems are for our particular DP, and we argued that the number of subproblems that are associated with this particular choice of subproblems corresponds to n if you have n requests in the original problem instance that you've given.
So the last thing that we have to do here to solve our DP is, of course, to write our recursion and to convince ourselves that this actually all works out, and let's do that.
And so what we have here is our DP guessing.
And we're going to try each request i as a plausible first request, and so that's where this works.
You might be thinking, boy, I mean, this R of fi looks a little strange.
Why doesn't it include all of the requests that are compatible with the i-th request? I mean, I'm somehow shrinking my subsequent problem size if I'm ignoring some requests that are earlier that really should be part of-- or are part of the compatible set, but they're not part of the R of fi set.
And so some of you may be thinking that, well, the reason this is going to work out is because we are going to construct our solution, as I said before, from the beginning to the end.
So we're going to try each request as a plausible first request.
So even though this request might be in our chart all the way to the right, it might have a huge weight, and so I'm going to have to try that out as my first selection.
And when I try that out as my first selection, then the definition of my subproblem says that this will work.","I don't understand why there are n subproblems. If we're taking the max over n things, that's O(n), but Srini said that just solves a single subproblem. Can someone give an concrete example of two different subproblems here, and why we need to calculate them? It seems to me that Srini is saying we need to calculate something else after we find that max in order to really solve the problem, but he didn't write it out anywhere and it's not in CLRS :("
"I sum up all the currents, and I set that equal to 0.
So in this case--.
Or-- pretty simple.
Let's practice on this particular circuit.
One thing to note is that when you're solving circuits in the general sense, both when you want TA help and when you're solving for a mid-term and want partial credit, you want to label all of your nodes, all of your elements, and all of the currents that you're interested in solving.
See, I've got my voltage drop across this resistor, this resistor, and this resistor labeled, as well as these currents, which I'll also be solving for.
The first thing that I would do when approaching this problem is attempt to reduce this circuit to something that is a little bit simpler.
The first thing that I'm going to do is try to figure out how to change these two resistors in parallel into a single resistor and still have an equivalent circuit.
That'll allow me to solve for I1.
There will be 0 nodes in my system.
I'll just have one single loop.","if the output of voltage of RL circuit is taken to be the voltage across the inductor, the circuit is said to be what
How would i2 relate to it being correlated to R3 being over the sum and for i3 visa versa? Why can you not use i2 with R2 over the sum and so on?
What would you use, Maxwell equation within time or frequency domain?"
"So, in other words, I can make-- out of $0.03 and $0.05 stamps, I can make m minus 3 plus $0.08 but, look, if I can make m minus 3 plus $0.08, then obviously m is postal also, because I just add $0.03 to that and minus 3 number, and I wind up with m plus $0.08, which says that m is postal and is a contradiction.
So assuming that there was a least non-postal number, I reach a contradiction and therefore there is no non-postal number.
Every number is postal-- 0 plus 8 is postal, 1 plus 8 is postal, 2 plus 8 is postal.
Every number greater than or equal to $0.08 can be made out of $0.03 and $0.05 stamps.
","How did he choose 3 as a postal number, he explained 0, 1, 2 are postal then supposed m>3 is the least non postal?
How do we know j and k are products of primes? Isn't that presupposing what we are trying to prove?
What does ""nonproducts"" mean ?
I'm still confused on the stamp example
So sir please explain which 2 primes make up the product of the integer 2???
m=3 is also postal right then why did he choose m>=3
We have a pre-assumption that states that integers are products of primes. We arent supposing that all integers are non products of primes. We are supposing that there exists a set of such integers that are non products of primes and by well ordering principle we know that they will have a least integer. Now the least integer is greater than 1 and can be any value. Now comes the j and k and that are also greater than 1 but less than k but the j and k donot exist in the set our number ""m"" does so the numbers j and k are bound by the theorem and thus are product of prime numbers"
"Let me go to a board and write, uh, a few more things.
So the score, uh, remember is w dot phi of x.
And this is just gonna be a number, um, and uh, the predictor.
So linear predictor actually let me call this linear.
To be more precise, it's a linear classifier not just a predictor.
Classifier is just a predictor that does classification.
Um, so a linear classifier um, denoted f of w.
So f is where we're going to use, you know, predictors.
W just means that this predictor depends on a particular set of weights.
And this predictor is, uh, going to look at the score and return the sign of that score.
So what is the sign? The sign looks at the score and says, is it a positive of a number? if it's positive then we're gonna return plus 1.
If it's a negative number, I'm gonna return minus 1.
And if it's 0 then you know, I don't care.
You can return plus 1 if you want, it doesn't matter.
Um, so what this is doing the remember the score is either, is a real number.
So it's either gonna be kind of leaning towards um, you know, large value, large positive values or leaning towards, uh, s- large small- negative values.
And the sign basically says, okay you gotta commit are you- which side are you on? Are you on the positive side or you on the negative side? And just kind of discretizes it.
That's what the sign does.
Okay.
Okay, so, so let's look at a simple example because I think a lot of what I've seen before is kind of more the, uh, formal machinery behind and the math behind how it works but it's really useful to have some geometric intuition because then you can draw some pictures.",Why phi(x) is an array in each point value?
"The other option is what if I don't want to limit my propositional logic, what if I want to look at all of propositional logic, can I make my inference rule a little bit fancier or a little bit more powerful? So in this module, we are going to be talking about a new type of inference rule, specifically called resolution, as a way of getting both soundness and completeness.
All right, so to start with I want to just write out a few things that we're all aware of but let's just get on the same page on all of them.
All right, so let me just write out a few things, so if we have p implies q, well what is that equivalent to? That is equivalent to negation of p or q.",how did you go from A -> (B V C) to ~A V B V C? wouldn't it be ~A V (B V C) ?
"Yeah? So the first question you might ask is, like, well, why the heck wouldn't I just start on the first floor, drop the egg.
If it's not broken, go on to the next one and then drop the egg, and so on? That would be the most egg-efficient plan.
And indeed that is the case, because you'll only break at most one egg.
But you're schlepping up and down the stairs a bunch of times when you solve that, right? Every single time, you've got to go retrieve that unbroken egg.","Great content. One confusion is about the lazy egg drop problem. I think in the classical egg drop the run time is O(floor^2*egg). Not sure why for the lazy egg drop case, it becomes O(floor^3*eggs). Can anyone explain, please?"
"If you take the complex conjugate you go from up here to down here-- the reflection to the x-axis.
That's the same thing as if you measure theta as a clockwise angle from the x-axis.
That's the same thing as negating the angle.
So that's just a little geometry.
You can prove it algebraically, although I don't know how off-hand.
It's not hard.
So if I want to take some angle and then flip it through the x-axis, it's the same as the negative of the angle.
So the complex conjugate of this number is the same thing with a minus sign.
And then we have to divide by n.","For the entries of the IFFT matrix, aren't they just the FFT entries raised to the negative 1, over n? Rather than the complex conjugate?"
"And what does it do? It says, inside of this frame evaluate the body of fact.
OK, so it says as n equal to 1? Nope, it's not, it's 4.
So in that case, go to the else statement and says, oh, return n times fact of n and n as 4, fact of n minus 1 says I need to return 4 times fact of 3.
4 is easy, multiplication is easy, fact of 3, ah yes, I look up fact.
Now I'm in this frame, I don't see fact there, but I go up to that frame.
There's the definition for fact, and we're going to do the rest of this a little more quickly, what does that do? It creates a new frame called by fact.
And the argument passed in for n is n minus 1, that value, right there, of 3.
So 3 is now bound to n.
Same game, evaluate the body is n equal to 1? No, so in that case, I'm going to go to the return statement, it says return 3 times fact of 2.","During the mathematical induction step he explained, he said he wants to show that ""this is equal to that"". I believe the ""that"" it the k+1 * k+2 ,,,"" but what is the ""this""?
Would inclusion in the lecture of the 'call stack' or Python's symbol table concept help explain recursion? As you recursively call the function object's return value, the frames get 'popped' off the stack (symbol table on Python I think?) Sorry, self-taught. Still learning every day.
Ken MacDonald Yeah I know but as n goes up, there will be hundreds of steps. So do they recurse on the other because the same rules still applied?
Shouldn't there be 2 cases for the factorial function? if n==1 or n==0: return 1
One can think n==0 as base case. Which was not considered as it is preceded by n==0 base case in the lecture. One can re-write code assuming n==0 as base case."
"And then we use a learned matrix Y in order to sort of do the mixing Y is d by d, and that's the output of multi-headed attention, multi-headed self attention.
And so each head gets to look at different things, right, because they can all sort of, the linear transformations you can stay focused on different parts of the X vectors, and the value vectors also get to be different as well.
So pictorially, this is what we had before, single headed attention.
You had X multiplied by Q in order to get XQ.
And what's interesting and you can see this, you can see this from this diagram I think, is that multi-headed attention doesn't necessarily have to be more work, right? We saw that the Q, K and V matrices in multi-head attention, have a lower output dimensionality.","Good lecture, but I think there is some confusing usage of the K, Q, and V matrices. You introduce them by saying: (1) K*x_i = k_i, Q*x_i = q_i, V*x_i = v_i In this way you're saying that X, Q, and V operate on x_i column vectors from the left to yield another column vector. But then later, when you stack the transposes of the x_i to produce the X matrix for the tensor attention calculation, you multiple from the right with K: (2) softmax(XK(XQ)^T) = Attention weights I think both K and Q should be transposed in (2) as you're using them in order to be consistent with the way you define the matrix in (1).
Are matrices K, Q, V parameters to be learned?
Great lecture, very clear! Thanks for making it freely available! In multihead self-attention, each head is going to produce values vectors of dimension d/h whereas the single head self-attention produces value vectors of dimensions d. That allows the multihead self-attention to look at multiple places at the same time however reducing the dimension is going to discard some information. Could it be possible that single head self-attention works better in some cases than multihead self-attention because it doesn't reduce the dimensionality?
Agree. It seems that this lecture cannot even tell the reason of why we should use K, Q, V clearly..."
"All right, but we have to prove this claim.
So let's do a little bit of algebra.
No pain, no gain.
Good.
So let's look at vjk prime.
Sorry.
So let's look at p, the product of v, and v complex conjugate-- what I was calling v prime up there.
I claim that this thing is n times the identity matrix, with 1's down the diagonal and 0's everywhere else.
So let's look at this product.
In general, let's look at the the jk-th item in the product.
That's going to come from row j of V, dot product with column k of the complex conjugate.
Now the matrices here are symmetric, so actually rows and columns are the same, but that's the general definition of the cell and the product of two matrices.","For the entries of the IFFT matrix, aren't they just the FFT entries raised to the negative 1, over n? Rather than the complex conjugate?
is there a mistake ? we know V.A = Y ( V - vandermond matrix, A - coefficien matrix, Y - samples matrix) (multiplying by V inverse i.e. V^(-1) both sides) => V^(-1).V.A = V^(-1).Y => A = V^(-1).Y So to go from samples matrix to coefficient matrix we need to do V^(-1).Y right ??"
"There's a lot behind that question that we'll go into later this week.
But that's an example of, if I do it in a silly way, I stick something in the middle of a list and I have to move everything.
That's an operation that could take linear time.
So linear time is a type of function.
We've got a number of these.
I'm going to start with this one.
Does anyone know this one is? Constant time-- basically, no matter how I change the input, the amount of time this running time-- the performance of my algorithm takes, it doesn't really depend on that.
The next one up is something like this.
This is logarithmic time.
We have data n, which is linear, and log n.
Sometimes we call this log linear, but we usually just say n log n.
We have a quadratic running time.
In general, if I have a constant power up here, it's n to the c for some constant.
This is what we call polynomial time, as long as c is some constant.
And this right here is what we mean by efficient, in this class, usually.
In other classes, when you have big data sets, maybe this is efficient.","Can you please if following this playlist will tech algorithm and data structure? Or is not enough?
i forgot, what does prime mean? specifically when he says K prime
Is this course related to data structures...?
why does he use k prime instead of k?
I believe an algorithm is not a function as functions have predefined output based on a set of sequential operations. It may be made up of multiple functions but Algorithm is more closely related to the technique to derive a function or procedure to find a solution to the problem.
The definition of the problem is a mathematical relation, but it is not necessarily a function since the problem may have many correct outputs for a given input"
"And all of those are interventions, and all those interventions are recorded in the data so that one could then ask counterfactual questions from the data, like what would have happened if this patient had they received a different set of interventions? Would we have prolonged their life, for example? And so in an intensive care unit setting, most of the questions that we want to ask about, not all, but many of them are about dynamic treatments because it's not just a single treatment but really about a service sequence of treatments responding to the current patient condition.
And so that's where we'll really start to get into that material next week, not in today's lecture.
Yep? AUDIENCE: How do you make sure that your f function really learned from the relationship between T and the outcome? DAVID SONTAG: That's a phenomenal question.",Where does the counterfactual data come from?
"So it's to print ""person:"", their name, and then, their age.
p1.speak() just says ""hello."" And then, the age difference between p1 and p2 is just 5.
So that's just subtracting and then printing that out to the screen.
OK, so that's my person.
Let's add another class.
This class is going to be a student, and it's going to be a subclass of Person.
Since it's a subclass of Person, it's going to-- a student is going inherit all the attributes of a person, and therefore, all the attributes of an animal.
The __init__ method of a student is going to be a little different from the one of Person.
We're going to give it a name, an age, and a major.
Notice we're using default arguments here.
So if I create a student without giving it a major, the major is going to be set to None originally.
Once again, this line here, Person.
init (self, name, age), tells Python, hey, you already know how to initialize a person for me with this name and this age.
So can you just do that? And Python says, yes, I can do that for you.
And so that saves you, maybe, like five lines of code just by calling the __init__ method that you've already written through Person, OK? So Student has been initialized to be a person.
And additionally, we're going to set another data attribute for the student to be the major.
And we're going to set the major to be None.
The student is going to get this setter here, this setter method, which is going to change the major to whatever else they want if they want to change it.
And then, I'm going to override the speak() method.
So the speak method for the person, recall, just said ""hello."" A student is going to be a little bit more complex.
I'm going to use the fact that someone created this random class, OK? So this is where we can write more interesting code by reusing code that other people have written.","What's that end variable in Student class code when she was running exercise on it.
why do we have a self.name=None? what does it do?
Excuse me but I don't quite understand why the _eq_ method will be called over and over again when comparing objects directly. Thanks in advance~~^^
Can I call a class' method w/o making an instance of it? For example, Anna is using a method of 'random' class which she imported w/o instancing.
Hi, Why with the class cat(Animal) example do you leave out the _init_ statement, but with the person (animal) example do you include the _init_ statement of animal.__init__ etc....? If the cat example inherits name, age etc.. of animal class then why do you have to write Animal.__init__(self, age)? Won't the person class inherit this without the statement and if not what makes it different to the cat class? Thank you"
"That's the flow across the cut.
So what I can do now is just talk about-- let's just go up here back to this.
And I'm going to look at exactly what I have here.
Is that right? Not exactly.
I'm going to change this a little bit, because I want to make sure I don't have to add up numbers and do that incorrectly.
So I need a 1.
Yup.
That's all I need to do is change it.
So I'm going to change our example here, not the topology of the example, but the actual numbers.
And you'll need to verify that what I have here satisfies our flow network properties.
And there's one more.",That's the proof I come to as well when I try to come up with the proof on my own. Any idea why he uses a different method?
"Now, an intuitive way to justify this is you think about it the way that induction works is you start off at 0, and then you make a step to 1, and you make another step to 2, and you make another step to 3, and the induction step going from n to n plus 1 justifies each of those steps.
By the time you get to n and you have to prove you can get to n plus 1, you've already been through 0 1 up to n.
You might as well take advantage of that fact.
Instead of only remembering that you're at the n step, you might as well remember that you got there.
That's an intuitive hand-wavy argument which can be justified formally in a way that will emerge in the next segment.
So let's hold off on that and just bite the bullet and accept this as a basic principle of math that we're going to live with and use.
As an application of it, let's prove something that we've already proved by well ordering and, in fact, strong induction and well ordering are closely related, as we'll also discuss later.
So let's prove that using $0.03 and $0.05 stamps that you can get any amount of postage greater than or equal to $0.08 stamps, and I'm going to prove this by strong induction, with the induction hypothesis P of n that says I can form n plus $0.8 stamps.","Hey, This is for those who don't get it, like I did ... Why do we use strong induction in postage example? Because we need to prove not only the base case (like in induction), but + two cases after it. Why? If we able to get three consecutive numbers (e.g. 8<-base case, 9, 10) we can get any number more then 8, adding 3 for every of particular cases. 8+3 = 11, 9+3 = 12, 10+3 = 13 and so on 11+3 = 14, 12+3=15, 13+3 = 16. Now I have 9 numbers (8,9,10,11,12,13,14,15,16). And I can do it infinitely and get all the numbers >=8. Formula for making P(n) = 0+8 cents depends on the making P(n-3) cents, P(n+1) depends on the making P(n-2). P(n+1) = P(n-2) + 3. We have to directly prove three BS for 8, 9 and 10 (P(0), P(1) and P(2)). We must construct it using only 3s and 5s. And I can definitely do it: P(0) = 8 is one 5 and one 3, P(1) = 9 is three 3s, P(2) = 10 is two 5s. We have shown how to get cents for every value of P(n) from 0 to some m, where 0<=m<=n (in other words how to get values from 8 to ... 10). Next we need to prove that our P works for m+1 case. We know that the biggest base case P(2) = 10, then our P(m+1) >= 11. Subtract 3 cents for each side P(m+1)-3 >= 8 . We know how to construct 8 with some amount of 3s and 5s. P(m+1) - 3 = let say a*3+b*5 (where a and b are non negative ints). Then P(m+1) = a*3+3+b*5 or (a+1)*3+b*5 is also combination of 3s and 5s. It's what we needed to prove. And we have proved it. I'm not afraid to take P((m+1)+1), because I have the second base case proved P(1) = 9.
Why the base case is zero while the axiom only when greater than or equal eight
Ive done induction on this particular problem. I know it as the ""Coin problem"". They give us the two coin denominations first, then the Frobenius Number which claims they can make n cents out of blah cent coins and blah cent coins for all K greater than or equal to n. Example: Prove with induction that with 8 cent and 3 cent coins, for all ""n"" cents greater than or equal to 14 can be paid with 8 and 3 cent coins. I was taught my base case for this will be the trivial case of where n = 14, how it can be paid with 8 and 3 cents i.e 14 = 8a+3b. Not this P(0) as you point out. My hypothesis would be that P(k) is true such that K can be paid in 8 and 3 cent coins where K is greater than or equal to 14 and in the step prove P(K+1) in the step with cases. For this specific example, My cases would be the following: Case 1: I have at LEAST FIVE 3-cent coins. I can take out all five 3-cent coins and put in 2 8-cent coins. Case 2: I have at least FOUR 8-cent coins. I remove all four 8-cent coins and put in 11 3-cent coins. Case 3: I have at most FOUR 3-cent coins and i have at most THREE 8-cent coins. But in that case, that doesnt get me what I'm trying to prove so that can't happen."
"And in the L_1 soft margin SVM we're going to relax this.
We're gonna say that this needs to be bigger than 1 minus c.
There's a Greek alphabet C.
Um, and then we're gonna modify the cost function as follows.
[NOISE] Where these c I's are greater than or equal to 0.
Okay.
So remember, um, if the function margin is greater or equal to 0, it means the algorithm is classifying that example correctly, right? So long as this thing is getting 0, then, you know, y and this thing will have the same sign either both positive or both negative.","Question for the kind hearted people who understood: in the geometric margin, y is either 1 or -1 right? But wasnt it mentioned before that y is either 1 or 0? I got a little messed up on that.
What is that that weird symbol he wrote during L1 soft margin svm? He called it c-i , i guess"
"[NOISE] And then you return Theta transpose x, right? So you fit a straight line and then, you know, if you want to make a prediction at this value x you then return say the transpose x.
For locally weighted regression, um, you do something slightly different.
Which is if this is the value of x and you want to make a prediction around that value of x.
What you do is you look in a lo- local neighborhood at the training examples close to that point x where you want to make a prediction.
And then, um, I'll describe this informally for now but we'll- we'll formalize this in math for the second.
Um, but focusing mainly on these examples and, you know, looking a little bit at further all the examples.
But really focusing mainly on these examples, you try to fit a straight line like that, focusing on the training examples that are close to where you want to make a prediction.
And by close I mean the values are similar, uh, on the x axis.
The x values are similar.
And then to actually make a prediction, you will, uh, use this green line that you just fit to make a prediction at that value of x, okay? Now if you want to make a prediction at a different point.
Um, let's say that, you know, the user now says, ""Hey, make a prediction for this point."" Then what you would do is you focus on this local area, kinda look at those points.
Um, and when I say focus say, you know, put most of the weights on these points but you kinda take a glance at the points further away, but mostly the attention is on these for the straight line to that, and then you use that straight line to make a prediction, okay.
Um, and so to formalize this in locally weighted regression, um, you will fit Theta to minimize a modified cost function [NOISE] Where wi is a weight function.","can anyone explain me where that x came from in the final equation of gradient ascent
I have a question about locally weighted regression. Imagine we want to calculate studentized residual. we have different hat matrix (projection matrix) for each observation and each hat matrix is a matrix (k by k) which k is a number of the observation in the span. Now I would like to calculate the leverage. I would like to know how to determine leverage for each observation?
then what is the difference between the locally weighted regression and polynomial regression? in application"
"ERIK DEMAINE: I'd like to divide and conquer.
Maybe find the minimum weight over here, minimum weight over here.
Of course, I don't know which nodes are in what side.
So that's a little trickier.
But what do I do but E itself? Let's start with that.
Yeah.
AUDIENCE: You remove it? ERIK DEMAINE: You could remove it.
That's a good idea.
Doesn't work, but worth a Frisbee nonetheless.
If I delete this edge, one problem is maybe none of these red edges exist and then my graph is disconnected.
Well, maybe that's actually a good case.
That probably would be a good case.
Then I know how to divide and conquer.","if you delete edge e (and the components are still connected), you are in essence creating a new MST with a min red edge"
"But I'll let you guys piece through that at home.
It's essentially the same argument.
And instead, we should jump to an algorithm that actually matters, which is something called merge sort.
How many of us have encountered merge sort before? Fabulous.
Good.
So then I'm done.
So let's say that I have a list.
Now, I'm sending a message back to Jason.
I made this one up last night.
So I have 7, 1, 5, 6, 2, 4, 9, 3.
This is not in sorted order.
But I can make a very deep observation, which is that every number by itself is in sorted order if I think of it as an array of length 1.",why are we doing comparison/merges in reverse order for selection and merge sort? what's the point?
"Yeah.
Could we also sum over all the negative examples using the triplet loss? Yeah, so the question was, can we also sum over all the negatives using the triplet loss? This actually ends up being very, very similar to doing that.
So if you actually write this out with the log, the numerator will become log of e to the negative distance.
And so-- or negative log of e to the negative distance.
And so this actually just becomes d of z and z plus.
And then with the denominator, you get something like plus log of sum of e to the negative distance of z and z minus.
And so you have a log sum x up here.
But if you basically think of the log and the e canceling out, this is basically just like summing up the distances there.
Yeah.
So I didn't get that.
Can you basically explain what you're trying to achieve here? Yeah, so the question was just-- the question was, can we just basically take the triplet loss and just add up all the negatives? And what I was simply saying is that this loss function is actually already very similar to the triplet loss but where you sum over the negatives here.
And so the question is, why don't we just do something like this? And this loss is actually already doing something a lot like that, except instead of something, we're going to do a log sum x.","Also, why did the professor use a negative sign for the distance? The paper clearly doesnt do that. I think this is corrected but it makes it confusing"
"This is just another way of looking at Horn clauses.
So going back here, right, so we have A implies C, how can we write it, we can write it as negation of A or C.
We have A and B implying C, right, what does that equal to? Its negation of this first part that can use De Morgan's Law and that gives me negation of A or negation of B or C.",how did you go from A -> (B V C) to ~A V B V C? wouldn't it be ~A V (B V C) ?
"So this is a new bipartite graph, where I'm using S for the left vertices and E of S for the right vertices.
And here, I'm using the complement of S for the left vertices and the complement of E of S for the right vertices.
And the previous lemma says that if there's no bottlenecks overall, there's no bottlenecks in either of these two restricted graphs.
So what I can do now is, by induction, since there's no bottlenecks in S, E of S, I can find a match there.
So I can match up all of the girls in S with compatible boys in E of S.
And I can match up all of the girls that are not in S, in the complement of S, with boys that I haven't used already.
And there will be a perfect match there.
And then these two separately will provide a match for the entire set of left vertices-- S union the complement of S will be all of the vertices.
And these two matchings don't overlap.
So combining them gives a complete matching.
That knocks off this case.
That's the nice case, where I can find some subset with the number of boys is exactly equal to the number of girls, which means that that has to work by itself.
And I worked that one by itself and what's left over by itself.
And that's how I can find an overall match.",What is a bottleneck?
"So in particular, at initialization, anything not reachable from s is set correctly.
And s itself is set as an upper bound to the shortest path distance.
Now we're going to process each vertex u in a topological sort order.
So remember, our input to DAG relaxation is a DAG.
So this thing has a topological sort order.
We're going to process these vertices in that order.
You can imagine we're starting at the top, and all my vertices are-- all my edges are pointed away from me.
And I'm just processing each vertex down this topological sort line.
And then for each of these vertices, what am I going to do? I'm going to look at all the outgoing neighbors.
And if the triangle inequality is violated, I'm going to relax that edge.
The algorithm is as simple as that.
For each outgoing neighbor of v-- sorry, of u-- I always get u and v mixed up here.
If my shortest path estimate to v violates the triangle inequality for an edge, for an incoming edge, then I'm going to set-- relax u, v, i.e.
set d, s,v, equal to d, s,u, plus w, u,v.
So that's the algorithm.
So if I were to take a look at this example graph over here, maybe a is my start vertex.","great lecture, thanks mit! I have a doubt, in the dag relaxation algo we init the distance estimate from the source to the source equal to 0, but the professor said that this value could negative or -infinity, however that can occur when the graph has a negative cycle, given that the graph is a DAG then the shortest path distance from the source to the source has to be 0 because DAG has no cycles
d(s,u) isn't infinity, because you're starting from d(s,s), which is 0. Then you get to the next node, say t. d(s,t) = infinite > d(s, s) + w(s, t), so d(s, t) = d(s, s) + w(s, t). So you need a starting point, and that starting point is the distance estimate from your source node to your source node. I think the reason you can't just say delta(s, u) is because you're not certain yet that it's actually the shortest path. Only at the end, when the algorithm 'converges', and the triangle inequality holds for all vertices, can you actually say that your d's (your estimates) are the deltas (shortest paths)
but I think d(s,v)>delta(s,u)+w(u,v) can be correct if vertex u comes before v in topological order.because d(s,u) is the same as delta(s,u) as there is no subroute that can reach vertex u from vertex s going through any vertex that come after u in the topological order. am I right?"
"And I'm defined by the problem statement that I'm only going to have even inputs.
And I'm going to set, at first, my a to be the starting place.
I'm going to just have a little temporary variable that's going to say this is going to be equal to the head of my list.
And what I'm going to do is what your colleague was saying-- is I'm just going to loop through n times until I reach the n-th thing.
Actually, how many times do I have to travel through next pointers to get to node a? n minus 1, actually-- yep.",The last problem works the same for lists with a size of an odd number?
"And they see this confidence interval and say, what does that really mean? Most people don't know.
But it does have a very precise meaning, and this is it.
How do we compute confidence intervals? Most of the time we compute them using something called the empirical rule.
Under some assumptions, which I'll get to a little bit later, the empirical rule says that if I take the data, find the mean, compute the standard deviation as we've just seen, 68% of the data will be within one standard deviation in front of or behind the mean.
Within one standard deviation of the mean.
95% will be within 1.96 standard deviations.
And that's what people usually use.
Usually when people talk about confidence intervals, they're talking about the 95% confidence interval.
And they use this 1.6 number.
And 99.7% of the data will be within three standard deviations.
So you can see if you are outside the third standard deviation, you are a pretty rare bird, for better or worse depending upon which side.
All right, so let's apply the empirical rule to our roulette game.
So I've got my three roulette games as before.
I'm going to run a simple simulation.
And the key thing to notice is really this print statement here.","Thanks Professor. I have a doubt. With a mathematical model, I have estimated aircraft parameters using the extended Kalman filter. Now I have to verify whether the proposed system is robust against uncertainties. I have estimated data and true sensor data. How can I do Monte Carlo analysis with this information? I am not using any physical model of the aircraft but I am using Kinematic relationships of the aircraft i.e Kinematic model. How to perform the Monte Carlo analysis of my FDD algorithm using MS-excel or matlab? Could you please reply me out on this?
Sir, is their way to account actually experimented pattern for this random event simulation ? (like some numbers are more likely to result in this gambling event because of the biasness of the roulette). Thank you
So we cannot calculate confidence interval when empirical rule does not hold?
Can someone explain when he calculated the confidence interval on the mean, why did he use the empirical rule instead of the central limit theorem? Since he has 20 means shouldnt he have divided z,alpha/2*sigma by sqrt(20)?
The computer program is no really random as the random samples are generated using psuedo-random generators.
I understood almost everything except for assumptions underlying emprical rule. It says the mean estimation error should be 0. What is the mean estimation error ? Does he mean standart error of the mean?
Hello, thank you for your lecture on Monte Carlo. I have a question, I dont understand why it is better to run the simulation rather than just use the mean? For the roulette example, were providing the code the 1/36 probability and using a simulation to show that well win 1/36 times. But of course we know that already, and so why run the simulation? My question related to why Im researching Monte Carlo. I have a real world problem where I am trying to forecast unplanned shutdowns for machinery. I have a detailed history of unplanned shutdowns for the past 10 years. Say I have 1005 machines and I have on average 7 breakdowns per month. Can I use Monte Carlo to forecast how many machines will breakdown per month going forward? Or should I just assume Ill have 7 breakdowns per month? What would be the benefit of each? Im happy to self learn, Im hoping someone can point me in the right direction. Thank yoy"
"What I'm going to talk about the original Super Mario Brothers, NES classic which I grew up with.
Now the real Super Mario Brothers is on a 320 by 240 screen.
It's a little bit small.
Once you go right, you can't go back left, except in the maze levels anyway.
So I need to generalize a little bit.
Because if you assume that the screen size of Super Mario Brothers is constant, in fact you can dynamic program your way through and find the optimal solution in polynomial time.
So I need to generalize a little bit to arbitrary board size, arbitrary screen size.
So in fact, my entire level will be in one screen, no scrolling.","Wait, so you are proving not that the game super mario is NP-hard but that you can construct NP-hard levels in mario? The fact that people have said Super Mario is NP-hard has always confused me."
"Also looks intimidating, but it's not so bad.
So we're initializing a bunch of stuff up here.
The most important couple of things we're initializing are, first of all, this high and this low boundaries.
So with the guessing game, the low boundary was 0, and the high boundary was 100.
When we're looking at the cube root, the low boundary is going to be 0, and the high boundary is going to be just my cube, because a guess to the power of 3 cannot be any greater than cube.
And then, I'm just going to do the same procedure that I did with the guessing game, which is I'm going to make my guess, be halfway in between.
So with this guessing game, I had to sort of choose, if there were four numbers in between, should I go higher or lower? Well, when we're doing by bisection search, here, we don't care about that.
We're just going to do floating point division, because we want decimal numbers.
So I have a low boundary and a high boundary.
And I figured out my halfway point.
Then I have this while loop here.
A while loop is similar to the approximation method, where, as long as I don't have a guest that's good enough-- so this, depicted by this greater or equal to epsilon-- as long as my guess is not good enough, I'm going to keep guessing.
That's what this while loop is saying.
So if the guess to the power of 3 minus cube is not good enough, keep guessing.","can someone explain for me what is the meaning of iteration in programing in a simple way
I though it was called binary search why it's called bisection in here?
Cube root simple guess. Why does that code keeps running?
what about the other challenge (if the cube are negatives)?
Can anybody explain me in Assignment 1 part C of the downpayment problem(The one where we calculate ideal monthly savings rate) Why am I getting an Infinite loop. I followed all instructions. Help would be appreciated. annual_salary = float(input(""Please enter your yearly salary: "")) monthly = annual_salary/12 portion_saved = 0.0 cost = float(input(""Please enter the cost of your dream home: "")) down_payment = 0.25*cost semi = float(input(""Decimal rate of increase of semi annual salary: "")) low = 0.0 high = 1.0 x = (low+high)/2 r = float(input(""Rate of interest yearly for savings:"")) current_savings = 0.0 month = 0 while abs(current_savings - down_payment)>= 100.0 : portion_saved = x*monthly while month <= 36: if month%6 == 0 and month != 0: portion_saved = (1 + semi)*portion_saved current_savings += r*current_savings/12 + portion_saved if current_savings < down_payment: low = x else: high = x x = (low + high)/2 print(x) print(""Your saving rate should be: "", x)
and guess**3<=cube is better choice ?
What I get from the approximation algorithm is that it will only give you an answer if the difference between guess**3 and cube is equal to epsilon, if it manages to jump above epsilon, guess will increase in value until it is <= cube, and thus the message ""failed to obtain a value"" will appear. How can we make it so that if the difference does jump above epsilon that the value will get will be the value of guess at time - 1 of that moment.
For bisection I wanted to use this code to take direct input, but error is coming. #To find the cube root of a number using bisection method cube=float(input(""Put the number... "")) accuracy=float(input(""Give accuracy limit... "") low=0.0 high=cube guess=((high+low)*0.5) iteration=0 while abs(guess**3.0-cube)>accuracy: if (guess**3.0)<=cube: low=guess else: high=guess guess=((high+low)*0.5) iteration += 1 print (guess, ""is close to the cube root of"", cube) print (""Total iteration="", iteration) **************************** syntax error in low=0 why?
Love it. Dr. Bell, I hope you're still at this: you do it really well, and as I've watched from the first lecture, you're losing (or taming?) what seems to be a little stage-fright. I've never lost mine, but I did tame it enough to teach by interactive lectures way back when. Good onya. :-) One question: the ""bisection"" method seems just like (electronics engineering technology 35 years ago) what we called ""successive approximation by halves""... and we did it with logic-gate constructs and not code... but is that the same idea?
can anyone tell me what am i doing wrong!! code: cube = int(input('type the no....')) for guess in range(cube+1): if guess**3 == cube: print(""Cube root of"", cube, ""is"", guess) else: print(""cube root does'nt exist"") output:- type the no....1(#1 was the input) cube root does'nt exist Cube root of 1 is 1
How do you do a bisection search for a cuberoot when x is a decimal? I cant be find an answer anywhere.
I know she said it is a simple if statement but I can't figure it out. What did you do to make it work? My thought is: if cube < 0: guess = -guess
What is the if statement for negative values in the bisection search
what is the meaning of epsilon here ???
Any idea on the code for a negative integer? Tried a handful of if statements, but it kept turning my while loop into an infinite loop."
"These days, that has a very different meaning.
But back in the day, AI was all about solving board games, and Rubik's cubes, and all these kinds of things, using algorithms.
And the way that they would do that is by searching the different spaces of configurations.
And so now, if we think of every face of this cube as painted with a color, there are different configurations of my graph that I get by flipping the three sides.",what if the graph is a 3D graph?
"And then I want to multiply these spaces, meaning the number of subproblems is going to be the product of the number of suffixes here times the number of suffixes here.
And in other words, every subproblem in LCS is going to be a pair of suffixes.
So let me write that down.
So for LCS, our subproblems are L of i, j-- this is going to be the longest common subsequence-- of the suffix of A starting at i and the suffix of B starting at j.
And just to be clear how many there are, I'll give the ranges for i and j-- not going to assume the sequences are the same length, like in the example.
So I'll write lengths of A and lengths of B.
I like to include the empty suffix.
So when j equals the length of B that's 0 items in it, because that makes for really easy base cases.
So I'd like to include those in my problems.
So that was the S in SRTBOT.
Now I claim that set subproblems is enough to do a relation-- recursive relation among them.
So I'd like to solve every subproblems L i, j.
Relation is actually pretty simple, but it's maybe not so obvious.",Is the topology reversed when you go from top down vs bottom up? I'm a bit confused by the topology. Are suffix subproblems always have topology of decreasing i ?
"I'm going to look at some set of vertices.
S here is a subset of the vertices, and that leaves in the graph, everything else.
This would be V minus S.
OK, so there's some vertices over here, some vertices over here, there's some edges that are purely inside one side of the cut.
And then what I'm interested in are the edges that cross the cut.
OK, whatever they look like, these edges.
If an edge has one vertex in V and one vertex not in V, I call that edge a crossing edge.
OK, so let's suppose that e is a least-weight edge crossing the cut.
So let's say, let me be specific, if e is uv, then I want one of the endpoints, let's u, to be in S, and I want the other one to be not in S, so it's in capital V minus S.","in what situation is w(T') < w(T* - e)? Isn't w(T') = w(T* - e)?
he says a crossing edge is defined by whether or not it has a vertex in V and the other vertex not in V. Did he mean to say a vertex is in S and the other is not in S?
onwards, if an edge(red) from 1st part ends in 2nd part, then the graph would not be an MST...but we have already supposed it to be a MST with e being an edge.... Please clarify this doubt....!
what if u and u' are connected by a path that includes e' ?"
"And when you run that program, you get an answer that's very similar to the program that we looked at previously, the imperative program.
Except that the answer now is a list of functions.
So rather than generating a text-string like the previous example did, this program generates a list as the answer-- the program generates a list, which is a list of functions.
The first element in the list is increment, the second element in the list is the function increment, the third element is square increment square.
So it gave you the same answer, it just represented it differently.
Here I'm representing the answer as a list of functions.
That make sense? So there's a number of advantages to this approach.
One of them is that it kind of automatically coerces you to think about modules.
By specifically focusing my attention on, how could I break up that problem by defining functions? I end up with a bunch of functions here, apply and addLevel.",Why should you focus on procedures that minimise mathematical functions?
"So we're not going to go into a whole lot of algebra with these things.
Just what we need in order to go through that network.
So the next thing we need to deal with is conditional probability.
And whereas those are axioms, this is a definition.
We say that the probability of a given b is equal to, by definition, the probability of a and b.
I'm using that common notation to mean [INAUDIBLE] as is conventional in the field.
And then we're going to divide that by the probability of B.
You can take that as a definition, and then it's just a little bit of mysterious algebra.
Or you could do like we did up there and take an intuitionist approach and ask what that stuff means in terms of a circle diagram and some sort of space.
And let's see, what does that mean? It means that we're trying to restrict the probability of a to those circumstances where b is known to be so.
And we're going to say that-- we've got this part here, and then we've got the intersection of a with b.",What does a hack in this context even mean ?
"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.
ERIK DEMAINE: All right, let's get started.
I am Erik Demaine.
You can call me Erik.
Today we're going to do another divide and conquer algorithm called the fast Fourier transform.
It's probably the most taught algorithm at MIT.
It's used in all sorts of contexts, especially digital signal processing like MP3 compression, and all sorts of things.
But we're going to think about it today in the context of divide and conquer and polynomials.
So let me remind you-- I mean, this class is all about polynomial time, but usually with polynomial time we only care about the lead term.
Today-- and today only, pretty much-- we're going to be thinking about all the terms in a polynomial.
So I'll talk about polynomials mostly a and b.
You have a constant term, and then a linear term, and a quadratic term, and so on up to-- I will say that there are n terms-- which means the last one is a n minus 1.
Normally the degree the polynomial here is n minus 1.
I wish the degree was defined to be n here, but whatever.
That's-- I'm not-- I can't change the definitions in algebra.
So this is the traditional algebraic way of thinking about a polynomial.
Of course, you can write it with summation notation.
akx to the k, k equals 0 to n minus 1.
We'll jump back and forth between them.
I'm also going to introduce a vector notation for polynomials because-- so the ai's are real numbers, typically.","How does one perform FFT on a larger domain consisting of multiple cosets of a multiplicative subgroup of the field? I've heard it can be done but couldn't find any sources that explained how.
Okay, we've figured out how to convert between different representations of polynomials, but how do we go from there to the familiar application of the FFT - converting between the time domain and frequency domain? Given a bunch of samples, we want a weighted sum of sinusoids, but what we get here is the coefficients of a polynomial.
So is the Nfft value for the FFT function in the matlab signal analyzer app the same as the 'n value rounded to the next largest power of 2' he talks about in the video?"
"And since 4 has a higher weight than 3, I'd pick that first.
And then 3 would have to go over here.
So if I go do the math, the cost is going to be 1 times 2-- and I'll explain what these numbers are.
I did tell you it was going to get a little cluttered here with lots of numbers.
But if you keep that equation in mind, then this should all work out.
So what do I have here? I'm just computing-- this is W1.
So you see the first numbers in each of these are the weights.
And so this would be W2 and et cetera.
And you can see that this is at depth 1, which means I have to have 2 here, because I'm adding 1 to the depth.","It is very subtle, indeed. As the instructor said, for e(i,j) you add wi to wj once. For the next depth, when you compute e(i, r -1) you again add wi to w(r-1) so in total you add wi to w(r-1) twice, which corresponds to the increasing depth. The same thing goes for e(r+1, j). you add w(r+1) to wj once again for this calculation and in total w(r+1) to wj is added twice. But wr is added only once since it remained in the upper level (depth) and not included both e(i, r-1) and e(r+1, j)"
"So I'll only ever get to this if in fact I couldn't find anything here.
And so it's an appropriate way to simply raise the error to say, if I get to this point, couldn't find it, raise an error to say the node's not there.
The last piece looks a little funky, Although you may have seen this.
I like to print out information about a graph.
And I made a choice, which is, I'm going to print out all of the links in the graph.
So I'm going to set up a string initially here that's empty.
And then I'm going to loop over every key in the dictionary, every node in the graph.
And for each one, I'm going to look at all the destinations.
So notice, I take the dictionary, I look up the things at that point.
That's a list.
I loop over that.
And I'm just going to add in to result, the name of the source, an arrow, and the name of the destination followed by a carriage return.
I'll show you an example in a second.
But I'm simply walking down the graph, saying for each source, what can it reach? I'll print them all out.","Have I missed something or he didn't defined printPath anywhere?
Theres one issue with the code that is given. Nowhere in the lecture notes or in the video defines the printPath() function. Also how does he print out in that format when the only way to do it is by calling on the Edge class method to print? especially when he is appending nodes and not edges. I am guessing it is done in the printPath() function"
"Then also, what's the cost prior, what's the prior probability that the next e-mail you receive in your, uh, in your- in your inbox is spam e-mail? And so to fit the parameters of this model, you would s- similar to Gaussian discriminant analysis, write down the John- joint likelihood.
So the joint likelihood of these parameters, right? Is a product, you know, given these parameters, right? Similar to what we had for Gaussian discriminant analysis.
And the maximum likelihood estimates, um, if you take this, take logs, take derivatives, set derivatives to 0, solve for the values that maximize this, you find that the maximum likelihood estimates of the parameters are, Phi_y, this is pretty much what you'd expect, right? It's just a fraction of spam e-mails and, uh, Phi of j given y equals 1 is, um, well, I'll write this out in indicator function notation.","Phi is the probability that he also denotes as p(y). This is by definition the probability that a new person will be positive, given no further information, so given no data. Basically, it is the best guess you have for whether a random person is positive, before you know anything about the person. It makes sense that no further information on the individual your best guess is just the ratio in the general population. He is trying to clarify this in his sentence: ""What is the chance that the NEXT patient that walks into your office has a malignant tumor?"". The key word is next, so this is happening in the future and you can't have any data yet on that person, but I agree it's misunderstandable. The probability that given the features the person is positive is denoted as p(y|x). He gives the way how to calculate this earlier in the lecture. Unsuprisingly, this does depend on the data x. So if you have features, you do consider them to make your prediction, which is p(y|x)."
"ASON KU: I'm Jason.
Hopefully you guys saw me on Tuesday.
This is our first ever 6.006 problem session that we'll be having on Fridays this term.
It's really an experiment.
We've never done this before.
But one of the things that we were discussed while preparing for this class is that we have two different methods of instruction, formally, usually in this class-- a lecture, which is there to present you with the fundamental material, the data structures, and the algorithms that are the base, the foundation of what-- how you will be approaching problems in this class; and then the problem sets that you will work on, our applications of that material.
But there's usually a much different feel between those problems that we'll give you then the underlying foundational material.","Great lecture, but i don't understand problem 3.. can anyone explain it to me
is the 2011 version and this 2020 version talk about the same topics? Are they more than 90% overlap, so its enough to watch only one of them, or the topics were modified for the 2020 version compared to the 2011 sessions?"
"And uh, this is essentially, uh, the PageRank, uh, equation and iteration one can run.
Uh, just note that this formulation here assumes M has no dead ends.
The way you can do is you can pre-process matrix M to remove all the dead ends, um, and or- or explicitly follow random teleports with probability 1.0 out of dead-ends.
So that's how you can, uh, fix this.
But you can see again, this is very fast and very simple, uh, to iterate.
So I just gave you the equation in this, um, the, uh, flow-based formulation in some sense.
You can also write it in a matrix form, where you say my new matrix, uh, uh, G, right? So this should be G equals Beta times the stochastic matrix M plus 1 minus Beta, um, times the, uh, the matrix that has all the entries, uh, 1 over N.","I have a question. Before adding teleport, the original matrix is super sparse, which makes computation cost affordable. However, after adding the teleport part, the matrix becomes dense. How do we solve the issue?
If M has more than 1 linearly independent eigenvectors with eigenvalue 1, the power iteration may not converge; there is more than one solution to the flow equation. One can assume it's not the case; it's interesting why.
According to the slides, we solve the rank vector r via Power Iteration, we don't use sparse properties of the matrix."
"Let's look at an example.
I'm going to define a set E that's a subset of the integers, and I'm going to give you a recursive definition of E.
The base case is that I'm going to tell you that 0 is an E and I'm going to give you two constructors.
The first one says that if you have an n that's an E, you can add 2 to it and get a new element in E, providing that n is not negative.
The second constructor is that if you have an n that's an E, you can negate it.
You can take minus n, providing that n is positive, and those are the two constructor rules.
Well, let's look at what goes on here.
What is this telling us? Well, let's just use the first constructor rule and use it repeatedly.
I can start off with 0.
That's the base case, and then I can apply the constructor to add 2 to it.
Then I can apply the constructor again to add 2 to 0 plus 2, and then I can apply the third time to get add 2 to 0 plus 2 plus 2.
And it's clear what I'm getting is 0, 2, 4, 6, and so on, and I'm going to get all of the non-negative even numbers in that way.
Now, I can apply to these, the positive numbers, I can apply the negation constructor.
So I can get minus 2, minus 4, minus 6, and it becomes apparent then that I can get all of the even numbers.",Video Point https://youtu.be/TXNXT3oBROw?t=689. At this point was (pi/2) intended rather than (pi)? With some other adjustments?
"This is a somewhat interesting one.
Let's define the depth of a string as follows, and the idea is it's how deeply nested are the successive pairs of left and right brackets.
Well, the depth of the empty string is 0.
You got to start somewhere, and it's got no brackets, we'll call it depth 0.
Now, what about the depth of the constructor putting brackets around s and then following it by t? Well, putting brackets around s gives you a string that's 1 deeper than s is, and then you follow it by t, and it's as deep as t is.
So the result is that the depth of the constructor is a string which is a number which is equal to 1 plus the depth of s and the depth of t, whichever is larger.
The max of 1 plus depth of s and depth of t, and that's our recursive definition of depth.
Let's look at maybe another even more familiar example of recursive definition.
Let's define the nth power of an integer or real number k.
The zeroth power k is defined to be 1, and the n plus first power of k is defined to be k times the nth power of k, and this would be an executable definition of the exponentiation function in a lot of programming languages.
And my point here is that this familiar definition, recursive definition on a nonnegative integer n, is in fact a structural induction using the fact that the nonnegative integers can be defined recursively as follows.
0 is a nonnegative integer, and if n is a nonnegative integer, then n plus 1 is a nonnegative integer.","I dont understand the length vs depth proof. Suppose I have this string. [][][], the length is 6, and the depth is 1. 6 > 2^(1 + 1) = 4 I'm misunderstanding something here, could anyone enlighten me?
The way I understand it, by the definition of M, the added brackets for d(r) can only enclose s, so the depth being max(d(s)+1, d(t)) should be correct see this video: https://www.youtube.com/watch?v=TXNXT3oBROw
In general, you can have a very long sequence of depth 1: [][][][][][][][][][][][]..., so how is the length upper-bounded by any function of the depth?"
"Um, and the same algorithm works, only thing is how do we define the set S, the teleportation, uh, set.
So to summarize, uh, a graph can naturally be represented as a matrix.
We then define the random walk process over, er, the graph.
We have this notion of a random surfer moving across links, uh, with- er, together with having a-a way to teleport, uh, out of every node.
This defined- allowed us to define this stochastic adjacency matrix M that essentially tells us with what probability the random surfer is going to navigate to each edge.","S is one of the columns(rows) of the matrix that is multiplied by the (1-betta) coefficient from the previous lecture, isnt it? It is necessary to have N number of identical such columns so that the matrix P is stochastic.
How to initialize vector S in case of PPR and how do we choose the starting node in case if PR with restart?"
"It could be, you know, uh, uh, um, you know, it's a- some shape.
Um, and now we want to find a Theta that minimizes this cost function, um, to the smallest possible value? Yes, question.
How do we know that there's only one value of Theta that minimizes the cost function? Very good question.
So how do we know that, um, um, um, there's only one value of Theta that minimizes the cost function? In general, it need not.
Uh, if you, uh, the correct way to actually write this is, you know, Theta belongs to argmin where argmin gives you a set of minimizers.
There could be multiple values of Theta that minimize your, uh, minimize your function.
For example, um, imagine this to be the cost function, and your cost function has this shape.
So any Theta along this line is a minimizer.
It's not a bowl-shaped, it's just, you know, um, that- it's just that the minimum value can be obtained by multiple values of Theta along the line.
In, in such cases argmin is actually a set of Theta values.
But for- for the most part we'll be assuming that there is one unique, uh, minimizer.
Good question.
So, um, so r- our, um, our goal is to find the value of- of- of Theta where J theta is the smallest.
And for this, we're gonna use an algorithm called ""Gradient Descent."" Now, how does gradient descent work? So gradient descent, we start with a random initial value of Theta.
So we said, Theta- let's call it Theta-naught [NOISE] equal to, you know, some initialization.
A common initialization, is you just start with Theta-naught equal to 0, at the origin, right? Theta-naught.","Hello Professor Avati, Hope you are doing well! I have a small question While equating the determinant of J(teta) to zero, we assume that the value of 'teta' at which the equation is zero is the minima. But we haven't deciphered the shape of the function. This value could easily be maxima if the function had an inverse bowl shape."
"So we're going to create the string here.
It's initially an empty string.
I want to mention that the plus equals-- for example, if I say a plus equals 1.
This is equivalent to a is equal to a plus 1.
We've seen this a couple of times before-- last lecture and the last lecture before that.
So all this means here, this line here, is we're going to take the previous string that we have, and we're just going to add it to itself, plus the last letter.
So the first thing we're going to have is going to be new_str is equal to 2.
Then this line here is going to say now I'm going to add to the two that I already have, the element at position 0, which is a 6.","Wait, I thought strings were immutable...
I see, so assigning a different variable name to the same string ""is not"" referring to the same string in memory. It points to a ""new copy"" of the original string. (Edit: Redaction below) In the case of lists and dictionaries, assigning a new variable name to the old variable name points to the ""same list or value:key pair"" in memory? Thank you for clearing that up. [Edit: **See below comments for follow up redaction on this topic.**] Just to revisit Gabriel's question, I think the confusion is that you can take an empty string and assign index positions from a different string. But the act of assigning indexes from an ""old string"" to a ""new string"" is misleading and appears to be making changes or to mutate the new string."
"Over here we don't have numbers.
We just have these pieces.
So whenever you convert from a number problem to a non-number problem, if you're representing the numbers in unary, which is what's going on here, you need strong NP-hardness for it to work.
Weak NP-hardness isn't enough.
Then we get jigsaw puzzles, which we know and love, are NP-complete.
That's it.
","Reduction ... Time point https://youtu.be/eHZifpgyH_4?t=22m28s . Chalkboard writing .. then A element of P ... seems to disagree with the notes from the MIT site ... which is written A element of NP ... . Are the notes incorrect?
Whether something is in P or NP, that something must be a decision problem. It does not make sense to talk about whether an algo is in P or NP. For example, we can talk about the problem ""Is 73642387 a prime?"" and ask whether it's in P or NP. However, we can't talk about whether a MergeSort algo is in P or NP becus it's an algo, not a decision problem. Here's an explanation by Alon Amit regarding this confusion: https://www.quora.com/Is-finding-prime-numbers-in-P-or-NP Hope this helps :)
Not being rude, but can someone explain the applications of such mathematics? What does this course teach and why is it useful? Thanks!"
"In other words, we can get a diverse population as well as a fit population if, when we make our selection, we consider not only their fitness but how different they are from the individuals that have already been selected.
So that's going to be mechanism number three.
So now we have a space, and we can measure fitness along one axis-- ordinary fitness-- and this is rank space fitness, so that's going to be P sub c.
There will always be some individual with the highest fitness.
And over here-- that might not be P sub c, actually.","Hey guys if we remove the diversity factor at some instance i.e. generation then that generation could be perfect?????
This is very helpful for me. But I have a question. What is Pc ? And how much is it. I watch the screen ,find the rank probability is 0.05. (1-Pc) equals 0.95,so 0.95^39 always more than 0.05,if Pc equals 0.05. I think I need some help.
John David Deatherage How are you sure you won't take out the other top ones when you reduce the population?Newbie here
Wischenbart Christian By Pn i was referring to the last probability. Let's say there are 4 individuals in your population. If you choose Pc = 0.3, you would have: P1 = 0.3 P2 = 0.21 P3 = (1-Pc)^(3-1) = 0.7^2 = 0.49 Thus P3 > P2. In the general case, let K be the number of individuals in your population, and Pc < 0.5. Pk-1 = (1-Pc)^(n-2) * Pc and Pk = (1-Pc)^(n-1) = (1-Pc)^(n-2) * (1-Pc). Because Pc < 0.5 => (1-Pc) > 0.5 => Pk>Pk-1"
"You're given a graph.
You want a spanning tree of that graph.
It's going to be a tree that lives inside the graph.
So we're going to take some of the edges of G, make a tree out of them, make a connected acyclic graph.
And that tree should hit all the vertices in G.
So this is going to be a subset of the edges, or subgraph.
Those edges should form a tree.
And, I'll say, hit all vertices of G.
OK, if I just said they should form a tree, then I could say, well, I'll take no edges, and here's a tree with one vertex.
That's not very interesting.
You want a vertex-- you want, basically, the vertex set of the tree to be the same as the vertex set of the graph.
That's the spanning property.
But you still want it to be a tree, so you want it to be connected and you want it to be acyclic.
Now if G is disconnected, this is impossible.
And for that, you could define a spanning forest to be like a maximal thing like this, but we'll focus on the case here as G is connected.
That's the interesting case.
And so we can get a spanning tree.
All right? So what is this minimum spanning tree problem? Minimum spanning tree.
We're given a weighted graph, just like last time, with shortest paths.
We have an edge weight function W giving me a real number, say, for every edge.
And we want to find a spanning tree of minimum total weight.
So I'm going to define the weight of a tree T to be the sum over all edges in T, because I'm viewing a spanning tree as a set of edges, of the weight of that edge.","> tree = directed connected acyclic graph a ->- b ->- d -->- c ->- according to the definition this must be a tree, but it's not
If the edges have no weight than any search algorithm (dfs, bfs) will find a spanning tree
Graph is cyclic and spanning trees are not"
"So using global variables, you can be inside a function and then modify a variable that's defined outside of your function.
And that sort of defeats the purpose of functions and using them in writing these coherent modules that are separate.
That said, it might sometimes be useful to use global variables, as you'll see in a couple lectures from now.
OK cool.
So let's go on to the last scope example.
OK this slide is here, and notice I've bolded, underlined, and italicized the Python Tutor, because I find it extremely helpful.
So the Python Tutor-- as I've mentioned in one of the assignments-- it was actually developed by a grad student here, or post-grad student slash post-doc here.","so, x = 3 def f(y): print(x) works but def g(y): x =+ 1 does not work. because it's not in the functions scope. so are only operations with a variable out of scope not possible? because in either case it has to know the value of x, whether you print it or whether you do something with it. so why does the second one not work?
Is it bad style to use a variable from outside the scope of a function?"
"Remember De Morgan's law was the thing that said that the negation of P or Q was the same as not P and not Q.
And remembering that the connection between universal quantification, [? an ?] AND, and existential quantification, [? an ?] OR, it turns out that by the same kind of reasoning, De Morgan's law comes out this way.
It says that if it's not true that everything has property P, that's possible if and only if there's something that doesn't have property P.
And so that's what De Morgan's law is.
It's another thing you could take as an axiom, or you could try one of these hand-waving proofs about.
But I think I've said enough to give you that example of another interesting valid formula, and we'll stop with that.
","A thoughts on this. Taking the last example, why are we replacing x with y? This is done though out the examples but it is never explained why. I must be missing the reason but it seems just as understandable to me to say not(all x.p(x)) IFF exists x.not(P(x))"
"Reductions.
This is what you might call a one call reduction.
This is kind of a technicality.
Also called a Karp style reduction because Karp gave a whole bunch of them in the '70s, '80s.
So the idea is you only get to call your solution to be once.
In general, you could imagine an algorithm that calls your solution to be many times.
That would also be a notion of as hard as.
For the problems we'll look, basically, these two notions don't seem very helpful, let's say.
So we'll stick to one call reductions because they seem sufficient for everything that we will cover in this class.
Probably.
Maybe in some very late lecture, we'll talk about multi-call reductions.
But they're not so prominent.
One call reductions are the bread and butter of hardness.
So as you might imagine, this is how you prove a problem hard.
Basically, all hardness proofs in the known universe are based on a reduction.
You start from a problem which you know is hard in whatever class you care about, and you reduce from that problem, the known hard problem, to your problem that you're not sure about.","Nice video and good explanations! However, I believe reductions should lead to the result ""at least as hard as"" instead of ""as hard as""."
"Now we're going to represent them all in a matrix X, which is in our sequence length bidimensionality.
So sequence length by d, capital T by d.
And now if we have the matrix for each of our key, query, and value, right, we're going to apply, like we're going to look at these things XK, XQ and XV, which are all of the same dimensionality as x because of the d by d transformations.
So how do we compute self attention? We have our output tensor, which is the same dimensionality as the input x.
This is going to be equal to soft max, there's a soft-max of this matrix multiplication, which we'll get into, times the value vector.
So the matrix multiplication here is computing affinities between keys and queries we'll see, and then here's our averaging.
What does that look like pictorially? So you take the key, query dot products.
So this term here, XQ, XK transpose, is giving you all dot products, all T by T pairs of attention scores.
So our e, i, j are in this matrix right here, it's T by T and this is just a big matrix multiplication.","Good lecture, but I think there is some confusing usage of the K, Q, and V matrices. You introduce them by saying: (1) K*x_i = k_i, Q*x_i = q_i, V*x_i = v_i In this way you're saying that X, Q, and V operate on x_i column vectors from the left to yield another column vector. But then later, when you stack the transposes of the x_i to produce the X matrix for the tensor attention calculation, you multiple from the right with K: (2) softmax(XK(XQ)^T) = Attention weights I think both K and Q should be transposed in (2) as you're using them in order to be consistent with the way you define the matrix in (1).
Are matrices K, Q, V parameters to be learned?"
"So, the proof of that is based on the following idea.
Let's say that a number k prime is an inverse of k mod n.
If k prime times k is congruent to 1 mod n.
So, k prime is like 1 over k with respect to mod n.
But of course, 1 over k is going to be a fraction unless k is 1.
And so, k prime is going to be an integer that simply acts like 1 over k.
So, how are we going to prove this? And it's going to turn out to be an easy consequence of the fact that the gcd is a linear combination.
So, how am I going to prove-- find this k prime that's an inverse of k? Well remember, given the gcd of k and n is 1, I have a linear combination of k and n is 1.
So, s times k plus t times n is 1.
But if you stare at that for a moment, what that means is that k prime is simply the coefficient s of k.
So, all you have to do is apply the pulverizer to k and n to get the coefficient s of k in the linear combination of k and n is equal to 1.
Let's look at that slightly more carefully and see what's going on.
I have that sk plus tn is 1.
So, that means in particular, since they're equal, they're certainly congruent to each other, modulo n.
sk plus tn is congruent to 1 mod n.
But, n is congruent to 0 mod n.
So, this becomes t times 0, and we're left with sk congruent 1 mod n, which is exactly the definition of s being an inverse of k.",in step a.1 equivalent to b.1 how we omit (mod n). I would guess it to be a.1 (mod n) equivalent to b.1 (mod n) (mod n). please clarify.
"This is the same as taking online gradient descent.
I guess I haven't defined online gradient descent, but let me define that in a moment after I write down-- on a sequence of changing objective L g theta 0 up to L g theta t.
So what does online gradient descent really mean? It just really means that every time you take the gradient of the new function-- you have a sequence of functions.
And every time you get a new function and you take the gradient of that function, you take a one step.
So that's online gradient descent.
So basically, you are saying that taking gradient descent on this fixed function L hat is the same as taking gradients updates with respect to a sequence of changing functions.",Quick clarification if possible: what does he mean by the notation y^(i) (or x^(i) )? looks like the i-th derivative to me
"The other thing I want to do is to compare two rabbits.
So if I want to compare two rabbits, I want to make sure that their parents are the same.
So I can compare the first parent of the first rabbit with the first parent of the second rabbit and the second parent of the first rabbit to the second parent of second rabbit or getting the combinations of those two.
So that's what these two Booleans are doing.
So these are going to tell me-- these are going to be Boolean values, either True or False.
And I'm going to return either they have the same parents of that type or the same parents criss-crossed, OK? So here, notice that I'm actually comparing the IDs of the rabbits as opposed to the Rabbit objects directly, OK? So if, instead of comparing the IDs in here, I was comparing the parents themselves, directly, what would end up happening is this function, this method, eq(), would get called over and over again.
Because here, we have parents that are rabbits.
And at some point, the parents of the very, very first rabbits ever created by this program are None.
And so when I try to call-- when I try to call the parent one of None, that's going to give me an error, OK, something like an attribute error where None doesn't have this parent attribute, OK? So that's why I'm comparing IDs here, OK? And the code in the lecture here shows you some tests about whether rabbits have the same parents.
And I've created new rabbits here, r3 and r4, the addition of those two.
And r5 and r6 are going to have the same parents down here-- True-- but r4 and r6 don't, OK? So just to wrap it up, object-oriented programming is the idea of creating your own collections of data where you can organize the information in a very consistent manner.","Excuse me but I don't quite understand why the _eq_ method will be called over and over again when comparing objects directly. Thanks in advance~~^^
why 1 == 2 won't get you the ERROR msg? They are integers, not from the rabbit class."
"Okay? Let's try it out for tram.
Again, remember tram can fail, so I'm gonna get two things here.
So these are the things I'm going to get for tram, I'm going to either end up in six with probability 0.5 with the reward of minus 2 or I will not go anywhere.
I'm still at three with probability 0.5 and that is with a reward of minus 2.
Okay? All right.
So that was just the tram problem and we formalized it as an MDP.
Again, the reason it's an MDP is, is that the tram can fail with probability 0.5.
So we added that in, then we defined our transition function and our problem- and our reward function.
Okay? All right, everyone happy with how we are defining MDPs? Yeah? Okay.
Pretty similar to search problems except for now we have these probabilities, okay? All right.
So, so now I have defined an MDP, that's great.
The next question that in general we would like to answer is to give a solution, right? So there's a question here.","The transportation example has a problem. The states are discrete. If you take the tram, the starting state equals 1, and with state*2, you will never end up in state=3. Let's assume the first action was successful, therefore, the next state is 2. If the second action is successful too, you will be end up in state = 4. you will never end up in state = 3."
"Now, search of course makes down moves and right moves, but this is a backward search so it's going to make left moves and up moves.
What else do I have here? Running out of room, so let me-- let's continue with that.
All right.
And now the case is if the node was promoted higher, that means we got heads here in that particular insertion.
Then we go, and that means that during the search we came from upstairs.
And then lastly, we stop, which means we start when we reach the top level or minus infinity if we go all the way back.",what is a quad node?
"So we're going to express what we call the growth of the program's runtime as the input size grows very large.
And because we're only interested not in the exact number, but in, if you like, the growth of that, we're going to focus on putting an upper bound on the growth, an expression that grows at least as fast as what the cost of the algorithm is.
Now, you could just cheat and pick a really big upper bound.
That doesn't help us very much.
So in general, we're going to try and use as tight an upper bound as we can.
What's the class of algorithm it falls in? But as we've seen before, we care about the order of growth, not being exact.","Could you not argue that the intToStr() example is linear with respect to the number of digits in the input number? How do we decide what the input space is when it's ambiguous?
Let's say the time required by a program for input of size n is n+100. If you now go from 10 to 20, for 10, it takes 110 secs. But for 20, it takes 120 secs. For 30, it takes 130 secs. But difference between times for 10 and 20 and between 20 and 30 is same (10). Observe that from 10 to 20, time taken doesn't double. But the difference remains same"
"ll right.
So in this module, we will be talking about inference rules.
So if you remember, so far we have been talking about syntax and semantics.
And now, we would like to talk about how we can play around with formulas and manipulate them, and apply inference rules on them.
But why don't we talk about this diagram a little bit more for a second, before jumping into inference rules? So let me go back to my whiteboard here.
So basically, what I've been drawing here is-- we live in the syntax land, and formulas live in the syntax land.
So I'm going to draw them like this.
So maybe I have formula f1, formula f2, through, maybe, formula fn, OK? And then in the semantics land, I give meanings to these formulas, right? So each formula has a corresponding set of models to it.",Great lecture. Just wanted to check that you wanted to say models of KB belonging to models of g right? Since g is a superset of KB.
"The exponential distribution is distinct from exponential family, and the exponential distribution is a member of the exponential family.
It may be confusing, but this is the exponential distribution not the exponential family.
Exponential, or you can use gamma distribution and there are a few more.
And generally when you're trying to predict the time to some event, you call it survival analysis.
And you can also have your data type to be probability distributions themselves.
So this may be a little- little advanced, but if you're doing Bayesian statistics, for example, you can define a probability distribution over Bernoulli distributions, in which case, the exponential family for Bernoulli would be a beta distribution, and you can give it some name.
Similarly, you know, your- your- your- your y variable can be a categorical distribution itself, in which case, you want to have a distribution over distributions like for example, the Dirichlet distribution, and all of these can be fit in to the generalized linear model framework, right? You start with what your y variable is, data type of y, and the data type of y is going to inform the choice of distributions that you can use, right? So if somebody- if you're wondering is generalized linear models used for regression or classification, the answer is, it can be used for anything.
Just use the appropriate choice of exponential family distribution, okay? A question? Yes.
[inaudible].
I'm not gonna go too deep into that.
You can- you can ask me after the lecture, I can, I can explain more, all right? [NOISE] Any- any- any- any questions? [NOISE] Now.
What was the- what did we gain by putting them all in a common framework, right? We saw that, you know, yes, it's kind of mathematically elegant to kind of put them all into a common framework, but what have we gained anything as such, right? The answer is yes, we have actually gained.
Once we make a choice of distribution, we can go through the exercise of algebraically massaging it and doing a pattern matching to find out what our G-function is.
So first thing you wanna do is make a choice of distribution according to the data type, right? Now- Um, and for that distribution, um, express it in exponential family.","So, Y(i)'s that we are given(Dependent variable) are nothing but different points (points that are determined by 'eeta') of an exponential family(Gaussian for Linear Regression). In our case h(x; teta) will be a linear combination of x values because we assumed the distribution to be Gaussian. What if, in reality Y(i) does not follow Gaussian distribution and has some deviations. Will developing a regression make sense in that case?"
"Right? But when we define it as a likelihood function, um, product from i equals 1 to n, 1 over 2 Pi^d by 2 sigma [NOISE] to the half exponent of [NOISE] half X^i minus mu inverse X^i minus [NOISE] It is basically the same expression, but we are interpreting it differently.
We consider in the likely, uh, when we see this as a likelihood function, we think of the very- the, the parameters to be the variables, right? And your data to be given, right? When we think of- reinterpret this as the probability density function, we think of the data as the variables and your parameters to be fixed.
Yes, question.
[inaudible] Right, so the question is, um, when you think- when, when you write probability of Mu, Sigma given x.
So for- so, er, er, a small correction over there when we, we use the term probability of data, right? We never use the term probability of your parameters.
And we use the term likelihood of your parameters, right? The, the, the correct phrases to use his probability of the data given parameters, and the likelihood of the parameters given data, right? You wanna, you wanna- [inaudible] That is, that is the only difference in terms of, uh, you know, uh, um, mathematically, but semantically, we are treating the, the, um, um, the data as, as the variable here, but the parameters as variable here.","In the case of the probability density of the multivariate gaussian distribution in the denominator, the normalising constant sqrt(2*pi) will be raised to the power of 'n' instead of 'd' because we are having n many random samples?"
"Now I'd like to give you an aside because it's easy to get confused about N queueing and extending.
In all of the searches we did last time, it would have been perfectly reasonable to keep a list of all the paths that we had put onto the queue.
An N queueing list.
And never add a path to our queue if it terminates in a node that some other path terminate in that has already gone to the queue.
What I said last time was let us keep track of the things that have been extended and not extend them again.
So you can either keep track of the nodes that have been extended and not extend them again.
Or look at the paths with nodes that terminate, and blah, blah, blah and been put on the queue, the queued ones.","What is the difference between enqueued list and extended list ?
A node is enqueued does not mean that it is extended. In fact, we can enqueue many times the same node, but with different distence value each time, they are just candidiates for extension. And we just choose the shortest for extension.
I think this consistency matter is there whenever we don't extend the same node twice -Sure it is true what he said, but I think he should have mentioned/clarified that whenever it is not a map (the distance eq AB+BC>=AC is not necessarily true) We should check not just whether we've extended this node before or not, but with what cost value???? I mean even if we are not using heuristics this could miss a shorter path (i. e. for example it could be yes we have extended C before but with a longer path, if SC=11 while SA->AC=5+2=7) . Or am I missing something here????
When he explains consistency, couldn't you use a check for your extended list where instead of checking if a node has already been reached, instead take whichever repeat is the shortest? Therefore although he reaches C through S-B-C and therefore he was hosed when he tried S-A-C, it'd instead take S-A-C because if there's a way between S & C that is less than the others you'd use that path
+qidas123 we only extend a node one time, algorithm actually never says extend if the path is shorter. What we keep in the ""Extended List"" is only the names of nodes we already extended, nothing about the length of path to those nodes.
So what does this mean for the search? It is invalid? Or does it now check down that path since it failed the consistency check?"
