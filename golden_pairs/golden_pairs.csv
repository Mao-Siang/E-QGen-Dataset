video_id,paragraph,questions
-1BnXEwHUok,"So what we got with the sample was 0.
So what's the obvious thing to do? If you're doing a simulation of an event and the event is pretty rare, you want to try it on a very large number of trials.
So let's go back to our code.
And we'll change it to instead of 1,000, 1,000,000.
You can see up here, by the way, where I set the seed.
And now, let's run it.
We did a lot better.
If we look at here our estimated probability, it's three 0's 128, still not quite the actual probability but darn close.
And maybe if I had done 10 million, it would have been even closer.
So if you're writing a simulation to compute the probability of an event and the event is moderately rare, then you better run a lot of trials before you believe your estimated probability.
In a week or so, we'll actually look at that more mathematically and say, what is a lot, how do we know what is enough.
What are the morals here? Moral one, I've just told you-- takes a lot of trials to get a good estimate of the frequency of a rare event.
Moral two, we should always, if we're getting an estimated probability, know that, and probably say that, and not confuse it with the actual probability.",29:54 How do we know the number of samples required so that the estimated probability is equal to the actual probability ? Thanks.
-DP1i2ZU9gk,"And the second thing that represents a list is going to be this second part, which is a pointer.
And internally this pointer is going to tell Python where is the memory location in the computer where you can access the element index 1.
So it's just essentially going to be a chain, going from one index to the other.
And at the next memory location you have the value at index 1, and then you have another pointer that takes you to the location in memory where the index 2 is located.
And in index 2 you have the value and then the next pointer, and so on and so on.
So this is how Python internally represents a list.
OK? How you manipulate lists, we've done this a lot, right? You can index into a list, you can add two lists together, you can get the length, you can append to the end of a list, you can sort a list, reverse a list, and so many other things, right? So these are all ways that you can interact with the list object as soon as you've created it.","I would like to get this clarified: At 5:53 , the diagram explains that python lists are represented internally as linked list. In my understanding, this is incorrect because python lists are internally represented as dynamic arrays as explained here https://youtu.be/CHhwJjR0mZA?t=2082"
-DP1i2ZU9gk,"And then I've also implemented some other special methods.
How do I add two fractions? How do I subtract two fractions? And how do I convert a fraction to a float? The add and subtract are almost the same, so let's look at the add for the moment.
How do we add two fractions? We're going to take self, which is the instance of an object that I want to do the add operation on, and we're going to take other, which is the other instance of an object that I want to do the operation on, so the addition, and I'm going to figure out the new top.
So the new top of the resulting fraction.
So it's my numerator multiplied by the other denominator plus my denominator multiplied by the other numerator and then divided by the multiplication of the two denominators.
So the top is going to be that, the bottom is going to be that.
Notice that we're using self dot, right? Once again, we're trying to access the data attributes of each different instance, right, of myself and the other object that I'm working with.
So that's why I have to use self dot here.
Once I figure out the top and the bottom of the addition, I'm going to return, and here notice I'm returning a fraction object.
It's not a number, it's not a float, it's not an integer.
It's a new object that is of the exact same type as the class that I'm implementing.
So as it's the same type of object, then on the return value I can do all of the exact same operations that I can do on a regular fraction object.
Sub is going to be the same.
I'm returning a fraction object.
Float is just going to do the division for me, so it's going to take the numerator and then divide it by the denominator, just divide the numbers.","First thank you for this great tutorial! In 36:00, why do we have inverse function without the leading and trailing underscores? What does it mean those underscores make a function ""magical""?
~36:00. c = a + b. How is a and b assigned to ""self"" and ""other"", how do you know which gets assigned to what? Would appreciate an explanation what happens and where a and b values are sent."
-DP1i2ZU9gk,"And this one on the right essentially says, what's the name of the class, dot, dot notation, what's the method you want to call, and then in parentheses you give it all of the variables including self.
OK.
So in this case you're explicitly telling Python that self is C and other is 0.
So this is a little bit easier to understand, like that.
But it's a little cumbersome because you always have to write coordinate dot, coordinate dot, coordinate dot, for every data attribute you might want to access, for every procedural attribute you might want to access.
So by convention, it's a lot easier to do the one on the left.
And as I mentioned, Python implicitly says, if you're doing the one on the left, you can call this method on a particular object and it's going to look up the type of the object and it's going to essentially convert this on the left to the one on the right.
And this is what you've been using so far.
So when you create a list, you say L is equal to 1, 2, and then you say L.append, you know, 3 or whatever.",In 26:00 can we write zero.distance(c) instead of c.distance(zero)??
09mb78oiPkA,"PATRICK WINSTON: Yes? [? SPEAKER 1: Are we, ?] necessarily, have it done with some sort of a [? politidy distance ?] metric? PROF.
PATRICK WINSTON: Oh, here we go.
We're not going to use any [? politidy distance ?] metric.
We're going to use some other metric.
SPEAKER 1: Like alogrithmic, or whatnot? PROF.
PATRICK WINSTON: Well, algorithmic, gees, I don't know.
[LAUGHTER] PROF.
PATRICK WINSTON: Let me give you a hint.
Let me give you a hint.
There are all those articles up there, out there, and out there, just for example.
And here are the Town and Country articles.
They're out there, and out there, for example.
And now our unknown is out there.
Anybody got an idea now? Hey Brett, what do you think? BRETT: So you sort of want the ratio.
Or in this case, you can take the angle-- PROF.
PATRICK WINSTON: Let's be-- ah, there we go, we're getting a little more sophisticated.
The angle between what? BRETT: The angle between the vectors.
PROF.
PATRICK WINSTON: The vectors.
Good.
So we're going to use a different metric.
What we're going to do is, we're going to forget including a distance, and we're going to measure the angle between the vectors.
So the angle between the vectors, well let's actually measure the cosine of the angle between the vectors.
Let's see how we can calculate that.
So we'll take the cosine of the angle between the vectors, we'll call it theta.
That's going to be equal to the sum of the unknown values times the article values.
Those are just the values in various dimensions.
And then we'll divide that by the magnitude of the other vectors.
So we'll divide by the magnitude of u, and we'll divide by the magnitude of the art vector to the article.
So that's just the dot product right? That's a very fast computation.
So with a very fast computation you can see if these things are going to be in the same direction.","At 23:01 the professor says 'so that's just the dot product, right' - but that's to say that cosine similarity = dot product, which is not precise, right? The dot product is the numerator in this case."
09mb78oiPkA,"So we'll call this the feature detector.
And out comes a vector of values.
And that vector of values goes into a comparator of some sort.
And that comparator compares the feature vector with feature vectors coming from a library of possibilities.
And by finding the closest match the comparator determines what some object is.
It does recognition.
So let me demonstrate that with these electrical covers.
Suppose they arrived on an assembly line and some robot wants to sort them.
How would it go about doing that? Well it could easily use the nearest neighbor sorting mechanism.
So how would that work? Well here's how if would work.
You would make some measurements.
And it we'll just make some measurements in two dimensions.",What is comparitor 8:32? Couldn't find on the web. Is this a spelling mistake?
09mb78oiPkA,"What's the variance of that going to be? x over sigma sub x.
Anybody see, instantaneously, what the variance of that's going be? Or do we have to work it out? It's going to be 1, Work out the algebra for me.
It's obvious, it's simple.
Just substitute x prime into this formula for variance, and do the algebraic high school manipulation.
And you'll see that the variance turns out not to be of this new variable, this transformed variable you want.
So that problem, the non uniformity problem, the spread problem, is easy to handle.
What about that other problem? No cake without flour? What if it turns out that the data-- you have two dimensions and the answer, actually, doesn't depend on y at all.
What will happen? Then you're often going to get screwy results, because it'll be measuring a distance that is merely confusing the answer.
So problem number two is the what matters problem.
Write it down, what matters.
Problem number three is, what if the answer doesn't depend on the data at all? Then you've got the trying to build a cake without flour.
Once somebody asked me-- a classmate of mine, who went on to become an important executive in an important credit card company-- asked me if we could use artificial intelligence to determine when somebody was going to go bankrupt? And the answer was, no.
Because the data available was data that was independent of that question.
So he was trying to make a cake without flour, and you can't do that.
So that concludes what I want to say about nearest neighbors.
No I want to talk a little bit about sleep.","At 41:45, the professor indicates that you cannot use AI for predicting bankruptcies in credit card companies. That's like making cake without flour. Wouldn't the credit card company have relevant data to be able to use AI to predict bankruptcies? Why is the answer ""no""?
did someone understand in 40:00 the derivative of x ?"
0CdxkgAjsDA,"Among all those nodes that have a job in delta f, in delta.
This is probably a tricky part.
Then by definition, since delta f u is less than delta f v, right, and I defined v to be the one with the smallest delta that satisfies that.
So all the u's-- so u is a predecessor of v, so u shouldn't be one of those nodes that have a drop in delta.
So I know this is probably a tricky part.
Yeah, I'll stop for questions and make sure we resolve this part before we move on.","the tricky part at 16:55 can be proved by contradiction: Assume we have delta_f'(u) < delta_f(u), then u is in the set of vertices x that satisfy delta_f'(x) < delta_f(x). Meanwhile, we know delta_f'(u)<delta_f'(v) as u is the predecessor of v. However, this contradicts that v is the smallest delta_f'(x) in the set of vertices x. In this case, u is the smallest. The contradiction implies that the assumption (""Assume we have delta_f'(u) < delta_f(u)"") is wrong, which means ""delta_f'(u) >= delta_f(u)""."
0CdxkgAjsDA,"How many people get it? OK.
Only two.
That's not good.
OK.
AUDIENCE: I'm confused about how v can be the one with the smallest delta f if you have a predecessor with a smaller delta f.
PROFESSOR: OK.
So v to be the smallest-- the one with smallest f such that delta f prime is less than delta f.
AUDIENCE: OK.
PROFESSOR: OK, maybe, yeah, that's why I confused you guys.
Yeah.
Sorry about that.
So we have a bunch of nodes who have a drop in delta, and I defined v to be the one among them that has the smallest delta f prime.
Question? AUDIENCE: Sorry, I'm lost at what delta f prime is versus delta.
PROFESSOR: OK.
Delta f of a node is the shortest path from source to that node in G of f, which is the residual graph given a flow.
So f is, well, some flow, and f prime is the flow after we augmenting a certain path.
So f prime is one step after f.
OK.
How many people get that now? Still not everyone.
OK.
Any questions about that? How many people still haven't got that? OK, so some people-- it's like Schrodinger's cat.","On 17:55 he didn't ask the question. If delta_f^'(v) is the smallest one, then v must be the successor of s. How can there be a u between s and v?"
0LixFSa7yts,"And that turns out to be pretty problematic.
So the next couple of slides sort of say a little bit about the why and how this happens.
What's presented here is a kind of only semi-formal wave your hands at the kind of problems that you might expect.
If you really want to sort of get into all the details of this, you should look at the couple of papers that are mentioned in small print at the bottom of the slide.
But at any rate, if you remember that this is our basic recurrent neural network equation, let's consider an easy case.
Suppose we sort of get rid of our non-linearity and just assume that it's an identity function.
OK, so then when we're working out the partials of the hidden state with respect to the previous hidden state, we can work those out in the usual way according to the chain rule.
And then, if sigma is simply the identity function, well then, everything gets really easy for us.
So only-- the sigma just goes away.
And only the first term involves h at time t minus 1.
So the later terms go away.
And so our gradient ends up as Wh.
Well, that's doing it for just one time step.
What happens when you want to work out these partials a number of time steps away? So we want to work it out, the partial of time step i with respect to j.
Well, what we end up with is a product of the partials of successive time steps.
And well, each of those is coming out as Wh and so we end up getting Wh raised to the l-th power.
And while our potential problem is that if Wh is small in some sense, then this term gets exponentially problematic.",at 41:00 why is del J / del h even needed?
0UFwGJe6ubg,"The averages there and the distributions look moderately similar.
If you're coming from a skilled nursing facility, if you are in a skilled nursing facility, you're probably old because younger people don't typically need skilled nursing care.
And I'm not sure why transfers within the facility are significantly younger ages, but that's true from the MIMIC data.
What about age at admission by language? So some people speak English.
Some people speak not available.
Some people speak Spanish, et cetera.
So it turns out the Russians are the oldest.
And that may have to do with immigration patterns, or I don't know exactly why.
But that's what the data show.
If you do it by ethnicity, it turns out that African-Americans, on the whole, are somewhat younger than whites.
And Hispanics are somewhat younger yet.
So that means that those subpopulations apparently need intensive care earlier in life than whites.
So this is a topic that's very hot right now, discussions about how bias might play into health care.","27:44 I don't quite see how can the professor interpret from the plot that ""Afro-Americans are somewhat younger than the whites"". Can someone explain to me how you could draw this ? Thanks :)"
0jljZRnHwOI,"And that forces you to indent everything that's a code block.
So you can easily see sort of where the flow of control is and where decision making points are and things like that.
So in this particular example, we have one if statement here, and it checks if two variables are equal.
And we have an if, elif, else.
And in this example, we're going to enter either this code block or this one or this one, depending on the variables of x and y.
And we're only going into one code block.
And we'll enter the first one that's true.
Notice you can have nested conditionals.
So inside this first if, we have another if here.
And this inner if is only going to be checked when we enter the first-- this outter if.
I do want to make one point, though.
So sometimes, you might forget to do the double equals sign when you are checking for equality, and that's OK.
If you just use one equals sign, Python's going to give you an error.
And it's going to say syntax error, and it's going to highlight this line.
And then you're going to know that there's a mistake there.
And you should be using equality, because it doesn't make sense to be using-- to assign-- to be making an assignment inside the if.",at 25:16 what's the use of the nested if? and when will it be used!
0jljZRnHwOI,"So the for loop says, for some loop variable-- in this case, I named it n.
You can name it whatever you want.
In range 5-- we're going to come back to what range means in a little bit-- print n.
So every time through the loop, you're going to print out what the value of n is.
Range 5 actually creates internally a sequence of numbers starting from 0 and going to that number 5 minus 1.
So the sequence is going to be 0, 1, 2, 3, and 4.
The first time through the loop, you're going to say n is equal to 0.
Or internally, this is what happens.
N gets the value of 0.
You're going to print n.
Then you're going to go back to the top.
N gets the value 1.
Then you're going to go execute whatever is inside.
So you're going to print 1.
Then you're going to increment that to the next value in the sequence.
You're going to print out 2, and so on.","At 34:35, where do you get 5 - 1?
She points at 34:35, at the whiteboard and say ""that number, 5-1"". When it shows the whiteboard, there is not a number or equation (5-1). Where does it come from?"
0rt2CsEQv6U,"Um, and then expected value of [NOISE] this quadratic term.
Um, because this quadratic term here, kind of the inductive case was what we showed was V star for the- for the next time step, right? So it turns out that, um, let's see.
So this is a quadratic function, and this expectation is the expected value of a quadratic function with respect to s drawn from a Gaussian, right? With a certain mean and certain variance.
So it turns out that, um, the expected value of this thing, right? Well, this whole thing that I just circled.","At 1:06:15, shouldn't the ""Big Quadratic Function"" include the second term because a_t is there as well?"
0zuiLBOIcsw,"And I can- I wanna contract them into a super-node, ah, and, uh, create a new network, a next level network, where super-nodes are connected if there is at least one edge between the nodes, ah, of the corresponding communities, um, and the, ah, weights, ah, of the edges between two super-nodes is the sum of the edge weights across all edges between their corresponding communities.
And now I will have a super graph.
And I simply go and run, ah, phase 1, ah, again.
All right, so the idea is, I have my original network, I run phase 1 to identify clusters.
Now I contract each cluster into a super-node.
I connect two clusters, if there is at least one edge, ah, between them.
And now this will be a weighted network where the, the edge weights are denoted here, so this will be the total number of edges between C_1 and C_2.
And this would be the total number of edges, ah, between the members of, ah, C_2 ah, and so on.
And now that I have the super graph, I simply apply my, ah, phase 1 algorithm again.
So the way this will work is, ah, you know, to summarize, I have my original network, I pick a node and, ah, initially I put every node into its own community.
Um, and then I ask, ah, a node, what if I move you to the same community as your member node 2 is up.","On the slide at 12:25 it shows Delta M_0,4 = 0.26. Since nodes 0 and 4 are not connected, I don't think we would compute this change in modularity, correct? We would only compute it for neighbors of node 0?"
1A6VoEkQnhQ,"And, uh, the reason why a, uh, uh, plain GNN cannot differentiate between, you know, node 1 in, uh, a cycle of length 3 versus a cycle of leng- length 4 is that if you look at the GNN computation graph, re- both- for both of these nodes V_1 and V_2, the computation graph is exactly the same.
Meaning, V_1- V_1 and V_2 have two neighbors, um, each, and then, you know, these neighbors have, uh, one neighbor each and the computation graph will always look like this.
Uh, unless, right, you have some way to discriminate nodes based on the features.
But if the nodes have the same, um, set of, uh, features that not- you cannot discriminate them based on their attributes, then you cannot learn, uh, to discriminate node V_1 from V_2.
They will- from the GNN point of view, they will all, uh, look the same.
I'm going to go into more, uh, depth, uh, uh, around this, uh, this example and, er, what are some very important implications of it and consequences of it when we are going to discuss the theory of, uh, graph neural networks.",The computational graph are structurally same for the node v_1 at 14:00 but they'll be fed different embeddings. The embeddings will implicitly include information about the nodes. A GNN can still struggle to distinguish cycles of varying types and shapes?
247Mkqj_wRM,"So I take the coefficients e that we have just defined, I, uh, exponentiate them, and then, you know, divide by the s- exponentiated sum of them so that, uh, these, uh, attention weights, uh, alpha now are going to sum to 1.
And then, right when I'm doing message aggregation, I can now do a weighted sum based on the attention weights, uh, alpha.
So here are the alphas.
These are these alphas that depend on e, and e is the, uh, is the, um, is- depends on the previous layer embeddings of nodes, uh, u and v.
So, for example, if I now say, how would aggregation for node A look like? The way I would do this is I would compute these attention weights, uh- uh, Alpha_AB, Alpha_AC, and Alpha_AD because B, C, and D are its neighbors.
Uh, these alphas will be computed as I- as I show up here, and they will be computed by previous layer embeddings, uh, of these, uh, nodes on the endpoints of the edge.
And then my aggregation function is simply a weighted average of the messages coming from the neighbors, where message is, uh- uh- uh, multiplied by the weight Alpha that we have, uh, computed and defined up here.
So that's, um, [NOISE] basically the idea of the attention mechanism.
Um, now, what is the form of this attention mechanism a? We still haven't decided how embedding of one node and embedding of the other node get- get combined, computed into this, uh- uh, weight, uh, e.
Uh, the way it is usually done is, uh- um, you- you have many different choices.
Like you could use a simple, uh, linear layer, uh, one layer neural network to do this, um, or, uh, have alpha, uh, this, um, function a have trainable parameters.
Uh, so for example a p- uh, a popular choice is to simply to say: let me take the embeddings of nodes A and B at the previous layer, perhaps let me transform them, let me concatenate them, and then apply a linear layer to them, and that will give me this weight, uh, e_AB, to which then I can apply softmax, um, and then based on that, ah, softmax transformed weight, I use that weight as, uh- uh, in the aggregation function.","Two questions that I had after watching this lecture: Is there a qeuery key value interpretation of the simple ""linear attention"" mechanism in graph attention networks? At 22:07 when describing how to normalize the unnormalized attention weights a_{vu} in the graph attention network scheme why do we not just divide by the total sum of all the attention weights? Why do we use a softmax to suppresses the non-maximal weights to mostly focus in on a single neighbor only?"
247Mkqj_wRM,"So, last lecture, we talked about graph convolutional neural network or a GCN.
And I've wrote this equation, I said, ah-ha, the embedding of node v at layer l is simply an average of the embeddings of nodes u that are neighbors of we normalized the by the- by the N-degree of node v and transformed with matrix W and sent through a non-linearity.
So now the question is, how can I take this equation that I've written here and write it in this message transformation plus aggregation function.
And the way you can- you can do this is simply take this W and distribute it inside.
So basically now W times h divided by number of neighbors is the message transformation function.
And then the message aggregation function is simply a summation.
And then we have a non-linearity here.","This might be nitpicking, but I would still like to ask for conceptual clarity: Doesn't it make more sense to ascribe (at 10:20, when trying to understand the classical GCN layer as a message transformation + aggregation scheme) the normalizing factor 1/|N(v)| to the aggregation phase since it depends on the node towards which the messages are being passed and not just the message itself? In other words, pull it out of the sum using the distributive law? If one does that the message transformation step will only depend on the message and not on the node it is being delivered to. That seems conceptually cleaner to me for something to be thought of as ""message transformation""."
247Mkqj_wRM,"Then we have to aggregate these messages into a single message and pass it on.
So the way you can think of this is that we get messages, denoted as circles here, from the three neighbors, from the previous layer.
We also have our own message, right? Message of the node v from the previous layer.
Somehow we want to combine this information to create the next level embedding or to the next level message for this node of interest.
What is important here to note is that this is a set.
So the ordering in which we are aggregating these messages from the children is not important.
What is arbitrary? And for this reason, these aggregation functions that aggregate, that summarize, that compress in some sense, the messages coming from the children have to be order invariant because they shouldn't depend, in which ordering, am I considering the neighbors? Because there is no special ordering to the neighbors, to the lower level, to the children in the network.
That's an important detail.
Of course, another important detail is that we want to combine information coming from the neighbor- from the neighbors together with a node's own information from the previous layer as denoted here.","When he said we have the value of (1:30) the self node from previous later, is he referring to the value of V as in last itteration because here layer is used for actually the layer before it where v doesn't actually exist. Its drilling in to the words too much but just wanna know for absolute certainty. Or it is the self loop that is being used as a previous layer here"
2P-yW7LQr08,"So the last thing I'll do-- and I just have one more minute-- is give you a sense of a small change to interval scheduling that puts us in that NP complete domain.
So so far, we've just done two problems.
There's many others.
We did interval scheduling.
There was greedy linear time.
Weighted interval scheduling is order n squared according to this particular DP formulation.
It turns out there's a smarter DP formulation that runs an order n log n time that you'll hear about in section on Friday, but it's still polynomial time.
Let's make one reasonable change to this, which is to say that we may have multiple resources, and they may be non identical.","I don't get 1:19:41. If recursive calls are O(1) then shouldn't the complexity be n? Why is it n^2, can somebody explain?"
2g9OSRKJuzM,"Now, this looks kind of random.
Anybody recognize these numbers? No one from the great City of New York? No? Yup, yup.
AUDIENCE: On the subway stops? SRINIVAS DEVADAS: Yeah, subway stops on the Seventh Avenue Express Line.
So this is exactly the notion of a skip list, the fact that you have-- could you stand up? Great.
All right.
So the notion here is that you don't have to make a lot of stops if you know you have to go far.
So if you want to go from 14th Street to 72nd Street, you just take the express line.
But if you want to go to 66th Street, what would you do? AUDIENCE: Go to 72nd and then go back.
SRINIVAS DEVADAS: Well, that's one way.
That's one way.
That's not the way I wanted.
The way we're going to do this is we're not going to overshoot.
So we want to minimize distance, let's say.
So our secondary thing is going to be minimizing distance travel.
And so you're going to pop up the express line, go all the way to 42nd Street, and you're going to say if I go to the next stop on the Express Line, I'm going too far.
And so you're going to pop down to the local line.
So you can think of this as being link list L0 and link list L1.","12:00 I don't understand why he avoids overshooting here. Since traveling on L1 is faster, going to 72 and then back to 66 should have the minimum amount of nodes, right ?"
3pU-Hrz_xy4,"So this is actually equal to 2 in this case.
And this corresponds to this value in the table which is again the agent is following a maximizer assuming the opponent is a minimizer.
Opponent was not a minimizer, opponent was just following Pi 7.
And this is just equal to 2 .
Okay.
So so far, the things I've shown are actually very intuitive.
They seem a little complicated but they're very intuitive.
What I've shown is that this value of minimax, it's an upper bound.
If you're assuming our, our opponent is a terrible opponent, now it's going to be an upper bound because the best thing I can do is maximize.
I've also shown it's a lower bound if my opponent is not as bad.
So, so that's what I've shown so far.
A question.
So here the opponent's policy is completely hidden to the agent.
Yeah.
So here, like, because- Yeah, the agent actually doesn't see the opponent- where the opponent goes, right? Even in the expectimax case, it thinks the opponent is going to follow Pi 7, but maybe the opponent follows Pi 7, maybe not.
Right so, so like when we talk about expectimax and minimax, it's always the case that the opponent doesn't actually see what the opponent does.
But the opponent can think- the agent can think what the opponent does, okay? And I'm going to talk about one more property.","47:09 Why is V(pimax,pi7)=2 and not 5, assuming agent will try to maximize his value while the opponent will act stochastically (ie. 0,2,5 as distributions)"
3v5Von-oNUg,"I think that's right.
But now we have our original aR bR plus 2q plus 8q is equal to this thing.
And finally, we can divide this thing by R very cheaply.
Because we just discard the low four zeros.
Make sense? Question.
AUDIENCE: Is aR bR always going to end in, I guess, 1,024 zeros? PROFESSOR: No, and the reason is that-- OK, here is the thing that's maybe confusing.
A was, let's say, 512 bits.
Then you multiply it by R.
So here, you're right.
This value is that 1,000 bit number where the high bit is a, the high 512 bits are a.
And the low bits are all zeros.
But then, you're going [? to do it with ?] mod q to bring it down to make it smaller.","45:08 omfg and I was so confused whole time lmaoo wonderful lecture tho, I was able to follow easily and deduce things from his prompts. Nonetheless, I think many properties he mentioned, like of inverses at 48:10 applies only to finite fields with order prime right? I think it should've just been mentioned once, in case I am not mistaken, will probably give it a look"
3v5Von-oNUg,"So instead of doing 500 mod qs for every multiplication step, you do it twice mod q.
And then you keep doing these divisions by R cheaply using this trick.
Question.
AUDIENCE: So when you're adding the multiples of q and then dividing by R, [INAUDIBLE] PROFESSOR: Because it's actually mod q means the remainder when you divide by q.
So x plus y times q, mod q is just x.
AUDIENCE: [INAUDIBLE] PROFESSOR: So in this case, dividing by-- so another sort of nice property is that because it's all modulus at prime number-- it's also true that if you have x plus yq divided by R, mod q is actually the same as x divided by R mod q.
The way to think of it is that there's no real division in modular arithmetic.
It's just an inverse.
So what this really says is this is actually x plus yq times some number called R inverse.
And then you compute this whole thing mod q.
And then you could think of this as x times R inverse mod q plus y [? u ?] R inverse mod q.
And this thing cancels out because it's something times q.
And there's some closed form for this thing.
So here I did it by bit by bit, 2q then 8q, et cetera.
It's actually a nice closed formula you can compute-- it's in the lecture notes, but it's probably not worth spending time on the board here-- for how do you figure out what multiple of q should you add to get all the low bits to turn to 0.","45:08 omfg and I was so confused whole time lmaoo wonderful lecture tho, I was able to follow easily and deduce things from his prompts. Nonetheless, I think many properties he mentioned, like of inverses at 48:10 applies only to finite fields with order prime right? I think it should've just been mentioned once, in case I am not mistaken, will probably give it a look"
4b4MUYve_U8,"Okay? Um, so in order to- [NOISE] so in order to simplify the notation, [NOISE] um, [NOISE] in order to make that notation a little bit more compact, um, I'm also gonna introduce this other notation where, um, we want to write a hypothesis, as sum from J equals 0-2 of theta JXJ, so this is the summation, where for conciseness we define X0 to be equal to 1, okay? See we define- if we define X0 to be a dummy feature that always takes on the value of 1, then you can write the hypothesis h of x this way, sum from J equals 0-2, or just theta JXJ, okay? It's the same with that equation that you saw to the upper right.
And so here theta becomes a three-dimensional parameter, theta 0, theta 1, theta 2.
This index starting from 0, and the features become a three dimensional feature vector X0, X1, X2, where X0 is always 1, X1 is the size of the house, and X2 is the number of bedrooms of the house, okay? So, um, to introduce a bit more terminology.
Theta [NOISE] is called the parameters, um, of the learning algorithm, and the job [NOISE] of the learning algorithm is to choose parameters theta, that allows you to make good predictions about your prices of houses, right? Um, and just to lay out some more notation that we're gonna use throughout this quarter.","May I ask, down to 7:50 what does O (teta) represent?"
4b4MUYve_U8,"Uh, that's times the partial derivative of Theta J of X Theta X minus Y, right? So if you take the derivative of a square, the two comes down and then you take the derivative of what's inside and multiply that, right? [NOISE] Um, and so the two and one-half cancel out.
So this leaves you with H minus Y times partial derivative respect to Theta J of Theta 0X0 plus Theta 1X1 plus th- th- that plus Theta NXN minus Y, right? Where I just took the definition of H of X and expanded it out to that, um, sum, right? Because, uh, H of X is just equal to that.","28:51, what is x0 and x1? If we have a single feature, say # of bedrooms, how can we have x0 and x1? Wouldn't x0 be just nothing? I'm confused. Or, in other words, if my Theta0 update function relies on x0 for the update, but x0 doesn't exist, theta0 will always be the initial theta0..."
4b4MUYve_U8,"Um, and then finally, by convention, we put a one-half there- put a one-half constant there because, uh, when we take derivatives to minimize this later, putting a one-half there would make some of the math a little bit simpler.
So, you know, changing one- adding a one-half.
Minimizing that formula should give you the same as minimizing one-half of that but we often put a one-half there so to make the math a little bit simpler later, okay? And so in linear regression, we're gonna define the cost function J of Theta to be equal to that.","Dear Dr. Andrew I saw yours other video with the cost function with linear regression by 1/2m but this video 1/2, so what is different between it?(footnote 16:00)"
4dj1ogUwTEM,"Anyway, just as there's a decimal expansion of every real number, there's a binary expansion just using base 2.
So here's the binary expansion of 3 and 1/3.
So what I'm going to do is I'm going to map 3 and 1/3 to this binary sequence.
I'm going to ignore the decimal place.
Binary is not decimal place.
It's a [? becimal ?] place, or binary position.
And I'm just going to take this to mapping the sequence, 11010101.
And I claim that this is a surjection because you're going to hit every possible binary sequence in this way.
Well, almost.
Let's take a closer look.
There's a problem with mapping to things that start with 0, because let's examine that a half is 0.10000000.
So I would map it to that.
And it will end.
But there's an ambiguity, because a half is also equal to 0.011111, just as 0.999999 is equal to 1.000000 in decimal, you get the same infinite carry issue here in binary.
So numbers that end in all ones have another way to represent the very same number by a sequence that ends in all zeroes.",18:50 How do we know we will hit every possible inifinite string?
4dj1ogUwTEM,"But we'll think about it as though we could.
Let's think about this matrix again.
So suppose A is this set of elements, a, b, s, t, d, e.
I'm scrambling up the alphabet on purpose, because I don't want you to get the idea that we're assuming that A is countable, that you can list all the elements of A.
I'm not assuming that.
But I'm just writing out a sample of elements of A.
And let's suppose that I was trying to get a surjection from A to the power set of A.
So suppose I have a function f that maps each of the successive elements of A to some subset of A.","9:44 Wth, why is f(a) supposed to have anything to do with subsets??? How is running a function on every element of a set supposed to make it its powerset???"
4nXw-f6NJ9s,"And you can even download the level-- an example of the level and play it, if you dare.
So that's a lot of-- we have a lot of fun in that world of hardness of different games and puzzles.
Where do I want to go next? OK.
Next topic is balloon twisting.
Totally different.
This is recreational, but not about hardness.
This is an octahedron twisted from one balloon.
I made another one on a stick.
Each of these is made for one balloon.
What graphs can you make for one balloon? Well, you should read our paper.
And you can characterize how many balloons you need to make each polyhedron.
And some of these problems are NP-hard, and it's a lot of fun.
Cool.
I think that's the end of the slides.
The last thing I wanted to show you is a problem, a puzzle/magic trick-- it comes from the puzzle world-- called the picture hanging problem.
So imagine you have a picture.
You want to hang it on a wall.
So you invested in some nice rope, and you hang it on a nail.","30:15 Is that basically an AND gate, using rope? The picture is hanging when both values are used (TRUE), else the picture drops (FALSE)"
51-b2mgZVNY,"Again, 8 factorial.
OK.
Finally, how many permutations are there that have all three patterns-- 60, 04, and 42? That of course, is exactly the same as the set of sequences with the single pattern 6042, the four digit pattern.
And again, we count that by saying that it's the number of permutations of the digits other than 6042-- six of them plus the 6042 object.
There are seven of these , and so there are 7 factorial permutations that have all three patterns.
So that means that I can go back to my inclusion-exclusion formula for the sequences that have one of the three patterns-- 60, 04, and 42-- and plug them in.
So I get 3 9 factorials for the first sum of three terms.
The intersections-- we all figured out each of them were-- I'm sorry it's 8 factorial.
So I'm going to subtract 3 times 8 factorial.
And this last term we figured out was 7 factorial.
Well, I can think of 3 times 9 factorial as 9 times 8 times 3 times 7 factorial, and this is 3 times 8 times 7 factorial.
And I wind up [NO AUDIO] PROFESSOR: 72,720.
That's how many permutations of the digits 0 through 9 there are that have one or another of these three patterns.
Turns out that's about 27% of the 10 factorial permutations of 0 through 9.
So that's the significance of applying the disjunction of constraints, this union of having either 60, 04, or 42.
",Which method is he using to calculate the result at 12:16?
5Bx5UhrJbJI,"This slide has a bunch of other annotations on it.
And the reason I included them is that the course repository includes a reference implementation of an autoencoder and all the other deep learning models that we cover in pure NumPy.
And so if you want it to understand all of the technical details of how the model is constructed and optimized, you could use this as a kind of cheat sheet to understand how the code works.
I think the fundamental idea that you want to have is simply that the model is trying to reconstruct its inputs.
The error signal that we get is the difference between the reconstructed and actual input.
And that error signal is what we use to update the parameters of the model.
Final thing I would mention here is that it could be very difficult for this model if you feed in the raw current vectors down here.
They have very high dimensionality.
And their distribution is highly skewed as we've seen.
So it can be very productive to do a little bit of reweighting and maybe even dimensionality reduction with LSA before you start feeding inputs into this model.","11:00 everything is confusing from there on , at least explain the labels a bit"
5cF5Bgv59Sc,"So that's the triangle inequality.
Pretty intuitive notion, right? Why is this useful? OK, well, if I find-- if I find an edge in my graph, if there's an edge u, v, in my graph such that this condition is violated for the estimates that I have-- it obviously can't be violated on my shortest path distances, but if it violates it on the estimates-- u, v, is bigger than u, x-- sorry, u-- how am I going to do this? I want this to be s.
I'm calculating shortest path distances from s and shortest path distances from s to some incoming vertex u plus the edge weight from u to v.
All right, so what is this doing? I have some edge u, v in my graph.
Basically, what I've said is that I have some distance estimate to u, but going through-- making a path to v by going through u, and then the edge from u to v is better than my current estimate, my shortest path estimate to v.
That's bad, right? That's violating the triangle inequality.
These cannot be the right weights.
These cannot be the right distances.
So how we're going to do that is lower-- this is what we said, repeatedly lower these distance estimates.
I'm going to lower this guy down to equal this thing.
In a sense, this constraint was violated.
But now we're relaxing that constraint so that this is no longer violated.","42:30 shouldnt it be d(s, v) > delta(s, u) + w(u,v)? bcoz d(s,u) is also an upper bound estimate, i.e, infinity"
5rlIYGJdPy4,"Exactly what we did on the example on the previous slide.
So let's revisit the Australia example and apply AC-3.
OK.
So here is the empty assignment.
And here are all the domains of each of the variables.
So let's suppose we set WA to be red, OK? So as before, we eliminate the other values from WA's domain, of course.
And then we enforce arc consistency on the neighbors of WA.
In this case, NT and SA.
So out goes red on both of these.
And now we continue to try to enforce our consistency on the neighbors of NT and SA.
But in this case, I can't actually eliminate anything.",6:04 why not? Why wasn't that possible
5wCZqdCDafc,"ROFESSOR: So connectivity is more than just an all or nothing affair.
We can talk about how connected a graph is.
So let's begin with two vertices.
Two vertices are said to be k-edge connected if they remain connected if you remove fewer than k edges from the graph.
Let's look at an example.
So here's a graph, and let's focus on those two vertices that I've highlighted in magenta.
They are 1-edge connected because they're connected, and if you remove one edge they become disconnected.
So they're 1-edge connected, but they're not 2-edge connected.
In particular, if I delete that edge, then they no longer is a path between the two magenta vertices.","Why do I think that the definition at the start (0:13) is slightly wrong. Shouldn't it be ""Vertices v and w are k-edge connected if they GET DISCONNECTED whenever fewer than k edges are deleted"" ? Please correct me :)"
6LOwPhPDwVc,"And then let's look at merge sort and do one more visualization of this.
Again let me remove that.
If we run it-- again, I've just put some print statements in there.
Here you can see a nice behavior.
I start off calling Merge Sort with that, which splits down into doing Merge Sort of this portion.
Eventually it's going to come back down there and do the second one.
It keeps doing it until it gets down to simple lists that it knows are sorted.
And then it merges it.
Does the smaller pieces and then merges it.
And having now 2 merged things, it can do the next level of merge.
So you can see that it gets this nice reduction of problems until it gets down to the smallest size.
So let's just look at one more visualization of that and then get the complexity.
So if I start out with this list-- sorry about that.
What I need to do is split it.
Take the first one, split it.
Keep doing that until I get down to a base case where I know what those are and I simply merge them.
Pass it back up.
Take the second piece.
Split it until I get down to base cases.
Do the merge, which is nice and linear.
Pass that back up.
Having done those two pieces, I do one more merge.
And I do the same thing.
I want you to see this, because again you can notice how many levels in this tree log.
Log in the size.
Because at each stage here, I went from a problem of 8 to two problems of 4.
Each of those went to two problems of 2, and each of those went to two problems of size 1.
All right.
So the last piece is, what's the complexity? Here's a simple way to think about it.
At the top level, I start off with n elements.
I've got two sorted lists of size n over 2.
And to merge them together, I need to do order n work.","42:12 merge isnt logn iterations its at least n iterations and at most 2n iterations. You can see at 39:49 that number of prints(iterations) isnt logn. 37:32 we arent cutting down the problem in half, its a tree 40:33. Its logn levels of iterations(not iterations themselves) where iterations at each level together have O(n) complexity(cost of each step(iteration) is O(n) but turns out that cost of steps at same level of a tree is also O(n) 41:24. He just mixes levels with steps 42:04 Normally what we do is we multiply number of steps(O(n->2n)) to their complexity(O(n)) but in this case we use the fact that at each LEVEL of a tree sum of conplexity of steps is O(n). So number of LEVELS in a tree (O(logn)) multipled with each levels complexity(O(n)) = O(nlogn)"
6LOwPhPDwVc,"Because as I said I got to do at least n comparisons where n is the length of the list.
And then I've got to do n plus n copies, which is just order n.
So I'm doing order n work.
At the second level, it gets a little more complicated.
Now I've got problems of size n over 4.
But how many of them do I have? 4.
Oh, that's nice.
Because what do I know about this? I know that I have to copy each element at least once.
So not at least once.
I will copy each element exactly once.
And I'll do comparisons that are equal to the length of the longer list.
So I've got four sublists of length n over 4 that says n elements.
That's nice.
Order n.
At each step, the subproblems get smaller but I have more of them.
But the total size of the problem is n.
So the cost at each step is order n.
How many times do I do it? Log n.
So this is log n iterations with order n work at each step.
And this is a wonderful example of a log linear algorithm.","42:12 merge isnt logn iterations its at least n iterations and at most 2n iterations. You can see at 39:49 that number of prints(iterations) isnt logn. 37:32 we arent cutting down the problem in half, its a tree 40:33. Its logn levels of iterations(not iterations themselves) where iterations at each level together have O(n) complexity(cost of each step(iteration) is O(n) but turns out that cost of steps at same level of a tree is also O(n) 41:24. He just mixes levels with steps 42:04 Normally what we do is we multiply number of steps(O(n->2n)) to their complexity(O(n)) but in this case we use the fact that at each LEVEL of a tree sum of conplexity of steps is O(n). So number of LEVELS in a tree (O(logn)) multipled with each levels complexity(O(n)) = O(nlogn)"
6LOwPhPDwVc,"Break the problem in half.
Keep doing it until I get sorted lists.
And then grow them back up.
So there's merge sort.
It says, if the list is either empty or of length 1, just return a copy of the list.
It's sorted.
Otherwise find the middle point-- there's that integer division-- and split.
Split the list everything up to the middle point and do merge sort on that.
Split everything in the list from the middle point on.
Do merge sort on that.
And when I get back those two sorted lists, just merge them.
Again, I hope you can see what the order of growth should be here.
Cutting the problem down in half at each step.","42:12 merge isnt logn iterations its at least n iterations and at most 2n iterations. You can see at 39:49 that number of prints(iterations) isnt logn. 37:32 we arent cutting down the problem in half, its a tree 40:33. Its logn levels of iterations(not iterations themselves) where iterations at each level together have O(n) complexity(cost of each step(iteration) is O(n) but turns out that cost of steps at same level of a tree is also O(n) 41:24. He just mixes levels with steps 42:04 Normally what we do is we multiply number of steps(O(n->2n)) to their complexity(O(n)) but in this case we use the fact that at each LEVEL of a tree sum of conplexity of steps is O(n). So number of LEVELS in a tree (O(logn)) multipled with each levels complexity(O(n)) = O(nlogn)"
6stKGH6zI8g,"This doesn't seem right? Uh, [LAUGHTER] uh, [NOISE] and that is- that's a good intuition to have.
[NOISE] But, uh, in this case, we- now- are now moving from meta-training- from training set to the meta-train sets, and test sets to meta test sets.
So each of these tasks, the training set and the test sets for the task correspond to the meta-training dataset.
And then at meta-test time were given new tasks.
Uh, and we don't wanna train on the meta-test set.
Okay.
Um, so we have our meta-learning data, this corresponds to training and test sets for every task, uh, where each of the training set- datasets corresponds to K data points.
Each of the test data- datasets correspond to a new set of K data points.
Um, yeah.
Okay.
So the complete, kind of, optimization problem is that at test-time we're gonna be inferring a set of task specific parameters.
Which can be some, some function that takes as input the training dataset and outputs the task specific parameters.
Where the parameters of that function or the meta parameters are theta star.
Um, and we essentially wanna learn a set of meta parameters such that, this function is good for held-out data points, after being- after ge- getting the training dataset as input.
Okay.
Um, so essentially you can view theta star as optimizing this, uh, objective [NOISE] where we want to optimi- optimize the, the probability of the parameters, um, being effective at new data points.","Meta learning is not complicated in practice, but the notion of the datasets in the literature is confusing, like 1:12:30. Neither support and query sets, nor meta training and test sets are clear. The first is not commonly used in regular neural networks. The second is confusing as it is long and might be confused with the regular model dataset (optimizee). Perhaps keeping training and test sets for the optimizee dataset, while using training and test tasks for the meta."
6stKGH6zI8g,"[inaudible] Yeah exactly.
So basically the weights that are, uh, right after the Z_i, the kind of, if you have a fully connected layer right after Z_i concatenated with your features, the, the part of that matrix that corresponds to, uh, that is basically right after Z_i will have basically different, um, different components that, that are not shared for each of the tasks.
Other than that half of that matrix, all the other parameters are shared.
Yeah.
There's also kind of forces [inaudible] Yeah.
Yeah, exactly.
So in this case, we assume that all the inputs have the same size, the same, uh, dimensions.
One thing that you could do is, uh, if, if different tasks have different sizes, you can basically like, you would have some sort of, uh, recurrent neural network, or some sort of attention based model that basically aggregates over the variable dimens- like if you- if one of them is time for example, it aggregates over that, uh, whereas maybe some tasks have- are text and others image.
Images in that case, you would probably wanna have different, um, different first parts of that network to take in those different modalities of data.
And we'll show- we'll see like an explicit example of, of how that has been done in the past.
Yeah.
So [inaudible] So this is, yeah, this is a good question.
It's, it's a f- it's a fairly nuanced point.
So basically each, um, you can ba- you can view, uh, the first- a fully connected layer corresponding to, um, a weight matrix times a vector.
And when you, um, when you take the, the top part of that matrix will correspond to- or sorry the left, um, rows of that- the left columns of that matrix will correspond to the, the features and the right columns of that will correspond to the task, uh, task vector that's being processed in this, in this input.","question of 17:15, the prof mentioned ""the parameter are shared except"". After the concatenation, is that all the full connected layer afterwards would be shared across different tasks?"
7lQXYl_L28w,"And inside of the loop, this is just constant.
It doesn't depend on the size of the integer.
So how many times do I go through the loop? Well, how many times can I divide i by 10? And that's log of i, right? So it's not i itself.
It's not the size of the integer.
It's the number of digits in the integer.
And here's another nice example of log.
I'll point you, again, right here.
I'm reducing the size of the problem by a constant factor-- in this case, by 10-- each time-- nice characteristic of a logarithmic algorithm.
OK, we've got constant.
We've got log.
What about linear? We saw it last time, right? Something like searching a list in sequence was an example of something that was linear.
In fact, most of the examples we saw last time were things with iterative loops.
So for example, fact, written intuitively-- factorial, right-- n times n minus 1 times n minus 2 all the way down to 1.",23:09 Aren't string immutables? So this means it will reassign the string res = digits[i%10] + res (by creating a new string). Wouldnt that mean creating a new string log(n) times.Hence shouldn't the complexity be (1 + 2 + 3 ....... log(n)) = (1+log(n))*log(n)/2 == O(log(n)*log(n))?
7lQXYl_L28w,"Something that grows linearly is not bad.
Something that grows, as we've seen down here, exponentially tends to say, this is going to be painful.
And in fact, you can see that graphically.
I'll just remind you here.
Something that's constant says, if I draw out the amount of time it takes as a function of the size of the input, it doesn't change.
Logarithmic glows-- gah, sorry-- grows very slowly.
Linear will grow, obviously, in a linear way.
And I actually misspoke last time.
I know it's rare for a professor to ever admit they misspeak, but I did.
Because I said linear is, if you double the size of the input, it's going to double the amount of time it takes.
Actually, that's an incorrect statement.
Really, what I should have said was, the increment-- if I go from, say, 10 to 100, the increase in time-- is going to be the same as the increment if I go from 100 to 1,000.
Might be more than double depending on what the constant is.
But that growth is linear.
If you want to think of it, take the derivative of time with respect to input size.
It's just going to be a constant.
It's not going to change within.
And of course, when we get down to here, things like exponential, it grows really fast.
And just as a last recap, again, I want to be towards the top of that.
There was my little chart just showing you things that grow constant, log, linear, log-linear, quadratic, and exponential.
If I go from n of 10, to 100, to 1,000, to a million, you see why I want to be at the top of that chart.
Something up here that grows logarithmically, the amount of time grows very slowly as I increase the input.
Down here, well, like it says, good luck.
It's going to grow up really quickly as I move up in that scale.","5:30 how? If its linear like k*n+b O(n) then if input(n) is doubled then its 2k*n+b. The worst case is almost doubled. How can get much more larger. 5:51 He said it can be more than doubled
Around 5:30 he says ""... linear is, if you double the size of the input, it's going to double the amount of time it takes. Actually, that's an incorrect statement. Really, what I should have said was, the increment--if I go from, say, 10 to 100, the increase in time is going to be the same as the incremeent if I go from 100 to 1000. Might be more than double depending on what the constant is. But that growth is linear."" I don't get the difference... he's saying 10->100 = 10x, 100->1000 = 10x, right? And the earlier version was 1->2 = 2x and 2->4 = 2x. Aren't these the same? What am I missing here?"
7lQXYl_L28w,"When you're given a new problem, how do I get this into a linear algorithm if I can? Log-linear, if I can, would be really great.
But you know, if I can't, how do I stay away from exponential algorithms? And finally, what we're going to show later on is that, in fact, there are some problems that, as far as we know, are fundamentally exponential.
And they're expensive to compute.
The very last thing is, you might have decided I was cheating in a different way.
So I'm using a set of built-in Python functions.
I'm not going to go through all of these.
But this is just a list, for example, for lists, of what the complexity of those built-in functions are.
And if you look through the list, they kind of make sense.
Indexing, you can go straight to that point.
Computing the length, you compute it once, you've stored it.
Comparison-- order n, because I've got to compare all the elements of the list.
Similarly, to remove something from the list, I've got to find where it is in the list and remove it.
Worst case, that's going to be order n.
So you can see that these operations are typically linear in the size of a list.
These are constant.
For dictionaries, remember, dictionaries were this nice thing.
They weren't ordered.
It gave me a power in terms of storing them.
But as a consequence, some of the costs then go up.
For a list, indexing, going to a particular point, I just go to that spot and retrieve it.
Indexing into a dictionary, I have to find that point in the dictionary that has the key and get the value back.
So that's going to be linear, because I have to, in principle, walk all the way down it.
It's a slight misstatement, as we'll see later on.
A dictionary actually uses a clever indexing scheme called a hash.
But in the worst case, this is going to be linear.",I think the complexity of finding the dictionary length should be O(1). Why is it O(n)? 48:27
8C_T4iTzPCU,"All right, I'm on a roll not just with Frisbees.
I finished the proof with a finger to spare.
So f of S T equals c of S T.
All right, so that's exactly what we want.
We are saying f of S T is obviously a cardinality of f so I've shown this thing over here.
So that's why the Ford-Fulkerson algorithm works.
It's because of this analysis that the Ford-Fulkerson algorithm works.
So are we done? What are we missing in algorithm design, our algorithm analysis? Not you, yet.
AUDIENCE: [INAUDIBLE] PROFESSOR: Sorry? AUDIENCE: A runtime? PROFESSOR: Runtime, good runtime.","in the proof from 17:00-35:00 Isn't he just proofing this for the special cut he defined? But the first part of the theorem states |f| = c(S,T) for ANY S,T cut"
8C_T4iTzPCU,"And how many iterations do you really need if you did it right? Two.
So that's a billion factor slowdown.
So this is a pathological example, a simple pathological example, to just show you what the problem is.
But you can imagine that if you use depth-first search you might be a factor of five slower on average than if you use some of the technique.
And a factor of 5 is nothing to be scoffed at, especially if you're running for minutes.
And, you know, back in the day computers were horribly slow.
So how is this problem fixed? Well, any number of ways.
But the first real way that took into account asymptotitc complexity, did analysis, and did all of that was due to Edmonds and Karp, which is a few years after Ford-Fulkerson.
In fact, several years after Ford-Fulkerson.
And their contribution was not as much a new algorithm, though it is called Edmonds-Karp algorithm.","at 42:49, I didn't understand why are there 2 billion iterations? Doesn't it keep oscillating between the two paths?"
8C_T4iTzPCU,"And the proof, the max flow, min-cut theorem, which is going to show this key result that we require, which is that when we terminate in the Ford-Fulkerson algorithm, we're going to have a max flow.
And that's the reason why it's a maximum flow algorithm.
If you don't have that proof, you don't have a max flow algorithm.
So hopefully all of that is clear.
Pipe up if you have questions.
And let's write out the max flow min-cut theorem, which I mentioned of it last time but never really got to even stating, but today we're going to state it and prove it.
So this is an interesting theorem.
I mean it's a legendary theorem.
And it's not your usual theorem in the sense that it makes a simple statement.
It actually makes three statements.
It says the following are equivalent, and it's got three things which are the following.","in the proof from 17:00-35:00 Isn't he just proofing this for the special cut he defined? But the first part of the theorem states |f| = c(S,T) for ANY S,T cut"
8C_T4iTzPCU,"So it's simply the S to T edges and summing over the capacities.
And if you take a look, obviously S to T, you've got 4 plus 4 plus 4.
So you've got 4 plus 4 plus 4, corresponding to this one, this one, and that one.
Do I need to add this edge in here? Over here? Where does this edge go? From this to over there? That goes from T to S.
So that's good because that has a capacity of infinity.
That would cause trouble.
And so the other edges are I got 1, 5, and 7.","1:21:00 Can someone help me out? srini said that it has capacity infinite in that case it makes sense in not considering it in min cut value cuz it is limited by the source (s), but we are going from 'S' to 'T' for those flows but the infinite flow is from ""T"" to ""S"" (S and T are the partition that we obtained after cut) so it must be -infinite ....why don't we consider the flow through cut here as we did in (https://youtu.be/VYZGlgzr_As?t=3288)....... I understood the capacity of the middle region is infinite because they are limited by the incoming flow from source(so that will be adjusted accordingly)."
8C_T4iTzPCU,"What are these pairs? As you can imagine, these pairs correspond to the games that each of these teams that are inside the circle play against each other.
So 3 plays 4 a certain number of times.
According to that table it's 4 times.
So I'm going to put a 4 in here.
This is 4 as well.
4, 5, 7, 2.
OK? And the edges in between here are going to have capacities of infinity and how are these edges structured? So far I've just explained how the left-hand side works.
These edges have a capacity of infinity.
1 2 goes to 1, 1 2 goes to 2.
1 3 goes to 1, 1 3 goes to 3.
And that's pretty much it.
So that's where these edges are.
That's how these edges are introduced.
And all of these edges have a capacity of infinity.
2 to 4, 3 to 4, and that.
So far, it's pretty straightforward.
There's one last thing that we need to do, which is add capacities to these edges.
This is actually crucial.
It turns out that we have to add capacities such that this max flow is going to represent elimination.","At 1:08:31, how the capacities of edges are infinite?"
8LEuyYXGQjU,"Okay, so what we've said here so far is that we have this approximation where what we do is we just take our policy, we run it out phi m times, for each of those m times we get a whole sequence of states and actions and rewards.
And then we average.
And this is an unbiased estimate of the policy gradient but it's very noisy.
So, this is gonna be unbiased and noisy.
If you think about what we saw before for things like Monte Carlo methods, it should look vaguely familiar, same sort of spirit, right? We have, um, we're just running out our policy.
We're gonna get some sum of rewards just like what we got in Monte Carlo, um, estimates.
But, [NOISE] so, it'll be unbiased estimate of the gradient.
So, it's unbiased estimate of the gradient, estimate of gradient.
But noisy.
So, what can make this actually practical? Um, there's a number of different techniques for doing that.
Um, but some of the things we'll start to talk about today are temporal structure and baselines.
[NOISE] Okay.","If you pause around 57:47 and work on the math yourself you will actually find the equation on the top (first line) does not equal to the last equation (on the bottom). To verifty this, you can use a simple case as a test example, such as a three time step episode with reward r0, r1, r2. So do not her derivation for granted as she is missing lots of details here. Why there is this dissonance here? The reason is because ""the policy's choice at a particular time step t only affects rewards received in later steps of the episode, and has no effect on rewards received in previous time steps"" (https://web.stanford.edu/class/cs234/CS234Win2019/slides/lnotes8.pdf). So the original equation on the top actually does not take this into account. It is imo a very important point that she did not mention here."
8NYoQiRANpg,"As well as how you make predictions is, um-uh, is expressed only in terms of inner products, okay? So we're now ready to apply kernels and sometimes in machine learning people sometimes we call this a kernel trick and let me just the other recipe for what this means, uh, step 1 is write your whole algorithm, [NOISE] um.
[NOISE] In terms of X_i, X_j, in terms of inner products.
Uh, and instead of carrying the superscript, you know X_i, X_j, I'm sometimes gonna write inner product between X and Z, right? Where X and Z are supposed to be proxies for two different training examples X_i and X_j but it simplifies the notation, uh, right a little bit.",12:00 discuss why \theta := \sum_{i=1}^n \beta_i \phi(x^{(i)}) is a reasonable assumption. 28:50 Kernel tricks 33:50 No free lunch theorem sucks
8NYoQiRANpg,"Right? So um, and it turns out that when X I is you know, 100 trillion dimensional, doing this will let us derive algorithms that work even in these  100 trillion or these infinite-dimensional feature spaces.
Now, I'm just deriving this uh, just as an assumption.
It turns out that there's a theorem called the representer theorem that shows that you can make this assumption without losing any performance.
Uh, the proof that represents the theorem is quite complicated.
I don't wanna do this in this class, uh, it is actually written out, the proof for why you can make this assumption is also written in the lecture notes, it's a pretty long and involved proof involving primal dual optimization.
Um, I don't wanna present the whole proof here but let me give you a flavor for why this is a reasonable assumption to make.
Okay? And when- just to- just to make things complicated later on uh, we actually do this.
Right? So Y I is always plus minus 1.",12:00 discuss why \theta := \sum_{i=1}^n \beta_i \phi(x^{(i)}) is a reasonable assumption. 28:50 Kernel tricks 33:50 No free lunch theorem sucks
8sOtXbQIOuE,"In general, the filtering distribution is the probability of the variables that you are considering conditioned on the evidence so far.
And suppose we have just two particles here, 0, 1, and 1, 2.
So now the propose step is going to take each of these particles, and I'm just going to sample a value for H3 the next variable-- given the transmission distribution.
Remember it was up and down with probability one quarter and the same with probability 1/2.
So that is going to produce these extended articles conditioned on the same evidence.
So for example, I'm going to take 0, 1.
I'm going to-- now that will produce this particle with probability 1/2 because I'm just keeping the value of the same year.
And I'm going to take this particle, and I'm going to extend it to 2.
And that's also going to happen with probability 1/2.
Now, this is a random algorithm.
So I could have sampled from the distribution.
I could have got 1 here.
I could have gotten 3 here, but let's just go with 1, 2.
So in the next step, I'm going to-- wait, so you should think about these particles as a guess as to what H3 is going to be.
But we need to fact-check this guess with evidence.
And so the weighting step is going to assign a weight to each article.
And that weight is going to be the probability of the new evidence I got conditioned on H3 here.
So this is going to produce a set of new particles, which are weighted representing the distribution-- the h1, h2, h3 conditioned on all the evidence so far.","at 10:07 example the probablities doesnt add up to 1, plz expln"
9TNI2wHmaeI,"And I'd like to talk a little bit about the relationship between NP-completeness and crypto.
Because we've made these assumptions about hardness.
Now, what's interesting here is that N composite is clearly in NP, but unknown if NP-complete.
So this is very interesting.
The tried and trusted algorithm for public key encryption relies on a computational assumption where the problem associated with that assumption is not even known to be NPC.
All right.
So that's kind of wild.
So how does this work? Or why does this work? Now, if you take other problems, like, is a graph 3-colorable? And so what does that mean? Well, you have three colors.
And you're not allowed to reuse the same color on two ends of an edge.
So if you put red over here, you can put red here, but you can't put red here and there.
And so that graph is 3-colorable.
But if you had a click, then this would not be 3-colorable.
Because you have all these edges.
You have three edges coming out.
And so clearly, the degree from a vertex is going to tell you what you have.
So if you have a 4-click over there, immediately it's not 3-colorable.
But checking whether a graphic is 3-colorable is NPC.
You can use a three set as a way of showing that.
So you can say, oh, wow, maybe I shouldn't be worried about RSA.","1:07:45 I don't think ""Is N composite?"" is the correct decision problem for integer factorization. Since ""PRIMES is in P"" via AKS, we can run AKS on an integer to answer if it's composite or not. -> Run AKS, return opposite of what it says. A correct decision problem for integer factorization would be something like: ""Given N, a, b, does N have a factor d such that a < d <= b?"" This is in NP because you can verify a factor in polynomial time (division algorithm). We can then factor an integer in polytime using a polytime oracle that solves this decision problem -- just binary search for factors starting from an interval (1, N-1] by consulting the oracle (we can restrict our search to prime factors using AKS), then divide out by prime factors as many times as possible whenever one is found. Return N as the only factor if the first oracle call returns `No`. This algorithm calls the oracle O(log N) times for each factor, and N has O(log N / log log N) unique factors (via a theorem of number theory), so all-in-all, this yields a polytime algorithm. Of course, you'd need a polytime oracle for this to actually be implemented, but I'll leave constructing such a thing to the experts. :)"
9g32v7bK3Co,"And again if you remember search problems, the solution to search problems was just a sequence of actions, said that's all I had, like a sequence of actions, a path that was a solution.
And the reason that was a good solution was like everything was deterministic, so I could just give you the path and then that was what you would follow.
But in the case of MDPs, the way we are defining a solution is by using this notion of a policy.
So a policy- let me actually write that here.
So we have defined an MDP but now I want to say well, what is a solution of an MDP? A solution of an Markov decision pro- process is a policy pi of S.","At 29:36, a policy is defined as a one-to-one mapping from the state space to the action space; for example, the policy when we are in station-4 is to walk. This definition is different compated to the one made in the classic RL book by Sutton and Barto; they define a policy as ""a mapping from states to probabilities of selecting each possible action."" For example, the policy when we are in station-4 is a 40% chance of walking and 60% chance of taking the train. The policy evaluation algorithm that is presented in this lecture also ends up being slightly different by not looping over the possible actions. It is nice of the instructor to highlight that point at 55:45"
9g32v7bK3Co,"Those are like, the only things I'm storing, because that allows me to compute and if I've converged then that kind of allows me to keep going because I only need my previous values to update my new values, right.
In terms of complexity, well this is going to take order of T times S times S prime.
Well, why is that? Because I'm iterating over T times step, and I'm iterating over all my states and I'm summing over all S primes, right.
So because of that- that's a complex idea yet, and one thing to notice here, is it- it doesn't depend on actions, right.
It doesn't depend on the size of actions.
And the reason it doesn't depend on the size of actions as you have given me the policy, you are telling me follow this policy.
So if you've given me the policy then I don't really need to worry about, like, the number of actions I have.
Okay.
All right.
Um, here is just another like the same example that we have seen.
So at iteration T equal to 1, in, is going to get 4, end is going to get 0, at iteration 2 it gets a slightly better value.","At 29:36, a policy is defined as a one-to-one mapping from the state space to the action space; for example, the policy when we are in station-4 is to walk. This definition is different compated to the one made in the classic RL book by Sutton and Barto; they define a policy as ""a mapping from states to probabilities of selecting each possible action."" For example, the policy when we are in station-4 is a 40% chance of walking and 60% chance of taking the train. The policy evaluation algorithm that is presented in this lecture also ends up being slightly different by not looping over the possible actions. It is nice of the instructor to highlight that point at 55:45"
A6Ud6oUCRak,"So let's go from here over to here and say that the probability of a whole bunch of things-- x1 through x10-- is equal to some product of probabilities.
We'll let the index i run from n to 1.
Probability of x to the last one in the series, conditioned on all the other ones-- sorry, that's probability of i, i minus 1 down to x1 like so.
And for the first one in this product, i will be equal to n.
For the second one, i will be equal to n minus 1.
But you'll notice that as I go from n toward 1, these conditionals get smaller-- the number of things on condition get smaller, and none of these things are on the left.","I have a question: in 28:45 he explain the general formula for conditional probabilities. However, if we apply it on the case P(a,b,c), where x1 = a, x2 = b, and x3 = c, we do not obtain the same result as the professor as the probabilities seem to be shifted in the other way. In other words, we finish with P(a) as the last term and not P(c). Can someone explain me where is my mistake ? Thank you"
A6Ud6oUCRak,"What's that mean? That means that if you know that we're dealing with z, then the probability of a doesn't depend on b.
b doesn't matter anymore once you're restricted to being in z.
So you can look at that this way.
Here's a, and here's b, and here is z.
So what we're saying is that we're restricting the world to being in this part of the universe where z is.
So the probability of a given b and z is this piece in here.
a given b and z is that part there.
And the probability of a given z is this part here divided by all of z.",Is the region of z at 35:25 correct? I think it should cover (a and b)
AbhV49lfaWw,"There are three parts.
And we also covered regularization before we go into regularization, so we've just discussed the three components.
We still haven't spoken of any trade-offs between them.
But this- this is the mental model to have in, you know, in your mind to decompose the- the test error into three parts, right? Part due to noise in the test data, that's irreducible error, part due to noise in the training data, that's variance and bias.
Now, we also spoke about regularization and we will see why regularization, er, plays a role in, um, um, shortly, soon.
So regularization is a way in which we want to penalize our estimated parameters from having very large values.","At 11:50 timestamp, we talk about ""noise"" in the training data that results in variance when we test different models on the same test data point. These different models are built when we take different samples. The noise that we are referring to, is it just due to different samples or a combination of different samples + irreducible error of each y_train(because each y has its own distribution, and in the case of regression, its gaussian) from the training data?"
Amd_bNYzgUw,"And this follows as a trivial consequence of the inclusion-exclusion rule for two sets, because the probability of A union B is equal to this plus this minus some probability, namely, the probability of the intersection.
So you're taking away something non-negative from these two in order to equal that.
In particular, then, this must be less than or equal to that.
And the closely related phenomenon is [? basi-- ?] [AUDIO OUT] The probability that A or B happens is greater than or equal to the probability that A happens.","8:06 should it's caption be ""union bound""? or ""boole's inequality""?"
B5y47gWt3co,"So the way we write this in terms of, um, uh, GIN operator is to say, aha, we are taking the messages from the children, we aggregate- we transform them using an MLP, this is our function f, and we summed them up.
Um, and then we also add 1 plus epsilon, where epsilon is some small, uh, learnable scalar, our own message transformed by f and then add the two together and pass through another function, uh, phi.",What is the use of the term (1 + epsilon) at 23:15 (slide 70)?
BZTWXl9QNK8,"So they're on a scale of minutes.
So if you connect within the same minute, then you're in good shape.
And if you connect on the minute boundary, well, too bad.
Yet another problem with the scheme-- it's imperfect in many ways.
But most operating systems, including Linux, actually have ways of detecting if there's too many entries building up in this table that aren't being completed.
It switches to this other scheme instead to make sure it doesn't overflow this table.
Yeah.
AUDIENCE: So if the attacker has control of a lot of IP addresses, and they do this, and even if you switch it the same-- PROFESSOR: Yeah, so then actually there's not much you can do.
The reason that we were so worried about this scheme in the first place is because we wanted to filter out or somehow distinguish between the attacker and the good guys.
And if the attacker has more IP addresses and just controls more machines than the good guys, then he can just connect to our server and request lots of web pages or maintain connections.
And it's very hard then for the server to distinguish whether these are legitimate clients or just the attacker tying up resources of the server.
So you're absolutely right.
This only addresses the case where the attacker has a small number of IP addresses and wants to amplify his effect.","If the timestamp is part of the message, then why does it matter if ""you connect on the minute boundary""? (1:08:32)"
C1lhuz6pZC0,"And if we know that, what's the order? AUDIENCE: [INAUDIBLE].
JOHN GUTTAG: N log n plus n-- I guess is order n log n, right? So it's pretty efficient.
And we can do this for big numbers like a million.
Log of a million times a million is not a very big number.
So it's very efficient.
Here's some code that uses greedy.
Takes in the items, the constraint, in this case will be the weight, and just calls greedy, but with the keyfunction and prints what we have.
So we're going to test greedy.
I actually think I used 750 in the code, but we can use 800.
It doesn't matter.
And here's something we haven't seen before.
So used greedy by value to allocate and calls testGreedy with food, maxUnits and Food.getValue.
Notice it's passing the function.
That's why it's not-- no closed parentheses after it.
Used greedy to allocate.","28:42 what is ""item"" that used for ?"
C1lhuz6pZC0,"So let's go look at the code that does this.
So here you have it or maybe you don't, because every time I switch applications Windows decides I don't want to show you the screen anyway.
This really shouldn't be necessary.
Keep changes.
Why it keeps forgetting, I don't know.
Anyway, so here's the code.
It's all the code we just looked at.
Now let's run it.
Well, what we see here is that we use greedy by value to allocate 750 calories, and it chooses a burger, the pizza, and the wine for a total of-- a value of 284 happiness points, if you will.","[36:00] I don't get why we get different answers in the greedy algorithms as long as we use the same items and the same key function It does local optimization, but it does not mean that local optimization is different each time we run the program given the same parameters"
C6EWVBNCxsc,"So each level, in fact, is going to be exactly n over b cost.
We should be a little careful about the bottom because the base case-- I mean, it happens that the base case matches this.
But it's always good practice to think about the leaf level separately.
But the leaf level is just m over b times n over m The m's cancel, so m over b times n over m.
This is n over b.
So every level is n over b.
The number of levels is log of n over m.
Cool.
So the number of memory transfers is just the product of those two things.","I am curious why in 45:42, the height is lgN - lgM rather than lg(N/B) - lg(M/B), although result is the same but a little confused."
CAKSh3M0y8k,"Now, I can also cancel k if it's relatively prime to n.
And the reason is that if I have ak equivalent to bk mod n and the gcd of k and n is 1, then I have this k prime that's an inverse of k.
So, I just multiply both sides by the inverse of k, namely k prime.
And I get that the left hand side is a times k, k inverse.
And the right hand side is b times k, k inverse.
And of course, that's a times 1 is equivalent to b times 1.",03:26 Where's the proof for associativity of modular multiplication then? ;>
CG4ihzTaGdM,"So y of minus 1 is x of minus 1 minus x of minus 2.
Since both of those are 0, it says that the output at time minus 1 is 0.
Trivial, right? Trivial.
And similarly, we can just iterate through the solution to the whole signal.
So y of 0 is x of 0 minus x of minus 1.
x of 0 is that special one, that is 1.
So now we get 1 minus 0, which is 1.
y of 1 is x of 1 minus x of 0.
Now the special one is on the other side of the minus sign, so the answer is minus 1.
y of 2 is x of 2 minus x of 1 -- they're both 0.
And in fact, all the answers from now on are going to be 0.
So what I just did is a trivial example of -- I use a difference equation to represent a system, and I figured out the output signal from the input signal.
That's the method that we call-- that's the representation for discrete time systems that we refer to as difference equations.","24:12 ??? Isn't x[2] = 2 Since on the x axis the numbers are: -1 for x[-1], 0 for x[0] and so on... If it isn't so, where do the x's come from? I don't get it, this is not trivial..."
CHhwJjR0mZA,"This is a bit circular.
I'm going to define an array in terms of the word RAM, which is defined in terms of arrays.
But I think you know the idea.
So we have a big memory which goes off to infinity, maybe.
It's divided into words.
Each word here is w bits long.
This is word 0, word 1, word 2.
And you can access this array randomly-- random access memory.
So I can give you the number 5 and get 0 1, 2, 3, 4, 5, the fifth word in this RAM.
That's how actual memories work.
You can access any of them equally quickly.
OK, so that's memory.
And so what we want to do is, when we say an array, we want this to be a consecutive chunk of memory.
Let me get color.
Let's say I have an array of size 4 and it lives here.
Jason can't spell, but I can't count.
So I think that's four.
We've got-- so the array starts here and it ends over here.
It's of size 4.
And it's consecutive, which means, if I want to access the array at position-- at index i, then this is the same thing as accessing my memory array at position-- wherever the array starts, which I'll call the address of the array-- in Python, this is ID of array-- plus i.
OK.
This is just simple offset arithmetic.
If I want to know the 0th item of the array, it's right here, where it starts.
The first item is one after that.
The second item is one after that.
So as long as I store my array consecutively in memory, I can access the array in constant time.
I can do get_at and set_at as quickly as I can randomly access the memory and get value-- or set a value-- which we're assuming is constant time.","If you cannot explain arrays without referring to physical structure, words and bits then you have failed. Even at 10:48 he gives the impression that array-elements are generic Words when an array really have elements of any size. My instructors 35 years ago did it better."
E-_ecpD5PkE,"So they're both important, they're doing slightly different things.
All right.
So let's talk about, ah, could we move this up, please? Thank you.
Okay.
So like we sort of started talking about before, um, well, let's- let's talk about first the baseline.
So how should we choose the baseline? Um, one thing that we can do for the baseline, is just to- like what that what we're seeing there, which is an empirical estimate of V_Pi i.
So we could say, in general, we wanna just have- use V_Pi i as a baseline.
That means we have to compute it somehow.
And the way we estimate that could be from Monte Carlo or it could be from TD methods.
All right.","1:03:06, why now the advantage function's definition becomes the subtration of two Q value functions? I though previously advantage function was defined as the subtraction of Q (state-action) and V (value) function?"
E3f2Camj0Is,"They don't have to be- This has- doesn't have to be anything to do with value iteration.
These are just two different value functions.
One could be, you know, 1,3,7,2 and the other one could be 5,6,9,8.
Okay.
So we just have two different vectors of value functions and then we re-express what they are after we apply the Bellman backup operator.
So there's that max a, the immediate reward plus the discounted sum of future rewards where we've plugged in our two different value functions.
And then what we say there is, well, if you get to pick that max a separately for those two, the distance between those is lower bounded than if you kind of try to maximize that difference there by putting that max a in.
And then you can cancel the rewards.
So that's what happens in the third line.
And then the next thing we can do is we can bound and say the difference between these two value functions is diff- is, um, bounded by the maximum of the distance between those two.
So you can pick the places at which those value functions most differ.
And then you can move it out of the sum.",Does anybody understand how did she get to 2nd step of the equation on 1:11:56?
E3f2Camj0Is,"Um, I in this case because we're thinking about processes that are infinite horizon, the value function is stationary, um, and it's fine if you have include self loops.
So, it's fine if some of the states that you might transition back to the same state there's no problem.
You do need that this matrix is well-defined.
That you can take that you can take the inverse of it.
Um, but for most processes that is.
Um, so, if we wanna solve this directly, um, this is nice it's analytic, um, but it requires taking a matrix inverse.
And if you have N states so let's say you have N states there's generally on the order of somewhere between N squared and N cubed depending on which matrix inversion you're using.
Yeah.
Is it ever actually possible for, uh, that matrix not to have an inverse or does like the property that like column sum to one or something make it not possible? Question was is it ever possible for this not to have an inverse? Um, it's a it's a good question.
Um, I think it's basically never possible for this not to have an inverse.
I'm trying to think whether or not that can be violated in some cases.
Um, if yeah sorry go ahead.
Okay.
[NOISE] Yeah.
So, I think there's a couple, um, if there's a- if this ends up being the zero matrix, um depending on how things are defined.
Um, but I'll double-check then send a note on a Piazza.
Yeah.
Well, actually I think the biggest side about the transition matrix [inaudible] Let me just double check so I don't say anything that's incorrect and then I'll just send a note on- on Piazza.","25:47 Conjecture: inverse exists if gamma in [0,1), and fails to exist if gamma=1. Easy to check for 1 or 2 state systems."
EC6bf8JCpDQ,"Ah.
But first of all, none of those expressions condition any of the variables on anything other than non-descendants, all right? That's just because of the way I've arranged the variables.
And I can always do that because are no loops.
I can always chew away at the bottom.
That ensures that whenever I write a variable, it's going to be conditioned on stuff other than its descendants.
So all of these variables in any of these conditional probabilities are non-descendants.
Oh wait.
When I drew this diagram, I asserted that no variable depends on any non-descendant given its parents.
So if I know the parents of a variable I know that the variable is independent of all other non-descendants.
All right? Now I can start scratching stuff out.
Well, let's see.
I know that C, from my diagram, has only one parent, D.
So given its parent, it's independent of all other non-descendants.
So I can scratch them out.
D he has two parents, B and R.
But given that, I can scratch out any other non-descendant.
B is conditional on T and R.
Ah, but B has no parent.
So it actually is independent of those two guys.
The trashcan, yeah, that's dependent on R.
And R over here, the final thing in the chain, that's just a probability.
So now I have a way of calculating any entry in that table because any entry in that table is going to be some combination of values for all those variables.","Thank you for this lecture! In the part where you're explaining the bottom-to-top approach starting minute 9:36 (chewing variables from the bottom), I noticed that you omitted the R variable from P(B | T , R) since B has no parents and is therefore not dependent on both T and R. Intuitively, this all makes sense to me, but there is the ""explaining away"" principle that links B to R since they both cause D. Given that D is correct, there is a relation between B and R. In other words, given that the dog barked, if there is a burglar, this explains away the theory of a Raccoon being present (and vice versa). My question is when and how is this ""explaining away"" principle used when modeling a system using belief networks? And if we are to use it, how is this relationship between B and R modeled? I would appreciate any input on this :)
The Markov blanket is used for something a bit stronger: we use it if we want conditional independence of a given node (A) to all other nodes. Then we must condition on the blanket of A. The blanket includes the nodes we saw in lecture 9:00 (parents and descendents of A), but it also includes the other parents of the descendents of A (if they exist). Intuitively, this is because a descendant of A surely depends on all of its parents. We better know the state of those parents if we want to claim how that descendant will behave with respect to A."
EK0sgHPLou8,"So here's one example for the Mixup.
We can generate some virtual examples between two classes.
So the first image in the left-hand side is for the cat.
And then next one is for dog.
And then we can combine them to generate some images in between.
So from cat and dog.
So this image has around the 70% probability to be classified as a cat and the 30% to be classified as a dog.
So this is a very common and a useful ways to do data augmentation.
Any questions about the process for the Mixup? So in domain generation, Mixup itself can improve the performance of domain generations.
So let's see.
We here, we want to do the tissue classification from Camelyon and also to do some learned type prediction.","48:54 how do we make sure that the final interpolated example is coherent, just use a low weight for the off class? or does that not matter much?"
EmSmaW-ud6A,"There's only one negative edge weight here.
What if I just added a large number, or in particular, the negative of the smallest edge in my graph to every edge in my graph? Then I'll have a graph with non-negative weights.
Fantastic.
Why is that not a good idea? Well, in particular, if I did that to this graph, if I added 2 to every edge, the weight of this path, which was the shortest path, changed from weight 3 to weight 9, because I added 2 for every edge.
But this path, which wasn't a shortest path in the original graph-- it had weight 4-- increased only by 2.
Now that is a shortest path.
Or it's a shorter path than this one, so this one can't be a shortest path.
So that transformation, sure, would make all the weights non-negative, but would not preserve shortest paths.","The telescope sum guarantees any path of ""a pair of vertices"" can be reweighted by the same value. But it doesn't maintain them in a global equally way to an entire graph. In other words, in a reweighted graph, the equality relation of edges could be changed. For example, A~D = 500 & B~Z = 300 may reweighted to A~D = 600 & B~Z = 700. So we cannot tell the edge relations rely on telescope-sum-reweighted graphs. This is the opposite of the intuitive add-weight-to-every-edge-till-non-negative method as at 22:22. That doesn't satisfying the reweighting requirement but it preserves the equality relation between edges. This is my blind spot, I was originally try to understand how it reweights while maintain the global relations."
EzeYI7p9MjU,"I mean, you could have defined it differently.
We're going to go with less than or equal to.
So in general, the rank, of course, is something that could be used very easily to find the median.
So if you want to find the element of rank n plus 1 divided by 2 floor, that's what we call the lower median.
And n plus 1 divided by 2 ceiling is the upper median.
And they may be the same if n is odd.
But that's what we want.","Did anyone find maybe the definition of rank at : 501 00:53:37,070 --> 00:53:53,880 And so in general, we're going to define, given a set of n numbers, define rank of x 502 00:53:53,880 -> 00:54:06,510 as the numbers in the set that are greater than- I'm sorry, less than or equal to x. 503 00:54:06,510 --> 00:54:09,270 I mean, you could have defined it differently. We're going to go with less than or equal 504 00:54:09,270 --> 00:54:10,750 to. is a typo? I checked the written note and find ""number of numbers in the set that are smaller than x"" makes more sense compared to rank defined on the black board in the video as ""numbers in the set that are smaller than x"" In short : ""number of numbers in the set"" versus ""numbers in the set """
EzeYI7p9MjU,"Let's say n is odd.
And it's floor of n over 2.
You can find that median.
Right, so it's pretty easy if you can do sorting.
But we're never satisfied with using a standard algorithm.
If we think that we can do better than that.
So the whole game here is going to be I'm going to find the median.
And I want to do it in better than theta n log n time.
OK, so that's what median finding is all about.
You're going to use divide and conquer for this.
And so in general, we're going to define, given a set of n numbers, define rank of x as the numbers in the set that are greater than-- I'm sorry, less than or equal to x.","Did anyone find maybe the definition of rank at : 501 00:53:37,070 --> 00:53:53,880 And so in general, we're going to define, given a set of n numbers, define rank of x 502 00:53:53,880 -> 00:54:06,510 as the numbers in the set that are greater than- I'm sorry, less than or equal to x. 503 00:54:06,510 --> 00:54:09,270 I mean, you could have defined it differently. We're going to go with less than or equal 504 00:54:09,270 --> 00:54:10,750 to. is a typo? I checked the written note and find ""number of numbers in the set that are smaller than x"" makes more sense compared to rank defined on the black board in the video as ""numbers in the set that are smaller than x"" In short : ""number of numbers in the set"" versus ""numbers in the set """
EzeYI7p9MjU,"PROFESSOR: O n-- exactly right.
So on test complexity-- and so we got over theta n cubed complexity, OK? So it makes sense to do divide and conquer if you can do better than this.
Because this is a really simple algorithm.
The good news is we will be able to do better than that.
And now that we have a particular algorithm-- I'm not quite ready to show you that yet.
Now that we have a particular algorithm, we can think about how we can improve things.
And of course we're going to use divide and conquer.
So let's go ahead and do that.
And so generally, the divide and conquer, as I mentioned before, in most cases, the division is pretty straightforward.
And that's the case here as well.
All the fun is going to be in the merge step.
Right, so what we're going to do, as you can imagine, is we're going to take these points.
And we're going to break them up.
And the way we're going to break them up is by dividing them into half lengths.
We're going to just draw a line.
And we're going to say everything to the left of the line is one sub problem, everything to the right of the line is another sub problem, go off and find the convex hull for each of the sub problems.
If you have two points, you're done, obviously.
It's trivial.
And at some point, you can say I'm just going to deal with brute force.
If we can go down to order n cubed, if n is small, I can just apply that algorithm.
So it doesn't even have to be the base case of n equals 1 or n equals 2.",why it is n3 at 20:45
EzeYI7p9MjU,"There's always a little bit of convenience thrown in here.
We will assume that the a has unique elements.
So there's nothing that's x, OK? Good.
So the recurrence, once you do that, is t of n equals-- we're going to just say it's order one for n less than or equal to 140.
Where did that come from? Well, like 140.
It's just a large number.
It came from the fact that you're going to see 10 minus 3, which is 7.
And then you want to multiply that by 2.
So some reasonably large number-- we're going to go off and we're going to assume that's a constant.
So you could sort those 140 numbers and find the median or whatever rank.
It's all constant time once you get down to the base case.
So you just want it to be large enough such that you could break it up and you have something interesting going on with respect to the number of columns.
So don't worry much about that number.
The key thing here is the recurrence, all right? And this is what we have spent the rest of our time on.
And I'll just write this out and explain where these numbers came from.
So that's our recurrence for n less than or equal to 140.
And else, you're going to do this.
So what is going on here? What are all of these components corresponding to this recurrence? Really quickly, this is simply something that says I'm finding the median of medians.",can somebody explain why is it T(n/5) and not 5T(n/5) in 1:17:29. Aren't we doing the recursion 5 times each step.
FgzM3zpZ55o,"So, what does planning involve? Involves optimization, often generalization and delayed consequences.
You might take a move and go early and it might not be immediately obvious if that was a good move until many steps later but it doesn't involve exploration.
The idea and planning is that you're given a model of how the world works.
So, your given the rules of the game, for example, and you know what the reward is.
Um, and the hard part is computing what you should do given the model of the world.
So, it doesn't require exploration.
And supervised machine learning versus reinforcement learning.
It often involves optimization and generalization but frequently it doesn't invo-, involve either exploration or delayed consequences.
So, it doesn't tend to involve exploration because typically in supervised learning you're given a data set.",11:41 I wonder why go game doesn't need exploration. In fact even human player would compute several steps after to see some possibilities.
FkfsmwAtDdY,"So now, I can start using these sets to make assertions about my database that can be useful to know.
So for example, if I want to say that every student is registered for some subject-- which, of course, they are-- what I would say is that D, the set of all students, is a subset of R inverse of J.
So this concise set theoretic containment statement-- d is a subset of R inverse of J-- is a slick way of writing the precise statement that says that all the students are registered for some subject.
Now, happens not to be true by the way.
Because if you look back at that example, Adam was not registered for a subject.","At 10:45, why not use D = R^(-1) (J) to indicate that every student is registered for some subject? It is against my intuition to use the ""is a subset of"" symbol because it is impossible for the domain of discourse to have fewer elements than the set of students who are registered for some subject."
FkfsmwAtDdY,"Which means that R of Jason is that set of two courses that he's associated with or that are associated with him-- that he's registered 6.042 and 6.012.
So at this point, we've applied R to one domain element-- one student Jason.
But the interesting case is when you apply R to a bunch of students.
So the general setup is that if x is a set of students-- a subset of the domain, which we've been showing in green-- then if I apply R to X, it gives me all the subjects that they're taking among them-- all the subjects that any one of them is taking.
Let's take a look at an example.
Well, another way to say it I guess is that R of X is everything in R that relates to things in X.
So if I look at Jason and Yihui and I want to know what do they connect to under R-- these are the subjects that Jason or Yihui is registered for.
The way I'd find that is by looking at the arrow diagram, and I'd find that Jason is taking 042 and 012.
And Yihui is taking 012 and 004.
So between them, they're taking three courses.
So R of Jason, Yihui is in fact 042, 012, and 004.
So another way to understand this idea of the image of a set R of X is that X is a set of points in the set that you're starting with called the domain.
And R of X is going to be all of the endpoints in the other set, the codomain, that start at X.
If I said that as a statement in formal logic or in set theory with logical notation, I would say that R of X is the set of j in subjects such that there is a d in X such that dRj.
So what that's exactly saying that dRj says that d is the starting point in the domain.
d is a student.
j is a subject.
dRj means there's an arrow that goes from student d to subject j.
And we're collecting the set of those j's that started some d.
So an arrow from X goes to j is what exists at d an X.
dRj means-- written in logic notation-- it's really talking about the endpoints of arrows, and that's a nice way to think about it.","Something is troubling me... At 6:53 the teacher uses the pipe symbol "" | "" and the dot symbol "" . "" but both are translated as "" such as "" Is there a difference between those two symbols ?"
FlGjISF3l78,"So you can have multiple levels of inheritance.
What happens when you create an object that is of type something that's been-- of a type that's the child class of a child class of a child class, right? What happens when you call a method on that object? Well, Python's are going to say, does a method with that name exist in my current class definition? And if so, use that.
But if not, then, look to my parents.
Do my parents know how to do that, right? Do my parents have a method for whatever I want to do? If so, use that.
If not, look to their parents, and so on and so on.
So you're sort of tracing back up your ancestry to figure out if you can do this method or not.
So let's look at a slightly more complicated example.
We have a class named Person.
It's going to inherit from Animal.
Inside this person, I'm going to create my own-- I'm going to create an __init__ method.
And the __init__ method is going to do something different than what the animal's __init__ method is doing.
It's going to take in self, as usual.
And it's going to take in two parameters as opposed to one, a name and an age.
First thing the __init__ method's doing is it's calling the animal's __init__ method.
Why am I doing that? Well, I could theoretically initialize the name and the age data attributes that Animal initializes in this method.","why to write Animal__init__(self, age ) at all? in 28:54"
G7mqtB6npfE,"We can use B to compute A, so then A must be easy.
But since we know that A is hard, that there's something wrong in our logic, so B must be hard.
Does that makes sense? Yes? Sort of? So let's move onto an actual problem.
So the first problem we're going to reduce is the Hamiltonian path.
So a well-known NP hard problem is the Hamiltonian cycle.
So here our A is-- so it's a Hamiltonian cycle.
So what's a Hamiltonian cycle? So what's a Hamiltonian cycle? So a Hamiltonian cycle-- so let's say you have a graph.
So we have this graph.
Let me draw this out.
That's it.
So a Hamiltonian cycle is a cycle in the graph which starts at some vertex, visits all the other vertices, and comes back to the starting vertex.
So in this case, we could do something like go here, and then take this vertex, take this vertex, take this vertex, and come back here.
So that is a valid Hamiltonian cycle.
So this graph is a Hamiltonian cycle.
So the decision problem is here that given the graph, does it have a Hamiltonian cycle? And that problem is NP-hard, so you can [INAUDIBLE] polynomial [INAUDIBLE].
So now the new polynomial shows NP-hard, which is B is Hamiltonian path.
So the Hamiltonian path is a very similar problem.
Instead of a cycle, you remove the requirement that you have to come back to the starting point.
You can just start anywhere and [INAUDIBLE] all the vertices and stop.
So for example, if you remove this edge, this graph no longer has Hamiltonian cycle, but it has a Hamiltonian path, which is just this line.
Simple.
So this is a simple reduction because the problems are very similar.
So the first step is, of course, showing that Hamiltonian path is an NP.
So that should be pretty clear because-- so what is our certificate here? So if someone says, OK, I have solved the Hamiltonian path and this is my Hamiltonian path.",5:40 how? Aren't we supposed to not repeat any vertex in Hamaltonion cycle?
GqmQg-cszw4,"In particular, lab one is going to rely on a lot of subtle details of C and Assembly code that we don't really teach in other classes here in as much detail.
So it's probably a good idea to start early.
And we'll try to get the TAs to hold office hours next week where we'll do some sort of a tutorial session where we can help you get started with understanding what a binary program looks like, how to disassemble it, how to figure out what's on the stack, and so on.
All right.
And I guess the one other thing, we're actually videotaping lectures this year.
So you might be able to watch these online.","Where can I learn ""what a binary program looks like, how to disassemble it, how to figure out whats on stack..."" (as mentioned in 4:20)? Also I wanted to know if it is possible to access the data stored in memory of one program using another program."
GqmQg-cszw4,"The system works.
But if I want to say that no one other than the TAs can access the grades file, this is a much harder problem to solve, because now I have to figure out what could all these non TA people in the world to try to get my grades file, right? They could try to just open it and read it.
Maybe my file system will disallow it.
But they might try all kinds of other attacks, like guessing the password for the TAs or stealing the TAs laptops or breaking into the room or who knows, right? This is all stuff that we have to really put into our threat model.
Probably for this class, I'm not that concerned about the grades file to worry about these guys' laptops being stolen from their dorm room.
Although maybe I should be.
I don't know.
It's hard to tell, right? And as a result, this security game is often not so clear cut as to what the right set of assumptions to make is.
And it's only after the fact that you often realize, well should have thought of that.
All right.","22:38 how threat models go wrong?
Is there a more detailed explanation of that atoi conversion that writes 0 1:01:00 ?"
IM9ANAbufYM,"There are other methods that you should be able to figure out right now.
Even if you don't know class activation maps.
[NOISE] So to sum it up.
[NOISE] We have an image, [NOISE] input image, [NOISE] put it in your new network that is a binary classifier.
[NOISE] And the network says one.
You wanna figure out why the network says one, based on which pixels, what do you do? Visualize the weights.
[NOISE] Visualize the weights.
Uh, what do you visualize in the weights? The edges.
So I think visualizing the weights, uh, is not related to the input.
The weights are not gonna change based on the input.
So here you wanna know why this input led to one.","1:17:39 he says visualize the weights, which infact is true. Because dY/dX actually gives weight transpose (same shape as input image), so what we are actually doing is visualising the weight. what am I missing here?"
IPSaG9RRc-k,"Can someone tell me what this is, asymptotically? Yeah? STUDENT: n cubed-- JASON KU: n cubed-- why is that? Well, if we plug this stuff into that definition here, we have n factorial over 3 factorial n minus 3 factorial.
n factorial over n minus 3 factorial just leaves us with an n, an n minus 1, and an n minus 2 over 6.
And if you multiply all that out, the leading term is an n cubed, so this thing is asymptotically n cubed.
I skipped some steps, but hopefully you could follow that.
And then the last thing to remain is this one right there.
That one's a little tricky.
Anyone want to help me out here? What we can do is we can stick it into this formula, and then apply Sterling's approximation to replace the factorials.
That makes sense? OK, so what I'm going to do is-- let's do this in two steps.
This is going to be n factorial over-- what is this? n/2 factorial-- and then what is n minus n over 2? That's also n/2.
So this is going to be n/2 factorial squared.
Is that OK? Yeah? Now let's replace this stuff with Sterling's approximation and see if we can simplify.
So on the top, we have 2 pi n n/e to the n over-- and then we've got a square here, pi n.
I cancelled the 2-- n/2 over e to the n/2.
Did I do that right? OK.
I can't spell, and a lot of times, I make arithmetic errors, so catch me if I am doing one.
OK, so let's simplify this bottom here.
I'm not going to rewrite the top.
The bottom here-- we square this guy.
It's the pi times n.
And then this guy, n/2 squared-- that just stays is an n.
Then we have n/2 to the n/e-- something like that.
n/2 over e to the n-- that makes more-- me happier.
OK, so now we have this over this.
How do we simplify? Well, we can cancel out one of the root n's.
So we've got square root of pi n down here and square root of 2 up top.
And then what have we got? We've got n to the n down here and n to the n down-- up there, so those cancel.","why (n 3) = (n^3)? Can someone explain me this moment please? On 16:20
How does he get n+1 n and n-2 over 6 16:05, I get how that will give us n cubed just how he got that from that I don't follow"
IPSaG9RRc-k,"I'm remembering that.
Now, I don't care about what's stored in x.next, because I've stored it locally.
That makes sense.
All right, so now I am free to relink that next pointer to my previous guy.
And now I can essentially shift my perspective over, so the thing that I'm going to relink now is the next one.
So x previous and x now equals x, x_next.
Does that make sense? Just relinked things over-- so that's the end of step 2.
Now, as I got down this at the end of this for loop, where is x? What is x_p, x, and x_next-- or x_n? Really, I'm only keeping track of x and x_p here.
So what are x_p and x at the end of this loop? I've done this n times.
I started with b at x.
So what is x? Yeah? So we have a vote that x is c.
STUDENT: [INAUDIBLE] JASON KU: So this is a little interesting.
All right.
I will tell you that c is either x_p, x, or x_n.
So we have one vote for x.
Who says something else? Eric doesn't like x.
There are only two other choices.",Can someone pls explain line 11 a1 1:23:13? Thanks
IPSaG9RRc-k,"OK, so that's the first thing.
Otherwise, what do we do? We shift one thing over and then we make a recursive call.
Does that make sense? OK.
So we'll delete the first thing as a temporary variable-- delete first.
And then we'll insert last, x.
And then we need to do the recursive call.
So what's a recursive call look like? Yeah? STUDENT: Shift_left D, K minus 1-- JASON KU: Yeah.
So shift_left D, K minus 1-- OK? And then we can return.
This thing doesn't need to return anything.
It's just doing stuff to the thing.
Right? And whenever we get this K, we make a call, that gets down to 0, we will terminate because we will return.
We're in this range somewhere between we-- have an input after this line.
We know that K is somewhere between 1 and n minus 1.
And what we'll do is, every time through this recursion, we will subtract 1 from K.
So this is a nice, well-ordered sequence.
We do the correct thing obviously in the base case, and as long as this thing was correct for a smaller value of K, this thing also does the correct thing, because we're shifting over one, as we are asked, and we're letting this do the work of the rest.
I don't have to think about that.
I just have to think about this one loop, this one part of the thing that I'm doing.
Constant amount of work is done in this section.
And how many times do I call a function? STUDENT: [INAUDIBLE] JASON KU: Yeah.",Can anyone explain to me how the first element of a data structure gonna be the last element after applying the algorithm on 00:44:00?
Ih0cPR745fM,"Clearly, by the pigeonhole principle if nobody goes to 0, there must be two guys that collide.
So this problem is also total by the pigeonhole principal.
So it always has a solution, no matter what the circuit is.
And the class PPP is all problems in NP that are reducible to this problem.
Finally, the hierarchy of problems I defined is this.
P, FNP, there is total FNP somewhere here, which I don't show.
And these are the relationships of these problems.
I haven't shown that these arrows true, that this is a subclass, PPD's a subclass of these two subclasses.
This is easy.
This is a simple exercise we can think about.
This is basically my introduction to PPA, PPAD and related classes.
The final thing that I want to point out is answering a question that was asked after the previous lecture, which was why did you define these classes, and not just a TFNP complete problem.
Why did you have to pay special attention to precise existence argument that gives rise to the guarantee that your problems are total? And the reason for that is that actually TFNP is not what's called a syntactic class.
In other words, if I give you a problem with TFNP-- if I give you a Turing machine, you cannot decide whether that is computing a total problem, that no matter what the input to that machine is, there's always an output.
So I had to pay attention to the specialized existence arguments because for specialized existence arguments, I know a priori that the problem is total.
So in particular, no matter what circuit I give you here, I don't even have to check anything.
No matter what input you give me, I know there is a solution.
No matter what pairs of circuits you give me here, I don't need to check anything.
I know the answer to this problem-- there's always an answer to this problem, and so on, so forth.
No matter what you give to me as input, it is important to define complexity classes for which you can show hardness results to find complete problems.
Otherwise, you would have what is called promise classes, which are not amenable to showing the completeness results.","in 1:20:16, does the hierarchy imply that P is a subclass of PPAD? Is that true or the P should be FP?"
IiD3YZkkCmE,"But they're also codified things, like if you see that the rheumatoid factor in a lab test was negative, then-- actually, I don't know why that's-- oh, no, that counts against-- OK.
And then various exclusions.
So these were the things selected by our regularized logistic regression algorithm.
And I showed you the results before.
So we were able to get a positive predictive value of about 0.94.
Yeah? AUDIENCE: In a the previous slide, you said standardized regression coefficients.
So why did you standardize? Maybe I got the words wrong.
Just on the previous slide, the-- PETER SZOLOVITS: I think-- so the regression coefficients in a logistic regression are typically just odds ratios, right? So they tell you whether something makes a diagnosis more or less likely.
And where does it say standardized? AUDIENCE: [INAUDIBLE].
PETER SZOLOVITS: Oh, regression standardized.
I don't know why it says standardized.
Do you know why it says standardized? KATHERINE LIAO: Couple of things.
One is, when you run an algorithm right on your data set, you can't port it using the same coefficients because it's going to be different for each one.
So we didn't want people to feel like they can just add it on.
The other thing, when you standardize it, is you can see the relative weight of each coefficient.","The correct answer to the question at 16:20: Why did you standardize the predictors? Is that you should always standardize your data before feeding them to a regularized regression model (mentioned at 16:03). This is because a regularized model will penalize large coefficients; and because you don't want the magnitude of the coefficient to depend on the scale of measurement of the predictor, you standardize them."
J8Eh7RqggsU,"Uh, this should be outside of the recurse object.
Yeah.
Glad you guys are paying attention.
Um, otherwise, yeah, it would do basically nothing.
Any other mistakes? [LAUGHTER] Yeah.
Um, there is also function decorators that like implement memoizing for you.
In this class, are you okay if we use that or would you rather us like make our own in this case? Um, you can use the deco- you can be fancy if you want.
Okay.
Um, yeah.
But- but I think this is, you know, pretty transparent.
Easy for learning purposes.
Okay.
So let's run this.
So now it runs instantaneously as opposed to- I actually don't know how long it would have taken otherwise.
Okay.
And sanity check for t is probably the right answer because there's four was the original answer and multiply by 10.",I didn't understand how the cache works. Can someone explain please? 1:14:47
JDW82csukhE,"Uh, and also, um, another interesting side note is that, uh, we fix the pre-training strategy and the pre-training different GNNs models, uh, use different GNNs models for pre-training.
And what we found out is that the most expressive GNN model, namely GIN, that we've learned in the lecture, um, um, benefits most from pre-training.
Uh, as you can see here, the gain of pre-trained model versus non-pre-trained model is the- is the largest, um, in terms of accuracy.
And- and the intuition here is that the expressive GNN model can learn to capture more domain knowledge than less expressive model, especially learned from large amounts of, uh, data during pre-training.
So to summarize of our GNNs, we've, uh, said- learned that the GNNs have important application in scientific domains like molecular property prediction or, um, protein function prediction, but, uh, those application domains present the challenges of label scarcity and out-of-distribution prediction, and we argue that pre-training is a promising, uh, framework to tackle both of the challenges.
However, we found that naive pre-training strategy of this, uh, supervised graph level pre-training gives sub-optimal performance and even leads to negative transfer.
And our strategy is to, um, effective strategy is to pre-train both node and graph embeddings, and we found this strategy leads to a significant performance gain on diverse downstream tasks.
Um, yeah, thank you for listening.
","18:30 Seems weird that the GAT performs the worst, and benefits the least from pretraining. Isn't it more expressive than GCN and GraphSAGE in theory?"
KlQiwkhLBg0,"But in the problem, in the homework problem, they were sneaky.
And they only said that f of T is big O of something.
So remember, what's the difference between big O and big theta? Well, intuitively, big theta says that my function really does look like this guy.
Somehow, it's bounded above and below as I go far enough out.
In big O, there's just a bound above, right? So this is somehow looser.
And so the way to apply master theorem in this case is say, well, at least f of n is upper bounded.
It looks like this.
So the best that I can do is to replace this guy also with an upper bound.
Yeah.","51:15 I think it should be 2^m-1 < k  2^m If 2^m-1  k then it wouldn't go to 2^m Correct me if I am wrong
48:30 This question about finding planets with the index K. Provided that the key indices are not duplicate can't we simply take left as 1 and right as K. since we know that the K key will never to at an index greater than K.
At 1:18:47, wouldn't the last house also be special because is is a house that has no easterly neighbor? Therefore, the sequence actually have two special houses instead of all but one. Maybe my understanding to all but one is wrong."
KsHOdr5UYZ0,"If you have tasks that are complex or require a lot of thinking and processing data, those are problems where AI can help.
If you have a problem with drug overdoses or with suicides, AI is probably not going to help.
And in fact, I've heard of cases where people collect data, and they do all of this quantitative analysis, but this is fundamentally a human problem.
And so it can be actually quite distracting.
So my point is simply that, at its best, AI can make the Air Force more efficient, more effective.
At its worst, it can distract us from actually solving difficult, challenging human and cultural problems.
So it's really important to be discerning about what kind of problem you're trying to solve and where AI can help.
But at this point, I hope, and I think that you have the tools to prevent that from happening, and to harness this very powerful new technology in support of building a more effective Air Force and pursuing America's interests.
So thank you all so much.
I'm going to stick around for questions, and I will leave you with this conclusion slide.
","39:30 ""If you have a problem with drug overdoses or with suicides AI is probably not gonna help""- I strongly disagree. Can someone tell me why he says that AI can't solve human problems? I mean I there is already proof that proves AI can help on those types of problems. I really don't know why he would say that."
KzH1ovd4Ots,"So you take two vectors and construct a matrix out of them.
All right, by- um, pick the- pick the ith, um, element from this vector, jth element from this vector, multiply them, and that becomes the ij element of- of- of the matrix, right? So this- this, uh, for the outer product, you don't need the vectors to have the same dimension.
And a matrix that you construct from one row vector and one column vector is also called a rank one matrix, right? Why is it called rank one? Because one way to think of it is it is made of one pair of a row and a column vector, right? So that makes it, um, um, what's, uh, what's also called as a- a rank one matrix.",I think at 46:40 the explanation for a rank 1 matrix should be that the columns of the resulting matrix are just scaler multiples of the first column. Hence the dimension of the column space is 1.
KzH1ovd4Ots,"Uh, now, next we are going to limit ourselves to only square matrices, which means the input and output spaces have the same dimensions, and for the purpose of visualization, we're gonna only consider, let's say, a three-dimensional space.
Right? Now, in this diagram we have two different, um, um, um, two different pictures for the input space and the output space.
But now, because A is symmetric, I'm sorry, A is- is, um, a square matrix, we're gonna overlay the input and output space onto the same space, right? So here, the input space and output space are- are being overlaid here.
Now let's ask the question- let's, you know, A, you know, let's ask the question.
We saw what happened, uh, for, you know, pick, you know, choose some points, you know, run it through A, you get an output, uh, uh, output point.
Now, what happens if we take the unit sphere around the origin? By unit sphere, I mean, just to the points on the surface of the unit sphere.
Think of it as a soccer ball at the center on the origin.
And you take every point on the surface, run it through A, you get a corresponding output point for every, you know, input point of- of- of the soccer ball, right? How would the resulting shape look? Right? So that's gonna look as an ellipsoid.
It's- it's, you know, almost like an ellipse.
So that's gonna be- right? So what- what- what exactly happened here? We- it's- it's a three-dimensional input and output space.
We started with the input as- we didn't have one input, but we had a collection of points as inputs.
And that collection was precisely those points that live on the surface of a unit sphere.
And let's say we- we took this point in the input space, run it through A, we got a corresponding out point, and that point, say, this one.
Right? So every point on the input surface maps to some point on the output surface of that shape, right? We could have done this with any input shape, but, you know, sphere is easy to kind of analyze, right? Now, similarly, um, you do it for another point, um, let's say this point, and let's say that maps here, and let's say we pick- what color do we have? Green- green, and let's say we pick this point and that maps here.
Okay? Now, we saw what- what- what- what happens when you- when we, uh, take some shape, for example, a sphere and run it through A, instead of thinking of um, um, running a point through A.","Great Lecture. One question in last part 1:44:30, why we are getting only ellipsoid output for sphere shape input, not any other shape? Does it have to do something with the assumption of A being symmetric?"
L5uBeAGJV1k,"And if you want to skip the short discussion of the meta theorems, that's fine, because it's never going to come up again in this class.
So let's look at this phrase in English, where the poet says, ""all that glitters is not gold."" Well, a literal translation of that would be that, if we let G be glitters, and I can't use G again, so we'll say Au is gold, then this translated literally would say for every x, G of x, if x is gold implies that not gold of x.
So is that a sensible translation? Well, it's clearly false, because gold glitters like gold.
And you can't say that gold is not gold.
So this is not what's meant.
It's not a good translation.
It doesn't make sense.
Well, what is meant, well, when the poet says, ""all that glitters is not gold,"" he's really leaving out a key word to be understood from context.
All that glitters is not necessarily gold.
He was using poetic license.
You're supposed to fill in and understand its meaning.
And the proper translation would be that it is not true that everything that glitters is gold.
It is not the case that for all x, if x glitters, then x is gold.
So it's just an example where a literal translation without thinking about what the sentence means and what the poet who articulated this sentence intended will get you something that's nonsense.
It's one of the problems with machine translation from natural language into precise formal language.","At 2:46, is it equivalent to say ""There exists an x such that (G(x) AND NOT Au(x))""? I think the logic is essentially the same, but I wonder if it's subtly different in that it may preclude the possibility of an empty set?"
MEz1J9wY2iM,"And so we're going to have two phases here in this particular approximation scheme.
The first phase is find an optimal partition, A prime, B prime, of S1 through Sm.
And we're just going to assume that this exhaustive search, which looks at all possible subsets, and picks the best one.
OK? And how many subsets are there for a set of size m? It's 2 raised to m.
So this is going to be an exponential order, 2 raised to m algorithm.
OK? I'm just going to find the optimum partition through exhaustive search for m.
Right? m is less than n.
So I'm picking something that's a smaller problem.
I'm going to seed this.
So, the way this scheme works is, I'm seeding my actual algorithm-- my actual heuristic-- with an initial partial solution.","It feels like the prof made a mistake when explaining aprox partition. He first said (when he was building the algorithm at 1:03:52) m < n, and then when he was proving he said imagine that the second part never executed because m is large. However the only case in which m never executes is if m = n, since otherwise there will still be leftover elements that are not added to any partition."
MEz1J9wY2iM,"I'm constantly taking stuff away.
When xk equals 0, I'm going to be done.
OK? And I'm going to move a little bit between discrete and continuous here.
It's all going to be fine.
But what I have is, if I just take that, I can turn this.
This is a recurrence.
I want to turn that into a series.
So I can say something like 1 minus 1 over t, raised to k, times n.
And this is the cardinality of x, which is the cardinality of x0.
So that's what I have up there.
And that's essentially what happens.
I constantly shrink as I go along.
And I have a constant rate of shrinkage here.
Which is the conservative part of it.
So keep that in mind.
But it doesn't matter from an analysis standpoint.
OK? So if you look at that, and you say, what happens here? Well, I can just say that this is less than or equal to e raised to minus-- you knew you were going to get an e, because you saw a natural algorithm here, right? And so, that's what we got.
And basically, that's it.
You can do a little bit of algebra.
I'll just write this out for you.
But I won't really explain it.
You're going to have Xk equals 0.
You're done.
The cost, of course, is k.
Right? The cost is k, because you've selected k subsets.
All right, so that's your cost, all right? So, when you get to the point, you're done.
And the cost is k.
So what you need is, you need to say that e raised to minus kt divided by n is strictly less than 1.
Because that is effectively when you have strictly less than 1 element.
It's discrete.
So that means you have zero elements left to cover.
That means you're done OK? So that's your condition for stopping.
So this done means that e raised to minus kt times n is strictly less than 1.","53:50, how do you get (1-1/t)^k <= e^(-k/t) ?"
MH4yvtgAR-4,"Basically here is the prediction of the label for a- for a- uh, for a given color whether it's toxic or not, this is whether it is truly toxic or not, um, and then the way you can think of this is y takes value one if it's toxic, and zero if it's not.
If the true value is zero, then this term is going to survive and it's basically one minus the lock predicted- prob- uh, one minus the predicted probability.
So here we want this to be the predicted probability- to be as small as possible so that one minus it becomes close to one because log of 1 is 0, so that this discrepancy is small.
And if the- if the, uh, class value is one, then this term is going to survive because 1 plus 1- 1 minus 1 is 0 and this- this goes away.
So here we want this term to be as close to one as possible.
Which again would say, if it's- uh, if it's toxic, we want the probability to be high.
If it's not toxic, we want the probability [NOISE] to be low- uh, the predicted probability, uh, of it being toxic.
And this is the cross, uh, entropy loss.
So this is the encoded input coming from node embeddings.
Uh, these are the classification rates, uh, for the final classification.
Uh and these are the, uh, node labels, is it basically toxic, uh, or not.",Shouldnt the loss function in 31:42 have a minus sign?
Mi8wnYc1m04,"Yeah, this over here should be small y.
Thank you.
Yeah, so you, um, um, so now we have, uh, something called the law of total expectation, right? So the law of total expectation tells us that, expectation of X can be written as the expectation of- expectation of X given Y, right? Now, this holds true for any X and any Y, right? Y could be completely independent of X, it could be dependent on X.
But the expectation of X can always be decomposed into this nested form where, um, you condition on Y, you get a new random variable, right? And you take the expectation of- of this random variable and you get back the expectation of X.","57:30, Is it true that E [X|Y] may or may not be normally distributed?"
MjbuarJ7SE0,"So I'm going to just temporarily hop out of the scope and see is there variable x outside of me? And it'll find this variable x here, and it's going to print out its values.
So that's OK.
This last example here is actually not allowed in Python-- similar to this one-- except that I'm trying to increment a value of x, but then I'm also trying to reassign it to the same value of x.
The problem with that is I never actually initialized x inside h.
So if I said-- if inside h, I said x is equal to 1, and then I did x plus equals to 1, then it would be this example here-- f of y.
But I didn't do that.
I just tried to access x and then incremented and then tried to reassign it.
And that's actually not allowed in Python.
There is a way around it using global variables.
But it's actually frowned upon to use global variables, though global variables are part of the readings for this lecture.
And the reason why it's not a great idea to use global variables is because global variables sort of give you this loophole around scopes, so it allows you to write code that can become very messy.","In the last example, where x is not defined in function g(x) should be giving an error (UnboundLocalError: local variable 'x' referenced before assignment ), isn't it? Like how the example shown in 32:42"
MjbuarJ7SE0,"So h returns None.
Back to whoever called it, which was this code inside g.
So that gets replaced with None-- the thing that I've-- this circled red h here.
As soon as h returns, we're going to get rid of that scope-- all the variables created within it-- and we're done with h.
So now we're back into g.
And we just finished executing this and this got replaced with None.
We're not printing it out, so this doesn't show up anywhere; it's just there.
So we're finished with that line.
And the next line is return x.
So x inside g is 4, so 4 gets returned back to whoever called it, which was in the global scope here.
So this gets replaced with 4.
So once we've returned x, we've completely exited out of the scope of g, and we've come back to whoever called us, which was global scope and we've replaced z is equal to g of x and that completely got replaced with 4-- the returned value.
So that's sort of showing nested functions.
All right just circling back to decomposition-abstraction.
This is the last slide.
You can see if you look at the code associated with today's lecture, there are some other examples where you can see just how powerful it is to use functions.
And you can write really clean and simple code if you define your own functions and then just use them later.","39:49 If the function h() had a return at the end, would the outer function have returned x as 'abc' instead of 4?"
MjbuarJ7SE0,"So using global variables, you can be inside a function and then modify a variable that's defined outside of your function.
And that sort of defeats the purpose of functions and using them in writing these coherent modules that are separate.
That said, it might sometimes be useful to use global variables, as you'll see in a couple lectures from now.
OK cool.
So let's go on to the last scope example.
OK this slide is here, and notice I've bolded, underlined, and italicized the Python Tutor, because I find it extremely helpful.
So the Python Tutor-- as I've mentioned in one of the assignments-- it was actually developed by a grad student here, or post-grad student slash post-doc here.",34:00 what is the difference between incrementing x and printing x? it is defined in the same place relative to g() and h()
N2lwsB1qfJw,"And I ask the question, how close is y hat_ i to y_i? And the performance metric is going to be the measure of how close y hat_i is to y_i.
And normally it's designed so that the smaller the metric, the better the prediction performance.
It's an error measure.
Some people call it prediction performance metric, the prediction error as a result.
So there are a few which are very commonly used.
The first one is the mean square error.
So this here, uh, is the two-norm.
Let me just mark it here.","Hi guys, in the slice of the time 2:30 demos different prediction metrcs. However a type of notation is confusing (may be a silly question): when prof said y_i or y hat_i, what is the subscript i? It seem an index symbol however which is supposed to put in the bottom right. What was shown is in the top right position of y. Much appreciate if any guy got an answer, thanks."
Nsc0Yluf2yc,"And the way we do that as I said is simply by taking the dark green representations of the output layer here and using them as inputs to subsequent blocks so they get attended to, and we proceed with the subsequent regularization and feed-forward steps just as before.
And when you work with these models in Hugging Face, if you ask for all of the hidden states, what you're getting is a grid of representations corresponding to these output blocks in green here.
And of course, just as a reminder I'm not indicating it here but there is actually multi-headed attention of each one of these blocks through each one of the layers.
So there are a lot of learned parameters in this model, especially if you have 12 or 24 attention heads.
At this point, I'm hoping that you can now fruitfully return to the original Vaswani, et al paper and look at their model diagram and get more out of it.
For me, it's kind of hyper compressed but now that we've done a deep dive into all the pieces, I think this serves as a kind of useful shorthand for how all the pieces fit together.","Is there a mistake at 09:50? ""You have 12 or 24 attention heads""; shouldn't that be 12 or 24 layers with as many attention heads as the length of tokens in the input / output sequence? Also, this is VERY well done lecture series! We will probably have our own NLU course at our university based on these materials! This is a huge service for the next generation of natural language related data scientists!"
Nu8YGneFCWE,"I want to generalize this to be a family of hash functions, which are this habk for some random choice of a, b in this larger range.
All right, this is a lot of notation here.
Essentially what this is saying is, I have a has family.
It's parameterized by the length of my hash function and some fixed large random prime that's bigger than u.
I'm going to pick some large prime number, and that's going to be fixed when I make the hash table.
And then, when I instantiate the hash table, I'm going to choose randomly one of these things by choosing a random a and a random b from this range.
Does that makes sense? AUDIENCE: [INAUDIBLE] JASON KU: This is a not equal to 0.
If I had 0 here, I lose the key information, and that's no good.",Universal hash function hash(k) = (((ak + b) mod p) mod m) at 41:31 How do we know the key value (k)? Can anyone explain?
Nu8YGneFCWE,"So basically, if I fix this location i, this is where this key goes.
Sorry.
This is the size of chain at h of Ki.
Sorry.
So I look at wherever Ki goes is hashed, and I see how many things collide with it.
I'm just summing over all of these things, because this is 1 if there's a collision and 0 if there's not.
Does that make sense? So this is the size of the chain at the index location mapped to by Ki.
So here's where your probability comes in.
What's the expected value of this chain length over my random choice? Expected value of choosing a hash function from this universal hash family of this chain length-- I can put in my definition here.
That's the expected value of the summation over j of xij.
What do I know about expectations and summations? If these variables are independent from each other-- AUDIENCE: [INAUDIBLE] JASON KU: Say what? AUDIENCE: [INAUDIBLE] JASON KU: Linearity of expectation-- basically, the expectation sum of these independent random variables is the same as the summation of their expectations.","48:50 As I learned in 6.042j 2010 from Prof. Leighton, Linearity of Expectation holds regardless of mutual independence property. Correct me if I misunderstood."
Nu8YGneFCWE,"So this is equal to the summation over j of the expectations of these individual ones.
One of these j's is the same as i.
j loops over all of the things from 0 to u minus 1.
One of them is i, so when xhi is hj, what is the expected value that they collide? 1-- so I'm going to refactor this as being this, where j does not equal i, plus 1.
Are people OK with that? Because if i equals-- if j and i are equal, they definitely collide.
They're the same key.
So I'm expected to have one guy there, which was the original key, xi.
But otherwise, we can use this universal property that says, if they're not equal and they collide-- which is exactly this case-- the probability that that happens is 1/m.
And since it's an indicator random variable, the expectation is there are outcomes times their probabilities-- so 1 times that probability plus 0 times 1 minus that probability, which is just 1/m.
So now we get the summation of 1/m for j not equal to i plus 1.
Oh, and this-- sorry.
I did this wrong.","Question: At 51:00 Jason says that chain length is constant if m is at least order n. Will 1+((n-1)/m) not be between 1 and 2 which is not constant. If m is massive and n is massive then 1+((n-1)/m) would be about 2. If massive and n is small, say m=10000000 and n=1, then 1+((n-1)/m) is about 1. I am not great at probability, which is probably why I don't understand. Thanks for any clarification.
At 50:51, shouldn't that be less than or equal to as probability 1/m is an upper bound to the probability of two keys having the same hash value?"
Nu8YGneFCWE,"So what I would need to do-- and if I was storing your keys as MIT IDs, I would need an array that has indices that span the tire space of nine-digit numbers.
That's like 10 to the-- 10 to the 9.
Thank you.
10 to the 9 is the size of a direct access road off to build to be able to use this technique to create a direct access array to search on your MIT IDs, when there's only really 300 of you in here.
So 300 or 400 is an n that's much smaller than the size of the numbers that I'm trying to store.
What I'm going to use as a variable to talk about the size of the numbers I'm storing-- I'm going to say u is the maximum size of any number that I'm storing.",Great video! Thank you MIT OCW :) 18:37...how is it 10^9? Can someone please explain?
Nu8YGneFCWE,"This isn't u.
This is n.
We're storing n keys.
OK, so now I'm looping over j-- this over all of those things.
How many things are there? n minus 1 things, right? So this should equal 1 plus n minus 1 over m.
So that's what universality gives us.
So as long as we choose m to be larger than n, or at least linear in n, then we're expected to have our chain lengths be constant, because this thing becomes a constant if m is at least order n.","51:20 shouldn't be n-2? instead n-1, the result of the sum, bc one of the n-1 elements is not counted and that's when j=i, can anyone explain? thanks!"
OgO1gpXSUzU,"A pretty small number.
But the probability of 26 consecutive reds when the previous 25 rolls were red is what? No, that.
AUDIENCE: Oh, I thought you meant it had been 26 times again.
JOHN GUTTAG: No, if you had 25 reds and then you spun the wheel once more, the probability of it having 26 reds is now 0.5, because these are independent events.
Unless of course the wheel is rigged, and we're assuming it's not.
People have a hard time accepting this, and I know it seems funny.
But I guarantee there will be some point in the next month or so when you will find yourself thinking this way, that something has to even out.","I feel like the slide at 22:00 is a good opportunity to introduce probability notation, since in English the second sentence sounds really misleading. The first sentence is P(26 consecutive reds). The second sentence is P(26 consecutive reds | the FIRST 25 are red). Strictly speaking the second sentence is grammatically incorrect, what the professor means is ""Probability of a single roll being red, given that the last 25 were red."" This makes it WAY easier to understand that rolls are not correlated. What is written on the slide makes it sound like there are 26+25 rolls taking place.
22:10 I understand (I think, I hope) that concept of independence, but then it is said we assume the wheel is not rigged. But is this assumption justified after 25 consecutive reds? Probably yes, things like this happen (and we don't forget them for decades), but on the other hand, at how many consecutive reds should we become suspicious?"
OgO1gpXSUzU,"All right, so we've got one flip, and it came up heads.
And now I can ask you the question-- if I were to flip the same coin an infinite number of times, how confident would you be about answering that all infinite flips would be heads? Or even if I were to flip it once more, how confident would you be that the next flip would be heads? And the answer is not very.
Well, suppose I flip the coin twice, and both times it came up heads.
And I'll ask you the same question-- do you think that the next flip is likely to be heads? Well, maybe you would be more inclined to say yes and having only seen one flip, but you wouldn't really jump to say, sure.
On the other hand, if I flipped it 100 times and all 100 flips came up heads, well, you might be suspicious that my coin only has a head on both sides, for example.
Or is weighted in some funny way that it mostly comes up heads.
And so a lot of people, maybe even me, if you said, I flipped it 100 times and it came up heads.
What do you think the next one will be? My best guess would be probably heads.
How about this one? So here I've simulated 100 flips, and we have 50 heads here, two heads here, And 48 tails.
And now if I said, do you think that the probability of the next flip coming up heads-- is it 52 out of 100? Well, if you had to guess, that should be the guess you make.
Based upon the available evidence, that's the best guess you should probably make.
You have no reason to believe it's a fair coin.","At 8:30 he misses implications of Bayes theorem - if you observe 52 heads from 100 flips, it is still much more likely that the coin is fair than biased. Because as he mentions, there are many many more fair coins and dice our there than weighted ones. The probably you have to assess is P(52 heads | coin is fair) * P(coin is fair) vs P(52 heads | coin is biased) * P(coin is biased). Far more likely that it is fair."
OgO1gpXSUzU,"And that gets us to what's in some sense the fundamental question of all computational statistics, is how many samples do we need to look at before we can have real, justifiable confidence in our answer? As we've just seen-- not just, a few minutes ago-- with the coins, our intuition tells us that it depends upon the variability in the underlying possibilities.
So let's look at that more carefully.
We have to look at the variation in the data.
So let's look at first something called variance.
So this is variance of x.
Think of x as just a list of data examples, data items.
And the variance is we first compute the average of value, that's mu.
So mu is for the mean.
For each little x and big X, we compare the difference of that and the mean.",32:31 what is the symbol | X | is it the same as the symbol n?
OgO1gpXSUzU,"But you wouldn't bet that it would have fewer than 5.
Because of this, if you now look at the average of the 20 spins, it will be closer to the mean of 50% reds than you got from the extreme first spins.
So that's why it's called regression to the mean.
The more samples you take, the more likely you'll get to the mean.
Yes? AUDIENCE: So, roulette wheel spins are supposed to be independent.
JOHN GUTTAG: Yes.
AUDIENCE: So it seems like the second 10-- JOHN GUTTAG: Pardon? AUDIENCE: It seems like the second 10 times that you spin it.
Like that shouldn't have to [INAUDIBLE]..
JOHN GUTTAG: Has nothing to do with the first one.
AUDIENCE: But you said it's likely [INAUDIBLE]..
JOHN GUTTAG: Right, because you have an extreme event, which was unlikely.
And now if you have another event, it's likely to be closer to the average than the extreme was to the average.
Precisely because it is independent.
That makes sense to everybody? Yeah? AUDIENCE: Isn't that the same as the gambler's fallacy, then? By saying that, because this was super unlikely, the next one [INAUDIBLE].
JOHN GUTTAG: No, the gambler's fallacy here-- and it's a good question, and indeed people often do get these things confused.
The gambler's fallacy would say that the second 10 spins would-- we would expect to have fewer than 5 reds, because you're trying to even out the unusual number of reds in the first Spin Whereas here we're not saying we would have fewer than 5.",what is he throwing at 26:22 and why? I noticed this already in the previous video... what is it it about?
OgO1gpXSUzU,"The next time they come to the plate, the idiot announcer says, well he struck out six times in a row.
He's due for a hit this time, because he's usually a pretty good hitter.
Well that's nonsense.
It says, people somehow believe that if deviations from expected occur, they'll be evened out in the future.
And we'll see something similar to this that is true, but this is not true.
And there is a great story about it.
This is told in a book by Huff and Geis.
And this truly happened in Monte Carlo, with Roulette.
And you could either bet on black or red.
Black came up 26 times in a row.
Highly unlikely, right? 2 to the 26th is a giant number.
And what happened is, word got out on the casino floor that black had kept coming up way too often.
And people more or less panicked to rush to the table to bet on red, saying, well it can't keep coming up black.
Surely the next one will be red.
And as it happened when the casino totaled up its winnings, it was a record night for the casino.
Millions of francs got bet, because people were sure it would have to even out.
Well if we think about it, probability of 26 consecutive reds is that."," I don't understand why, isn't the whole purpose of ""Law of Large Numbers"" to have a ""regression to the mean""? In the casino example 20:48 ""black came 26 times in a row"", so it's likely that ""regression to the mean"" rule will begin to correct this ""anomaly"", right? So the next 26 dices will have a slight bias to have - not 13/13 REDS/BLACKS, but maybe smth like 15/11 (slightly correcting the anomaly), right? Or even 14/12, right? (otherwise there's no regression to mean). If yes then you should switch to RED, right? Or did i completely misunderstood the ""regression to the mean"" rule?"
OgO1gpXSUzU,"Today we would think of a few microseconds, but those machines were slow.
Hence was born Monte Carlo simulation, and then they actually used it in the design of the hydrogen bomb.
So it turned out to be not just useful for cards.
So what is Monte Carlo simulation? It's a method of estimating the values of an unknown quantity using what is called inferential statistics.
And we've been using inferential statistics for the last several lectures.
The key concepts-- and I want to be careful about these things will be coming back to them-- are the population.
So think of the population as the universe of possible examples.
So in the case of solitaire, it's a universe of all possible games of solitaire that you could possibly play.","This guy made a lot of errors. One of the earliest ones is at 3:25 when he attempts to define Monte Carlo simulation. He mentions an unknown quantity but that is incorrect. Suppose for example I wanted to simulate a known quantity, perhaps just to confirm that my programming skills are still good. So I would have someone good in math compute the exact answer using the ""paper and pencil"" method, and I would use a computer to simulate it to get an approximation. According to his definition, I would NOT be doing Monte Carlo simulation because the value is known. To take that concept even farther, suppose I try to simulate an ""unknown"" quantity but unbeknownst to me, someone else already has the correct exact answer. According to his definition, that is also NOT Monte Carlo simulation. He didn't specify who the quantity is unknown to in the definition, therefore it is not a good definition because it is ambiguous. If you Google Monte Carlo simulation, you will see MUCH better definitions of it then the crappy definition presented here in this lecture. Another big problem with his definition is he states the random sample tends to exhibit the same behavior as the population from which it is drawn. That is NOT true if the population has some super rare occurrence we are looking for such as 1 out of 1 quintillion. Now suppose 2 people, each with a computer, can simulate 1 trillion random samples. Since they are still 6 orders of magnitude below the full population size, it is VERY likely that they will both come up with 0 ""hits"" and possibly falsely concluding that there are no good ""hits"" (cases they are looking to count). Even if they reran the simulation 10 times each, then would still be 5 orders of magnitude below the population size which is 2/100,000 sampling which is not enough to get accurate results. Someone might say well 0 is very close to the real expected value (and that is true), but my point is by getting 0 ""hits"", no information is given about if a ""good"" event (the ones we are trying to count up exactly) even exist."
OgO1gpXSUzU,"We're saying we'd probably have fewer than 10.
That it'll be closer to the mean, not that it would be below the mean.
Whereas the gambler's fallacy would say it should be below that mean to quote, even out, the first 10.
Does that makes sense? OK, great questions.
Thank you.
All right, now you may not know this, but casinos are not in the business of being fair.
And the way they don't do that is in Europe, they're not all red and black.
They sneak in one green.
And so now if you bet red, well sometimes it isn't always red or black.
And furthermore, there is this 0.","27:30 But if we start counting from the beginning of the series, when we have 5 blacks in row, then the next black would change the series of 5 into the series of 6 ,which is more extreme. Can't I think this way ?"
OgO1gpXSUzU,"What does this mean? If I were to conduct an infinite number of trials of 10,000 bets each, my expected average return would indeed be minus 3.3%, and it would be between these values 95% of the time.
I've just subtracted and added this 3.5, saying nothing about what would happen in the other 5% of the time.
How far away I might be from this, this is totally silent on that subject.
Yes? AUDIENCE: I think you want 0.2 not 9.2.
JOHN GUTTAG: Oh, let's see.
Yep, I do.
Thank you.
We'll fix it on the spot.
This is why you have to come to lecture rather than just reading the slides, because I make mistakes.
Thank you, Eric.
All right, so it's telling me that, and that's all it means.
And it's amazing how often people don't quite know what this means.
For example, when they look at a political pole and they see how many votes somebody is expected to get.","Thank you for the great lecture. One question....at 39:00 I see it saying ""The return on betting a pocket 10k times in European roulette is -3.3%"". Was that based on the Monte Carlo sim? I ask because there are 37 pockets on a European roulette wheel. If you win it returns 35 to 1, plus your original wager, for 36 units returned on a win. 1/37 = 0.0270, for an expected return of -2.7%, or 97.3% (depending how you look at it) on European roulette. Thanks again for the awesome info..."
P3YoIxiz6to,"So in general, we have some problem A-- decision problem A and parameter k.
And we want to convert into some decision problem B with parameter k prime.
And of course, as usual, the set up is we're given an instance x.
This is going to look almost identical to NP Karp-style reductions, but then we're going to have one extra condition.
So instance x of A gets mapped to by a function f to an instance x prime of B.
X prime is f o x as usual.
This needs to be a polynomial time function just like for NP reductions, which means, in particular, x prime has polynomials size reduced back to x.
It should be answer preserving.
So x is yes instance for A if and only x prime is a yes instance for B.
So, so far, exactly NP reductions.
And then when we need one extra thing which is parameter preserving.
This is there's some function g, which I'll call the parameter blow up-- or, I guess you call it parameter growth for g-- such that the new parameter value for the converted instance is, at most, that function of the original parameter value of the original instance.
Question? AUDIENCE: Are there any limits on the amount of time that g can take to compute? PROFESSOR: g should be a computable function.
I think that's all we need.
Probably polynomial time is also a fine, but usually this is going to be like linear or polynomial or exponential.
It's rarely some insanely large thing, but computable would be nice.
Cool.
So that is our notion of parameterized reduction.
And the consequence, if this exists and B is fixed parameter tractable, then A is because we can take an instance of A converting it to B.
If the original parameter was founded by some k, this new parameter will be bounded by g of k.
New instance will be bounded of g of k.
So we run the FPT algorithm for B, and that gives us the answer to the original instance of A.
So if we don't care about what this function is, we are basically composing functions.
So there's some f dependence on k in this algorithm, and we're taking that function of g of k is our new function.
And we get a new dependence on k and the running time over FPT.
So what that means is if we believe it A does not have an FPT, then B does not have an FPT if we can do these reductions.","At around 21:00 you give the definition of a parameterized reduction. The classic definition usually states that a reduction is computable by some FPT algorithm (for example, in Flum & Grohe). Why do you opt to give this liberal definition of an FPT reduction, instead?"
PNKj529yY5c,"A small detail, not a particularly important one.
Now where are we.
We've got that guy there.
We've got our complete architecture.
We've got our solved problem.
And now we can start reflecting on what we've done.
We can say, for example, how good an integration program is this? And the answer is, it was pretty good.
This machine that Slagle was using was a machine that was over in building 26.
And we were so proud of it, that it was behind glass, and you could go there and watch the tape spin, it was really a delight.
32k of memory, that's 32k of memory.
It's amazing that he was able to do anything with a machine of that size.",How did he find functional composition depth at 34:45
PNKj529yY5c,"All good, I see some confused, worried, concerned looks.
Maybe I've made a mistake, perhaps I should use notes.
Well no, wait a minute.
For those of you who have a concerned look, remember that if x equals a sine y, then dx is equal to cosine y dy.
That's why it's cosine to the fourth not cosine to the fifth, as you were perhaps thinking it might be.
So now we've made some progress.
We look at this, we say, are there any safe transformations that apply? And the answer is, no.
Now we look for a heuristic transformation that might apply, and I say, what do you see? Which one? What's that? AUDIENCE: [INAUDIBLE].
SPEAKER 1: She said something unintelligible, but what she probably said is, that this looks like a pattern that might match with the heuristic transformation A, right? Because we have a function in which the variable is buried, universally in sines, or cosines, or tangents, or cotangents, or secants, or cosecants.
And we know we can rewrite that in one of three ways.
It's already written as a function of sine and cosine.
But we can also rewrite that in terms of tangent and cosecant.
Or cotangent and secant.
So when we do that, we can go this way, and we can get the integral of 1 over the cotangent of x dx.",Hey could someone explain me why dx = cos y at about 19:25? that would be very nice. I cant finde an explanation online. A good link would help me too. Thanks
PNKj529yY5c,"All right so that's progress, maybe.
But don't see this in any of the heuristic transformations, what do I do now? I didn't have to look in the heuristic transformations, because one of the safe transformations applies.
Because this thing is a rational function and the degree of the numerator is greater that the degree of the denominator, so I have to divide.
And when I divide, and that by the way is number four, I get what? Is anybody good high school algebra that can help me out with that? AUDIENCE: Y squared minus 2 plus negative 2 over 1 plus y squared SPEAKER 1: Exactly, y squared minus 1 plus 1 over 1 plus y squared, I think.",can anyone tell me at 28:50 how did he get that division result
PNKj529yY5c,"But once we get a name for it, we'll get power over it.
And then we'll be able to deploy it, and it will become a skill.
We'll not just witness it, we'll not just understand it, we'll use it instinctively, as a skill.
So there you are, you've got that problem, there's your problem, and what do you do to solve it? I don't know, look it up in a table? You'll never find it in a table because of that minus sign and that 5.
So you're going to have to do something better than that.
So what you're going to do, is what you always do when you see a problem like that.
You try to apply a transform, and make it into a different problem that's easier to solve.
And eventually, what you hope is that you'll simplify it sufficiently, that the pieces that you've simplified to will be found in some small table of integrals.
So how long is this table? It's not the case that we're going to look at a table with 388 elements, because this is not a big table of integrals.
This is what a freshman might have in a freshman's head, after taking a course in integral calculus.
One of the interesting questions is, how many elements have to be in that table to get an A in the course? We're interested in how much knowledge is involved, that's one of the elements of catechism that I've listed over there, that will be part of the gold star ideas suite of the day.
So we'd like to take that problem, and find a way to make it into another problem that's more likely, or closer to being found in the table.
So what we're going to do is very simple, graphically.
We're going to take the problem we're given, and convert it into another problem that's simpler.
And we're going to give that process and name, and we're going to call it problem reduction.
And so, in the world of integral calculus, there are all sorts of simple methods, simple transformations, we can try that will take a hard problem and make it into an easier problem.
And some of these transformations are extremely simple and always safe.
Some of them are just, well let's try it and see what happens.
But some of them are safe, and I'd like to make a short list of safe transformations right now.
Now I'm going to be going into some detail.",what's problem reduction? 03:35
PNKj529yY5c,"Now what? Now we're really getting close to getting through this, because that is a sum.
And by virtue of the fact that it's a sum, that divides into three pieces, and the top piece is the integral of y squared, the middle piece is the integral of minus 1, and the bottom piece is the integral of 1 over 1 plus y squared dy in all cases.
Gosh, if I look this up, I've found it.
That's up there, that's letter B.
So I'm done with that.
This one I can transform again, by virtue of 1, and now I get the integral dy.
That's in there, that's B as well.
As this one, I don't know.
But I'd better keep track of what I'm doing here.
This is in the and node, so I've got to do all of those.
I can't give up on that last thing.
And that and transformation is transformation number 3.
So this is in the table, this is in the table, we still have this to do, but that's C, heuristic transformation C.
We have 1, plus y squared, then with the transformation C, with y-- this is y squared-- y equals tangent of z And then we get to the integral of dz and that's in the table and, we're done.",How did he get to dz from 1/(1+y^2) dy at 31:07 ?
PNKj529yY5c,"Or we can rewrite that as a function of tangent of x, and cosecant of x.
Or we can rewrite that as function of cotangent of x, and the secant of x.
So that's a transmission from trigonometric form, into another trigonometric form.
It's not always a good idea, sometimes it helps.
Well that's just part one of our suite of heuristic transformations.
Stop.
There are others that we need to have in our repertoire, in order to solve the problem.
One of them is a family of transformations, which I'll show you only one.
It goes like this, if you have the integral of a function, of the tangent of x, then you can rewrite that as the integral of a function of y over 1 plus y squared dy.
So that's a transformation from a trigonometric form into a polynomial form.
So it gets rid of all that trigonometric garbage we don't want to deal with.
And there's a whole family of things like that, just as there's a family of transformations like so, but this is enough to give you flavor.
Now there's a C that we need as well.
And that's going to be your proper knee-jerk reaction when you see something of the form 1 minus x squared.","Where the heuristic transformations after 15:30 come from, what is this?"
PNKj529yY5c,"We have no transformation that can take us further, so we need something else.
And what we need by way of something else, is some transformations that we will describe as-- perhaps we'll call them, heuristic transformations.
A funny word, meaning a method that often works isn't guaranteed to work.
It's not an algorithm in the usual sense that we talk about algorithms.
But rather, it's an attempt.
So these things I'm going to talk about now, are sometimes useful, not always useful.
Sometimes take you into a blind alley, don' always work.
But you can't get an A in calculus without knowing some of them.
So you said, some kind of trig substitution.
So here is some kind of trig substitution.
We'll call this heuristic transformation A.
You have a function sine x, cosine x, tangent of x, cotangent of x, secant of x, and cosecant of x.
And we all know from high school trigonometry, that we can rewrite that as a function of sine x, and cosine x.",Where can I find readings for the transformations mentioned at 14:30 ? I have taken calculus classes but I have never seen anything similar to that before and would like to learn.
QOtA76ga_fY,"One is that they assumed that encryption also provides authentication or integrity of messages.
So don't do that.
And Kerberos version 5 fixes this by explicitly authenticating messages.
Another thing they sort of had a problem with is the ability for arbitrary clients to guess people's passwords.
So any suggestions of how we could fix this? How do you prevent guessing attacks in a protocol like this? What could we try? Yeah.
STUDENT: Some sort of salting? I'm not sure.
PROFESSOR: Well, so salting would just means that the client has to hash the password in different ways, maybe.
But it still doesn't prevent them from trying lots of things.
So maybe it'll be more expensive to build a dictionary.","How to discover the password by brute force as the teacher asked at the minute 31:15? The password is hashed on Kc. But the attacker does not know the Kc, only the encrypted message: {{Tc,s}Ks, Kc,s}Kc. He can try brute force the encrypted message: {{Tc,s}Ks, Kc,s}Kc, but he has no idea what is the true value of {{Tc,s}Ks, Kc,s}Kc, he can try all Kc possibles, include the correct key, and he won't know if the result found is the correct one."
QUO-HQ44EDc,"So it means using the training data, we are going to train two classifiers.
For example, using a linear classifier, a neural network, or support vector machine or a decision tree that given a feature vector of a given node, it's going to predict its label.
And in phase 1, we are going to apply this classifier Phi_1, so that now every node in the network has some predicted, uh, label.
And then using the training data, we are also going to train this classifier Phi_2, that is going to use two inputs.
It's going to use the feature of the node of interest, as well as this summary vector z that captures or summarizes the labels of the nodes, um, in it's network neighborhood.
And this Phi_2 is going to predict the label of the node of interest v based on its features, as well as the summary z of its, uh- of the labels of its neighbors.
And then- right, so we have first applied Phi_1 then we also have trained Phi_2 so that now we will go into phase 2 where we are going to iterate and we are going to iterate until convergence, uh, the following- uh, the following, uh, iteration where on the, uh, test set, which means on the unlabeled nodes.
Uh, we are going to use this, uh, first, the classifier Phi_1 to assign initial, uh, labels.
We are going to compute the label summaries z, and then we are going to predict the labels with the second classifier, Phi_2.
And we are going now to repeat this process until it converges in a sense that we are going to update the vector z.","Thanks for the great lecture. I have a question. For Iterative classifier (about 16:05), in Phase 1 (i.e. training phase), we train 2 classifiers \phi_1 and \phi_2. For the label summary vector z_v required for the classifier \phi_2, is it derived from the output produced by \phi_1? Or are the classifiers trained separately, as is suggested by earlier comment? Because if we do train separately, the error from \phi_1 will be added to the error from \phi_2. Which means that \phi_2 was not trained to be somewhat robust to the possible noise (that is absent in training but present in test due to error from \phi_1) in z_v."
RN8qpSs8ozY,"Um, in online settings if you don't know that, you can also constantly be updating it with a time step.
It's a good question.
Yeah.
How is delta decided? How is what? Pardon.
How is delta, is like what is delta? Okay.
Good question.
Um, so, question is what is delta.
What we're gonna, I did not tell you, uh, in this ca- well, in this case it's telling us, um, it's specifying what is the probability this is holding like what this inequality is holding.
Later we're gonna pro- provide a regret bound that is high probability.
So we're gonna say we're gonna have a regret bound which is something that like with probability is 1 minus a function of delta, um, your regret will be sub-linear.
So that's how.
You can get expected regret bounds too and the UCB paper which, um, one of the original UCB papers provides an expected bound but I thought it was a little, the, this bound was a little bit easier to do in class.
So I thought I would do the high probability bound.
Yeah.
So before we were talking about regret, I didn't exactly understand how you use regret to update your estimate of the action value.
Oh, good question.
Um, so question is, do we use or how would we use the regret bound, the regret to update our estimate action, we don't.
Regret is just a tool to analyze our algorithm.
Great clarification.
So regret is a way for us to analyze whether or not an algorithm is gonna be good or bad in terms of how fast the regret gro- grows but it's not used in the algorithm itself.
The algorithm doesn't compute regret.
And it's not used in terms of the updating.
Excuse me.
Okay.","1:07:05 presumably, this is only true with the upper bound holds right?
I guess are you are referring to 1:02:29? Perhaps it was because in the beginning by the definition of upper bound, U(at)>Q(at) with HIGH PROBABILITY (I know it sounds vague and sloppy)?"
Rfkntma6ZUI,"So perhaps your GN- GCN, GNN may need to be a bit deeper, but you have reduced, uh, the number of parameters, uh, significantly, which will lead to, uh, faster training, perhaps more robust model, less overfitting, um, and so on.
So that's, uh, the technique of, uh, block diagonal matrices, uh, where basically you reduce the dimensionality of each W by assuming a block diagonal structure.
Uh, second idea how to, uh, make this, uh, RGCN more scalable and its learning more tractable is to- to build on the key insight which is we wanna share weights across different relations, right? We don't wanna consider relations as independent from each other, but we want relations to kind of share weights, uh, to also share some information.","In Basis Learning (18:42) it is unclear if the learned scalar a_rb are relation specific. which means the importance for each matrix V will be a vector rather than a scalar.
In Basis Learning (18:42), it is unclear how we get the basis matrices (V_b). Do we set them to be trainable just like the importance weight coefficients (a_rb)?"
RvRKT-jXvko,"So why would we want to use tuples? Tuples are actually useful in a couple of different scenarios.
So recall a few years ago, we looked at this code where we tried to swap the values of variables x and y.
And this first code actually didn't work, because you're overwriting the value for x.
So instead, what we ended up doing was creating this temporary variable where we stored the value of x, and then we overwrote it, and then we used the temporary variable.
Well, turns out this three liner code right here can actually be written in one line using tuples.
So you say x comma y is equal to y comma x.
And Python goes in and says, what's the value of y? And assigns it to x.
And then what's the value of x? And assigns it to y.
Extending on that, we can actually use tuples to return more than one value from a function.
So functions, you're only allowed to return one object.
So you're not allowed to return more than one object.",5:28 what are they useful for
RvRKT-jXvko,"We haven't created a new object in memory.
We're just modifying the same object in memory.
And you're going to see why this is important as we look at a few side effects that can happen when you have this.
So I've said this a couple of times before, but it'll make your life a lot easier if you try to think of-- when you want to iterate through a list if you try to think about iterating through the elements directly.
It's a lot more Pythonic.
I've used that word before.
So this is sort of a common pattern that you're going to see where you're iterating over the list elements directly.
We've done it over tuples.
We've done it over strings.
So these are identical codes.
They do the exact same thing, except on the left, you're going from-- you're going through 0, 1, 2, 3, and so on.
And then you're indexing into each one of these numbers to get the element value.
Whereas on the right, this loop variable i is going to have the element value itself.
So this code on the right is a lot cleaner.
OK.
So now let's look at some operations that we can do on lists.
So there's a lot more operations that we can do on lists, because of their mutability aspect than we can do on tuples or strings, for example.",at 19:19 a list was not created yet. Can someone explain how this works
SE4P7IVCunE,"Because this is just like we did in the very first program, when I'm checking 0, 1, 2, 3, 4, 5, 6, 7, 8, when I'm trying to find the cube root of 8.
Once I've reached 8, I'm going to stop.
And it's the same thing here.
So I just added this little clause that says, well, while I'm greater than or equal to epsilon and I'm still less than the actual cube, just keep searching.
But once I've reached the cube, then stop searching.
And with 10,000, you can see that I failed to actually find-- so that's what this part, here, does.
It tells me I've failed to find the cube root with those particular parameters.
The last thing we're going to look at is bisection search.
And to illustrate this, I'm going to need one volunteer.
And you're going to play a game with me in front of the whole class.","Nice tutorial and lesson. Just a small note, IMHO, on ~35:00 Anna made a small ""mistake"" when added ""and guess<= cube"" to the test condition. Epsilon is an error range, so the correct answer (in general) is << Cube-epsilon<=guess**3<=Cube+epsilon >>. Therefore the test condition in the example should be ""and guess<= cube+epsilon"" or test should be rewritten according like within <<...>>"
SE4P7IVCunE,"But it's an entirely different object that I've created here.
Again, it might not mean anything right now, but just keep this in the back of your mind, strings are immutable.
So the next thing I want to talk about is a little bit of recap on for loops.
And we're going to see how we can apply for loops, very easily, to write very nice, readable code when dealing with strings.
So remember that for loops had a loop variable.
My loop variable being this var, here, in this particular case.
It can be anything you want.
And this variable, in this particular case, iterates over this sequence of numbers, 0, 1, 2, 3, 4.
So the very first time through the loop, var has a value of 0.
It does the expressions in the loop.
As soon as they're done, var takes the value 1.
It does all the expressions in the loop.
And then var takes the value 2, and it does that all the way up until 0, 1, 2.","13:21 loop variable that iterates over a collection, not a set?"
SE4P7IVCunE,"So as long as the guess cubed minus the cube-- so how far away are we from the actual answer-- is greater than some epsilon, keep guessing, because the solution is not good enough.
But once this is less than epsilon, then we've reached a good enough solution.
So two things to note with approximate solutions.
So you can get more accurate answers if your step size is really, really small.
If you're incrementing by 0.0001, you're going to get a really good approximate solution, but your program will be a lot slower.
Same sort of idea with epsilon, you can change epsilon.
If you change epsilon to be a bigger epsilon, you're sacrificing accuracy, but you're going to reach a solution a lot faster.
So here's the code for the approximate solution of a cube root.
It might look intimidating, but, look, almost half this code is just initializing variables.
So we're initializing, this is the cube we want to find the cube root of.
We pick an epsilon of this.
We start with a guess of 0.
We start with an increment of 0.0001.
And just for fun, let's keep track of the number of guesses that it takes us to get to the answer.",Can someone help me out - surely the code at 29:55 is wrong? The While loop should not run because the condition is not true i.e. guess**3 - cube is NOT greater than or equal to 0.1. Rather 0x0x0 - 27 is LESS than 0.1? So surely the While loop would not run in the first place?
SE4P7IVCunE,"So my cube is 8.
I'm going to have a for loop that says, I'm going to start from 0.
And I'm going to go all the way up to-- So I'm going to start from 0 and go all the way up to 8.
For every one of these numbers, I'm going to say, is my guess to the power of 3 equal to the cube 8? And if it is, I'm going to print out this message.
Pretty simple, however, this code is not very user friendly, right? If the user wants to find the cube root of 9, they're not going to get any output, because we never print anything in the case of the guess not being a perfect cube.
or the cube not being a perfect cube.
So we can modify the code a little bit to add two extra features.
The first is we're going to be able to deal with negative cubes, which is kind of cool.
And the second is we're going to tell the user, if the cube is not a perfect cube, hey, this cube is not a perfect cube.
So we're not going to silently just fail, because then the user has some sort of feedback on their input.
So let's step through this code.
We have, first of all, a for loop just like before.
And we're going to go through 0 to 8 in this case.
We're using the absolute value, because we might want to find the cube root of negative numbers.
First thing we're doing is doing this check here.
Instead of guessing whether the guess to the power of 3 is equal to the cube, we're going to check if it's greater or equal to, and we're going to do that for the following reason.",at 24:50 why is there a +1 in range of that for loop? To make the loop go till 8 instead of stopping at *7*?
STjW3eH0Cik,"And there are b to d of those.
How many are there at this next level up? Well, that must be b to the d minus 1.
How many fewer nodes are there at the second to the last, the penultimate level, relative to the final level? Well, 1 over b, right? So, if I'm concerned about not getting all the way through these calculations at the d level, I can give myself an insurance policy by calculating out what the answer would be if I only went down to the d minus 1th level.
Do you get that insurance policy? Let's say the branching factor is 10, how much does that insurance policy cost me? 10% of my competition.","I have a question: At 36:22 he says what if we don't have enough time and we went only till the (d-1) th level. And then he also suggests we can have a temporary answer at every level as we go down as we should have some approximate answer at any point of time. But!! How can we have any answer without going to the leaf nodes because it's only at the leaf nodes we can conclude who can win the game. Think this for tic-tac-toe game. At (d-1)th level we don't have enough information to decide if this series of moves till this node at (d-1) will win me or lose me the game. At higher levels say at (d-3) it's so blur! Everything is possible as we go down! Isn't it? So, if an algorithm decides to compute till (d-1) th level then all those path options are equal!! Nothing guarantees a win and nothing guarantees a lose at (d-1)th level because if I understand correctly wins and losses are calculated only at the leaf nodes. This is so true especially in pure MinMax algorithm. So how exactly are we going to have an 'approximate answer' at (d-1)th level or say (d-5)th level?"
STjW3eH0Cik,"I'll need some help, especially from any of you who are studying cosmology.
So, we'll start with how many atoms are there in the universe? Volunteers? 10 to the-- SPEAKER 2: 10 to the 38th? SPEAKER 1: No, no, 10 to the 38th has been offered.
That's why it's way too low.
The last time I looked, it was about 10 to the 80th atoms in the universe.
The next thing I'd like to know is how many seconds are there in a year? It's a good number have memorized.
That number is approximately pi times 10 to the seventh.
So, how many nanoseconds in a second? That gives us 10 to the ninth.
At last, how many years are there in the history of the universe? SPEAKER 3: [INAUDIBLE].
14.7 billion.
SPEAKER 1: She offers something on the order of 10 billion, maybe 14 billion.
But we'll say 10 billion to make our calculation simple.
That's 10 to the 10th years.
If we will add that up, 80, 90, plus 16, that's 10 to the 106th nanoseconds in the history of the universe.
Multiply it times the number of atoms in the universe.
So, if all of the atoms in the universe were doing static evaluations at nanosecond speeds since the beginning of the Big Bang, we'd still be 14 orders of magnitudes short.","Question - Prof Winston says (around 12:30) that there are PI * 10^7 seconds in a year. Can anyone tell me where the PI comes from in this result? Does this take into account leap years somehow? Otherwise, shouldn't it be 60 secs/min x 60 min/hr x 24 hrs/day x 365 days/year = 3.1536 which is close to PI but not equal. Thanks"
STjW3eH0Cik,"So, let's carry on and see if we can complete this equal to or less than 8, equal to 8, equal to 8-- because the other branch doesn't even exist-- equal to or less than 8.
And we compare these two numbers, do we keep going? Yes, we keep going.
Because maybe the maximizer can go to the right and actually get to that 8.
So, we have to go over here and keep working away.
There's a nine, equal to or less than 9, another 9 equal to 9.
Push that number up equal to or greater than 9.
The minimizer gets an 8 going this way.
The maximizer is insured of getting a 9 going that way.
So, once again, we've got a cut off situation.
It's as if this doesn't exist.
Those static evaluations are not made.","29:34, for the deep cut, did he compare two Max nodes? or compared the bottom Min node with the root Max node?"
STjW3eH0Cik,"So, you have to see that.
This is the important essence of the notion the alpha-beta algorithm, which is a layering on top of minimax that cuts off large sections of the search tree.
So, one more time.
We've developed a situation so we know that the maximizer gets a 2 going down to the left, and he sees that if he goes down to the right, he can't do better than 1.
So, he says to himself, it's as if that branch doesn't exist and the overall score is 2.
And it doesn't matter what that static value is.
It can be 8, as it was, it can be plus 1,000.
It doesn't matter.
It can be minus 1,000.
Or it could be plus infinity or minus infinity.
It doesn't matter, because the maximizer will always go the other way.
So, that's the alpha-beta algorithm.
Can you guess why it's called the alpha-beta algorithm? Well, because in the algorithm there are two parameters, alpha and beta.
So, it's important to understand that alpha-beta is not an alternative to minimax.
It's minimax with a flourish.
It's something layered on top like we layered things on top of branch and bound to make it more efficient.
We layer stuff on top of minimax to make it more efficient.
As you say to me, well, that's a pretty easy example.
And it is.
So, let's try a little bit more complex one.
This is just to see if I can do it without screwing up.
The reason I do one that's complex is not just to show how tough I am in front of a large audience.
But, rather, there's certain points of interest that only occur in a tree of depth four or greater.","In the alpha-beta example, the branch that doesn't actually get created is just the right-most one that leads to the terminal node (not computed, because of the ""cut""). Is that right? If it's right, than the statement ""it's as if that branch doesn't exist"" (24:00) must be interpreted such that the algorithm will never choose the action that leads to the right-hand node (the one <= 1). Is this interpretation correct?"
STjW3eH0Cik,"That's level 0.
That's level 1.
This is level d minus 1.
And this is level d.
So, down here you have a situation that looks like this.
And I left all the game tree out in between .
So, how many leaf nodes are there down here? b to the d, right? Oh, I'm going to forget about alpha alpha-beta for a moment.
As we did when we looked at some of those optimal searches, we're going to add these things one at a time.
So, forget about alpha-beta, assume we're just doing straight minimax.
In that case, we would have to calculate all the static values down here at the bottom.",I have a problem understanding the insurance policy being taught at 35:50. Can anybody help?
STjW3eH0Cik,"This move generation is not made and computation is saved.
So, let's see if we can do better on this very example using this alpha-beta idea.
I'll slow it down a little bit and change the search type to minimax with alpha-beta.
We see two numbers on each of those nodes now, guess what they're called.
We already know.
They're alpha and beta.
So, what's going to happen is the algorithm proceeds through trees that those numbers are going to shrink wrap themselves around the situation.
So, we'll start that up.
Two static evaluations were not made.
Let's try a new tree.
Two different ones were not made.
A new tree, still again, two different ones not made.
Let's see what happens when we use the classroom example, the one I did up there.
Let's make sure that I didn't screw it up.
I'll slow that down to 1.
2, same answer.
So, you probably didn't realize it at the start.","At 30:25, shouldn't the root node be updated to >= 8 ?"
SUJCgisKW1Y,"So we'll be looking at what's the best way? Something like y hat test log y tests.
I should have written this down before.
I think the cross entropy loss is actually the opposite of this.
Is this correct for cross entropy loss? Something like this.
And so then you'll be minimizing the parameters of basically our comparator function with respect to how accurate your predictions are.
So more specifically, what this will look like is this corresponds to something called matching networks.
And what we're going to do is we're going to encode our training examples into some embedding space.
And then for each of those embeddings, we will compute this function that you can think of this as potentially taking the dot product between your embedding for x test and you're betting for xk.",Does the cross entropy loss shown in 17:38 need a negative sign before it?
SXBG3RGr_Rc,"OK, it's L'Hopital.
L'Hopital's Rule.
You have to differentiate the-- I guess we differentiate this guy as a ratio or something, and see what happens when it goes to 0.
And what we get when we use L'Hopital's Rule is that, oh thank God, this is still zero.
So now we know that we have a point up there and a point down there.
So now we've got three points on the curve, and we can draw it.
It goes like that.
No, it doesn't go like that.
It's obviously a Gaussian, right? Because everything in a nature is a Gaussian.
Can you put that laptop away, please? Everything in nature is a Gaussian, so it looks like this.
That right? No, actually, not everything in nature is a Gaussian.
And in particular, this one isn't a Gaussian either.
It looks more like one of those metal things they used to call quonset huts.
That's what it looks like.
Boom, like so.
So that is the curve of interest.
Now, did God say that using this way of measuring disorder was the best way? No, Got has not indicated any choice here.
We use this because it's a convenient mechanism, it seems to make sense, but in contrast to the reason it's used information theory, it's not the result of some elegant mathematics.
It's just a borrowing of something that seems to work pretty well.
Any of those curves would work just about the same, because all we're doing with it is measuring how disordered a set is.","at 29:48 why do we need to use L'Hospital Rule?? N/T is 0 (not tending to 0). So if we multiply anything with 0 we get 0, right??"
Sdw8_0RDZuw,"And at this point, I'm done.
I'm left with a set of edges called covering edges, which have the property that the only way to get from one vertex to another is going to have to be to use a covering edge to the target for vertex.
Or more precisely, the only way to get, say, from a to b is going to be to use that covering edge.
If there was any other path that went from a elsewhere and got back to b without using this edge, then it wouldn't be a covering edge anymore.
The fact that it's a covering edge means that if you broke it, there's no way anymore to get from a to b.
So that's the definition of covering edges and you'll do a class problem about them, more precisely, in a minute.
So the other edges are unneeded to define the walk relation.
And all we need to keep are the covering relations to get the minimum representation of the walk relation in terms of a DAG.
","11:00 Edge(b,d) is not a covering edge? So should it not be deleted?"
T1AtlGrCoU8,"And having proved both for all x Q of x and for all y P of y, clearly the AND holds.
And I've just proved that the right-hand side of this implication is true given that the left-hand side is true.
Now, having called this proving validity, let me immediately clarify that this is not fair to call a proof because the rules of the game are really murky here.
This theorem, you could read it as saying that universal quantification distributes over AND is one of these basic valid formulas that is so fundamental and intelligible that it's hard to see what more basic things you are allowed to assume when you're proving it.","5:53 Now this I just couldn't grasp clearly..... What exactly u mean by this proposition I mean? think with z u mean a set of all possible value of z... right? ... and P(z) and Q(z) u say some property of those elements... right? So z define a domain or set (here) of elements of type z (as far as elements are concern) ...no? ... My question is what makes u think x and y elements are part of that set z? So how come that UG logic applies?.... I mean I got confused in all these jargons here otherwise was going fine till now. :) Can u define the problem with some concrete real world examples or set... whatever... so that I can comprehend what u actually driving at? I mean it can be just set of number or anything like lets say z is a set of some male persons.... and P(z) or Q(z) be some property of them.... No how come u assume that x and y be the male persons as well... more specifically male persons from that z set? ... Like we say P(z) and Q(z) means a person is black and taller than 6 ft... how come some random x be black or taller than 6ft even if he happens to be male by chance? or even a person at all. There is nothing said what x is... might be goat, cow or even rock. "
TDo3r5M1LNo,"It's like interval arithmetic.
You know, interval arithmetic? I want to know, what are the extremes I can get on the output of a division if I'm given that a number is in some interval here and some interval here? If the answer is always, use one of the extreme endpoints here and use one of the extreme endpoints here, then this algorithm will work.
Otherwise, all bets are off.
Cool.
So if you negate-- if you put a minus here, that will work fine, because it's negating this range.
And then it's just like sum.
But-- AUDIENCE: [INAUDIBLE] ERIK DEMAINE: Oh, a divider-- if you're careful about 0, yeah.
Actually, it doesn't work, because we care about how close this can get to 0 for division.
It might be enough to consider those.
It's like, instead of minimizing and-- instead of computing this entire interval, if this interval spans 0, maybe I need to know-- if 0 is here, I need to know how close to 0 I can get on the left side and how close to 0 I can get on the right side.
Still just four quantities I need to know.
I would guess, for division, that's enough.
Yeah.
Nice.
Solved a little problem.
Then, we would be multiplying the subproblem space, instead of by 2, by 4.
Hey, maybe we should put this on the final.
No, just kidding.
Now it's in lecture, so we can't use it.
But it's a cool set of problems, right? You can do a lot with dynamic programming.
You don't need to be that clever, just brute force anything that seems hard.
And when it works, it works great.
And this class is all about understanding when it works and when it doesn't work.
Of course, we will only give you problems where it works.
But it's important to understand when it doesn't work.
For example, DAG shortest paths-- that algorithm on a non-DAG, very bad.
Infinite time.
OK.
Our last example is piano fingering.",Can someone explain to me 45:12 How to use this algo for division 
TOb1tuEZ2X4,"And correspondingly, you can have either one or two keys in a node.
Make sense? AUDIENCE: Yeah.
PROFESSOR: Cool.
OK So coming back to this.
So the root does not have a lower bound.
The root can have one child in any tree.
So you have a B equal to 5 tree, the root can still have one child-- sorry.
Not one child, one key element, two children.
All right.
It's good.
Also it's completely balanced.
So all the leaves are the same depth.
So you can see it here, right? So you can't have a dangling node here.
This is not allowed.
You have to have a leaf.
You have to have something going down, and everything ends at the same level.","1:32 Is no one in the class paying attention? It isn't at all clear why the depth is logN. 8:04 Finally mentioned that all the leaves are at the same depth, and only then do you know that the depth is actually logN and the tree is balanced."
TOb1tuEZ2X4,"It's less than 30, it goes to the left.
It's between 10 and 17, it goes in the middle.
16.
And it's greater than 14, so we add 16 here.
All right.
That seems good.
All the properties fine.
This still has two elements, which is the maximum, but it's good.
It doesn't overflow.
Let's insert something else.
Let's insert 2.
So 2 goes to 30, goes down, goes down.
And we have a problem, because 2 has overflowed this node.
So we split.
And the way we split is we take the middle element.
So we split the node here.
And 3 goes up to the parent, so 3 goes here.
And all good, except for the parent has overflowed.
So what do we do with the parent? We split the parent again.
And this time, it's right down the middle, the 10 goes up.
So OK, let's get rid of this.
So now that we split the parent, the 10 goes up here.
And you're good.
It's a bit cluttered, so let me reposition the 17.
Did those two operations make sense? Questions? AUDIENCE: If your node size [INAUDIBLE] number of-- PROFESSOR: So just pick the-- first of all-- OK.","Guys, the b-tree in 20:08 is not valid, isn't it? We are having problems with the root node"
TOb1tuEZ2X4,"So that's the general node.
So before we go into more details of the properties and everything, the question is why use B-trees.
So if we do a quick depth analysis, we can see that the depth is to log n rate.
Is that clear to everyone sort of, why the depth is log n? Because you have branching just like in binary search trees.
In fact, you have more branching.
But in any case, depth is to log n.
But why use B-trees over binary search trees? Anyone have a reason why you would prefer to use B-trees or not? So all the operations are still log n.","1:32 Is no one in the class paying attention? It isn't at all clear why the depth is logN. 8:04 Finally mentioned that all the leaves are at the same depth, and only then do you know that the depth is actually logN and the tree is balanced."
TOb1tuEZ2X4,"You do a merge.
So what do you have? So here you have B minus 2, and here you have B minus 1.
And you get 2B minus 3.
Well, you've got another element.
You also take the parent.
So how do you do the merge.
I just want to show you the merge first.
So the way you do it is you move the parent down, and you merge these two.
Seems OK? So you move the parent node down and merge these two.
And, well, now this comes together, and this points into the new node.
Sort of clear what's going on? Questions? Yes? AUDIENCE: So now the parent is underfull? PROFESSOR: Well, so you have-- yeah, exactly.
So you have decreased the size of the parent, so it might be underfull.
So you propagate.
Anything else? AUDIENCE: So are these all different techniques for doing that? PROFESSOR: So there are two cases.
So either you have a sibling which has extra nodes to donate to you or you don't.
If you don't, then you have to do this.
AUDIENCE: But what about that case? Or is that just like-- PROFESSOR: No, that is moving it down to the leaf.
Once you move the deletion down to the leaf, so here we have something now.
And now you move it all the way back up.",Have a question... The nodes are not supposed to have more than b-1 keys.. But in his example (26:16 m) he got 5 keys in one leaf... how is that right?
TU0ankRcHmo,"And notice what I've wrote here is basically, p of t equals M times p of t.
So now, because we have this correspondence, this means that r is a stationary distribution of the random walk.
So basically, it means that this flow based equations can be interpreted based on the flow, or can also be interpreted as this intuition that there is a random walker walking around infinitely long over the graph, so that after some time, basically, it doesn't matter where the walker- random walker started, but it's really all about this distribution of where the random walker is, is going to converge to this stationary, uh, distribution.
And in order for you to compute the stationary distribution, is the same problem as it was before to solve that system of equations or to solve this recursive equation of r equals m times r.","I enjoyed the lecture. It was very nice to learn page rank with that level of detail! I had a question about the connection between random walks and the flow equation. At 20:07 we had an assumption that the random walk reaches a stationary state in a finite number of steps. Then it's clear that the stationary state is nothing but r. I didn't find how we address this assumption later in the lecture. What if it doesn't converge to a fixed state in a finite number of steps? I.e. just comes closer and closer forever. Will the limit still be r? In the lecture there was also a statement that the start of the random walk doesn't depend for the final converged state which is going to be r anyway. I think I've got a proof that addresses both questions in case M has n independent eigenvectors (n is the number of nodes). However, I'm not sure if this assumption is generic and therefore if this proof has any value. Anyway, I'd gladly discuss any related ideas."
